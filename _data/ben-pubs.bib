
@inproceedings{attanayakeDisruptionCreativityLive2020,
  title = {Disruption and {{Creativity}} in {{Live Coding}}},
  booktitle = {{{VL}}/{{HCC}} '20: {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}}},
  author = {Attanayake, Ushini and Swift, Ben and Gardner, Henry and Sorensen, Andrew},
  date = {2020-08-14},
  publisher = {{IEEE}},
  location = {{Dunedin, New Zealand}},
  doi = {10.1109/VL/HCC50065.2020.9127204},
  url = {https://ieeexplore.ieee.org/abstract/document/9127204/},
  abstract = {Live coding is an artistic performance practice where computer music and graphics are created, displayed and manipulated in front of a live audience together with projections of the (predominantly text based) computer code. Although the pressures of such performance practices are considerable, until now only familiar programming aids (for example syntax highlighting and textual overlays) have been used to assist live coders. In this paper we integrate an intelligent agent into a live coding system and examine how that agent performed in two contrasting modes of operation---as a recommender, where on request the the agent would generate (but not execute) domain-appropriate code transformations in the performer's text editor, and as a disruptor, where the agent generated and immediately executed changes to the code and the streaming music being performed. A within-subjects study of six live coders suggested that participants of the study preferred the agent's disruptive mode as a mechanism for enhancing the creativity of their live coding performances.},
  file = {/Users/ben/Zotero/storage/52WJDYJT/attanayake_et_al_2020_disruption_and_creativity_in_live_coding.pdf}
}

@inproceedings{browneCameraAdversaria2020,
  title = {Camera {{Adversaria}}},
  booktitle = {{{CHI}} '20: {{Proceedings}} of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Browne, Kieran and Swift, Ben and Nurmikko-Fuller, Terhi},
  date = {2020},
  series = {{{CHI}} '20},
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  location = {{Honolulu, HI, USA}},
  doi = {10.1145/3313831.3376434},
  url = {https://dl.acm.org/doi/abs/10.1145/3313831.3376434},
  urldate = {2020-04-25},
  abstract = {In this paper we introduce Camera Adversaria; a mobile app designed to disrupt the automatic surveillance of personal photographs by technology companies. The app leverages the brittleness of deep neural networks with respect to high-frequency signals, adding generative adversarial perturbations to users' photographs. These perturbations confound image classification systems but are virtually imperceptible to human viewers. Camera Adversaria builds on methods developed by machine learning researchers as well as a growing body of work, primarily from art and design, which transgresses contemporary surveillance systems. We map the design space of responses to surveillance and identify an under-explored region where our project is situated. Finally we show that the language typically used in the adversarial perturbation literature serves to affirm corporate surveillance practices and malign resistance. This raises significant questions about the function of the research community in countenancing systems of surveillance.},
  isbn = {978-1-4503-6708-0},
  keywords = {adversarial examples,critical design,surveillance capitalism},
  file = {/Users/ben/Zotero/storage/G7LEVEI8/browne_et_al_2020_camera_adversaria.pdf}
}

@incollection{browneCriticalChallengesVisual2018,
  title = {Critical {{Challenges}} for the {{Visual Representation}} of {{Deep Neural Networks}}},
  booktitle = {Human and {{Machine Learning}}: {{Visible}}, {{Explainable}}, {{Trustworthy}} and {{Transparent}}},
  author = {Browne, Kieran and Swift, Ben and Gardner, Henry},
  editor = {Zhou, Jianlong and Chen, Fang},
  date = {2018},
  series = {Human–{{Computer Interaction Series}}},
  pages = {119--136},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-90403-0_7},
  url = {https://doi.org/10.1007/978-3-319-90403-0_7},
  urldate = {2019-01-23},
  abstract = {Artificial neural networks have proved successful in a broad range of applications over the last decade. However, there remain significant concerns about their interpretability. Visual representation is one way researchers are attempting to make sense of these models and their behaviour. The representation of neural networks raises questions which cross disciplinary boundaries. This chapter draws on a growing collection of interdisciplinary scholarship regarding neural networks. We present six case studies in the visual representation of neural networks and examine the particular representational challenges posed by these algorithms. Finally we summarise the ideas raised in the case studies as a set of takeaways for researchers engaging in this area.},
  isbn = {978-3-319-90403-0},
  langid = {english},
  file = {/Users/ben/Zotero/storage/LTQGGT7Y/browne_et_al_2018_critical_challenges_for_the_visual_representation_of_deep_neural_networks.pdf}
}

@inproceedings{browneEatingComputersConsidered2020,
  title = {Eating {{Computers Considered Harmful}}},
  booktitle = {Alt.{{CHI}} '20: {{Extended Abstracts}} of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Browne, Kieran and Swift, Ben and Nurmikko-Fuller, Terhi},
  date = {2020},
  series = {{{CHI}} '20},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  location = {{Honolulu, HI, USA}},
  doi = {10.1145/3334480.3381810},
  url = {https://dl.acm.org/doi/abs/10.1145/3334480.3381810},
  urldate = {2020-04-25},
  abstract = {Contemporary computing devices contain a concoction of numerous hazardous materials. Though users are more or less protected from these substances, recycling and landfilling reintroduce them to the biosphere where they may be ingested by people. This paper calls on HCI researchers to consider these corporal interactions with computers and critiques HCI's existing responses to the e-waste problem. We propose that whether one would consider eating a particular electronic component offers a surprisingly useful heuristic for whether we ought to be producing it on mass with vanishingly short lifespans. We hypothesize that the adoption of this heuristic might affect user behaviour and present a diet plan for users who wish to take responsibility for their own e-waste by eating it. Finally we propose an alternative direction for HCI researchers to design and advocate for those affected by the material properties of e-waste.},
  isbn = {978-1-4503-6819-3},
  keywords = {e-waste,recipe ideas,toxicity},
  file = {/Users/ben/Zotero/storage/7YI6SUIU/browne_et_al_2020_eating_computers_considered_harmful.pdf}
}

@inproceedings{browneEnactingCollectiveOwnership2020,
  title = {Enacting {{Collective Ownership Economies}} within {{Amazon}}’s {{Mechanical Turk}}},
  booktitle = {Proceedings of "{{Worker-Centered Design}}: {{Expanding HCI Methods}} for {{Supporting Labor}}" Workshop at {{CHI}}'20},
  author = {Browne, Kieran and Swift, Ben},
  date = {2020},
  url = {https://sites.google.com/andrew.cmu.edu/workercentereddesign/},
  eventtitle = {{{CHI}} '20},
  file = {/Users/ben/Zotero/storage/SGPQU8PP/browne_swift_2020_enacting_collective_ownership_economies_within_amazon’s_mechanical_turk.pdf}
}

@inproceedings{browneOtherSideAlgorithm2018,
  title = {The {{Other Side}}: {{Algorithm As Ritual}} in {{Artificial Intelligence}}},
  shorttitle = {The {{Other Side}}},
  booktitle = {Alt.{{CHI}} '18: {{Extended Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Browne, Kieran and Swift, Ben},
  date = {2018},
  series = {{{CHI EA}} '18},
  pages = {alt11:1--alt11:9},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3170427.3188404},
  url = {http://doi.acm.org/10.1145/3170427.3188404},
  urldate = {2019-01-23},
  abstract = {Our cultural and scientific understandings of neural networks are built on a set of philosophical ideas which might turn out to be superstitions. Drawing on methodologies of defamiliarisation and performance art which have been adopted by HCI, we present an analog apparatus for the ritualistic performance of neural network algorithms. The apparatus draws on the interaction modes of the Ouija board to provide a system which involves the user in the computation. By recontextualising neural computation, the work creates critical distance with which to examine the philosophical and cultural assumptions embedded in our conception of AI.},
  eventtitle = {{{CHI}} '18},
  isbn = {978-1-4503-5621-3},
  keywords = {defamiliarisation,neural networks,performance art,ritual,séance},
  file = {/Users/ben/Zotero/storage/ZJLBQ7DP/browne_swift_2018_the_other_side.pdf}
}

@article{browneSemanticsExplanationWhy2020,
  title = {Semantics and Explanation: Why Counterfactual Explanations Produce Adversarial Examples in Deep Neural Networks},
  shorttitle = {Semantics and Explanation},
  author = {Browne, Kieran and Swift, Ben},
  date = {2020-12-18},
  url = {https://arxiv.org/abs/2012.10076v1},
  urldate = {2021-08-26},
  abstract = {Recent papers in explainable AI have made a compelling case for counterfactual modes of explanation. While counterfactual explanations appear to be extremely effective in some instances, they are formally equivalent to adversarial examples. This presents an apparent paradox for explainability researchers: if these two procedures are formally equivalent, what accounts for the explanatory divide apparent between counterfactual explanations and adversarial examples? We resolve this paradox by placing emphasis back on the semantics of counterfactual expressions. Producing satisfactory explanations for deep learning systems will require that we find ways to interpret the semantics of hidden layer representations in deep neural networks.},
  langid = {english},
  file = {/Users/ben/Zotero/storage/Z8F3XN4K/browne_swift_2020_semantics_and_explanation.pdf}
}

@article{chrzeszczykInfiniCloudDistributingHigh2016,
  title = {{{InfiniCloud}} 2.0: Distributing {{High Performance Computing}} across Continents.},
  shorttitle = {{{InfiniCloud}} 2.0},
  author = {Chrzeszczyk, Jakub and Howard, Andrew and Chrzeszczyk, Andrzej and Swift, Ben and Davis, Peter and Low, Jonathan and Tan, Tin Wee and Ban, Kenneth},
  date = {2016-06-27},
  journaltitle = {Supercomputing Frontiers and Innovations},
  volume = {3},
  number = {2},
  pages = {54-71-71},
  issn = {2313-8734},
  doi = {10.14529/jsfi160204},
  url = {http://superfri.org/superfri/article/view/95},
  urldate = {2019-01-23},
  abstract = {InfiniCloud 2.0 is World’s first native InfiniBand High Performance Cloud distributed across four continents, spanning Asia, Australia, Europe and North America. The project provides researchers with instant access to computational, storage and network resources distributed around the globe. These resources are then used to build a geographically distributed, virtual supercomputer, complete with globally-accessible parallel file system and job scheduling.This paper describes high level design and the implementation details of InfiniCloud 2.0. A gene sequencing pipeline as well as plasma physics simulation code are used to demonstrate system’s capabilities.},
  langid = {english},
  file = {/Users/ben/Zotero/storage/Q6PTUNLN/chrzeszczyk_et_al_2016_infinicloud_2.pdf}
}

@inproceedings{curranArtistsMastercodersConsensual2020,
  title = {Artists and Master-Coders in the Consensual Domain},
  booktitle = {Proceedings of "{{Where Art Meets Technology}}: {{Integrating Tangible}} and {{Intelligent Tools}} in {{Creative Processes}}" Workshop at {{CHI}}'20},
  author = {Curran, Tony and Wang, Weidi and Swift, Ben},
  date = {2020},
  url = {https://userinterfaces.aalto.fi/WhereArtMeetsTechnology/},
  eventtitle = {{{CHI}} '20},
  file = {/Users/ben/Zotero/storage/E7Z5K7HA/curran_et_al_2020_artists_and_master-coders_in_the_consensual_domain.pdf}
}

@inproceedings{grahamSocialHypergraphAnalysis2017,
  title = {"{{Social Hypergraph Analysis}}": {{Towards}} an Operationalisation of {{Actor-Network Theory Using}} Hypergraphs},
  booktitle = {67th {{Annual Conference}} of the {{International Communication Association}}},
  author = {Graham, Timothy and Ackland, Robert and Rizoiu, Marian-Andrei and Swift, Ben},
  date = {2017},
  location = {{San Diego, CA}},
  url = {https://eprints.qut.edu.au/127948/},
  urldate = {2020-03-08},
  abstract = {Despite its wide-ranging influence in social science, the field of actor-network theory (ANT) has proven difficult to operationalise quantitatively. Although social network analysis (SNA) and ANT appear to share certain affinities (e.g., the term ‘network’), attempts to develop an ANT approach to SNA (and vice versa) have stumbled upon fundamental problems or ‘discontinuities’ between them (Venturini, Munk, and Jacomy, 2016). These problems constitute serious obstacles for progressing ANT research using digital data. In this paper, we propose hypergraphs as one way forward to operationalising ANT. Broadly, we term this method ‘social hypergraph analysis’ (SHA). We outline SHA in this paper and apply it to analyse social media data, using a case study of the anti-vaccination debate on Twitter...},
  langid = {english},
  file = {/Users/ben/Zotero/storage/HDLSIQX4/graham_et_al_2017_social_hypergraph_analysis.pdf}
}

@inproceedings{heReducingLatencyCollaborative2019,
  title = {Reducing {{Latency}} in a {{Collaborative Augmented Reality Service}}},
  booktitle = {{{VRCAI}} '19: {{Proceedings}} of the 17th {{International Conference}} on {{Virtual-Reality Continuum}} and Its {{Applications}} in {{Industry}}},
  author = {He, Wennan and Swift, Ben and Gardner, Henry and Xi, Mingze and Adcock, Matt},
  date = {2019},
  pages = {1--9},
  publisher = {{ACM Press}},
  location = {{Brisbane, QLD, Australia}},
  doi = {10.1145/3359997.3365699},
  url = {http://dl.acm.org/citation.cfm?doid=3359997.3365699},
  urldate = {2020-01-09},
  eventtitle = {{{VRCAI}} '19},
  isbn = {978-1-4503-7002-8},
  langid = {english},
  file = {/Users/ben/Zotero/storage/Z92GTSM2/he_et_al_2019_reducing_latency_in_a_collaborative_augmented_reality_service.pdf}
}

@inproceedings{heSpatialAnchorBased2021,
  title = {Spatial {{Anchor Based Indoor Asset Tracking}}},
  booktitle = {{{IEEE VR}} '21: {{Proceedings}} of the {{IEEE Conference}} on {{Virtual Reality}} and {{3D User Interfaces}} ({{VR}})},
  author = {He, Wennan and Xi, Mingze and Gardner, Henry and Swift, Ben and Adcock, Matt},
  date = {2021},
  doi = {10.1109/VR50410.2021.00047},
  url = {https://ieeexplore.ieee.org/document/9417665},
  file = {/Users/ben/Zotero/storage/98BX7P3U/he_et_al_2021_spatial_anchor_based_indoor_asset_tracking.pdf}
}

@inproceedings{liTSPNetHierarchicalFeature2020,
  title = {{{TSPNet}}: {{Hierarchical Feature Learning}} via {{Temporal Semantic Pyramid}} for {{Sign Language Translation}}},
  booktitle = {{{NeurIPS}} '20: {{Poster}} Proceedings of the Thirty-Fourth {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Li, Dongxu and Xu, Chenchen and Yu, Xin and Zhang, Kaihao and Swift, Ben and Suominen, Hanna and Li, Hongdong},
  date = {2020},
  url = {https://neurips.cc/virtual/2020/public/poster_8c00dee24c9878fea090ed070b44f1ab.html},
  abstract = {Sign language translation (SLT) aims to interpret sign video sequences into text-based natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of sign videos to learn more discriminative features. To this end, we first present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet. Specifically, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96) on the largest commonly used SLT dataset. Our implementation is available at https://github.com/verashira/TSPNet.},
  file = {/Users/ben/Zotero/storage/JXLPNHZU/li_et_al_2020_tspnet.pdf}
}

@inproceedings{luAnalysisVisualisationComplex2020,
  title = {Analysis and {{Visualisation}} of {{Complex Familial Relationships}} in {{Greek Mythology}}},
  booktitle = {{{DH}} '20: {{Proceedings}} of the {{International Conference}} of the {{Alliance}} of {{Digital Humanities Organizations}} (Peer-Reviewed Poster)},
  author = {Lu, Yaya Chenyue and Swift, Ben and Hawes, Greta},
  date = {2020},
  location = {{Ottawa}},
  url = {https://dh2020.adho.org/wp-content/uploads/2020/07/198_AnalysisandVisualisationofComplexFamilialRelationshipsinGreekMythology.html},
  eventtitle = {{{DH}} '20}
}

@inproceedings{martinExploringPercussiveGesture2014,
  title = {Exploring {{Percussive Gesture}} on {{iPads}} with {{Ensemble Metatone}}},
  booktitle = {{{CHI}} '14: {{Proceedings}} of the 2014 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Martin, Charles and Gardner, Henry and Swift, Ben},
  date = {2014},
  series = {{{CHI}} '14},
  pages = {1025--1028},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2556288.2557226},
  url = {http://doi.acm.org/10.1145/2556288.2557226},
  urldate = {2019-01-23},
  abstract = {Percussionists are unique among western classical instrumentalists in that their artistic practice is defined by an approach to interaction rather than their instruments. While percussionists are accustomed to exploring non-traditional objects to create music, these objects have yet to encompass touch-screen computing devices to any great extent. The proliferation and popularity of these devices now presents an opportunity to explore their use in combining computer-generated sound together with percussive interaction in a musical ensemble. This paper examines Ensemble Metatone, a group formed to explore the "infiltration" of iPad-based musical instruments into a free-improvisation percussion ensemble. We discuss the design approach for two different iPad percussion instruments and the methodology for exploring them with the group over a series of rehearsals and performances. Qualitative analysis of discussions throughout this process shows that the musicians developed a vocabulary of gestures and musical interactions to make musical sense of these new instruments.},
  eventtitle = {{{CHI}} '14},
  isbn = {978-1-4503-2473-1},
  keywords = {expression,gesture,multitouch,music,percussion,user experience},
  file = {/Users/ben/Zotero/storage/GQWAWBLD/martin_et_al_2014_exploring_percussive_gesture_on_ipads_with_ensemble_metatone.pdf}
}

@inproceedings{martinIntelligentAgentsNetworked2016,
  title = {Intelligent {{Agents}} and {{Networked Buttons Improve Free-Improvised Ensemble Music-Making}} on {{Touch-Screens}}},
  booktitle = {{{CHI}} '16: {{Proceedings}} of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Martin, Charles and Gardner, Henry and Swift, Ben and Martin, Michael},
  date = {2016},
  series = {{{CHI}} '16},
  pages = {2295--2306},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2858036.2858269},
  url = {http://doi.acm.org/10.1145/2858036.2858269},
  urldate = {2019-01-23},
  abstract = {We present the results of two controlled studies of free-improvised ensemble music-making on touch-screens. In our system, updates to an interface of harmonically-selected pitches are broadcast to every touch-screen in response to either a performer pressing a GUI button, or to interventions from an intelligent agent. In our first study, analysis of survey results and performance data indicated significant effects of the button on performer preference, but of the agent on performance length. In the second follow-up study, a mixed-initiative interface, where the presence of the button was interlaced with agent interventions, was developed to leverage both approaches. Comparison of this mixed-initiative interface with the always-on button-plus-agent condition of the first study demonstrated significant preferences for the former. The different approaches were found to shape the creative interactions that take place. Overall, this research offers evidence that an intelligent agent and a networked GUI both improve aspects of improvised ensemble music-making.},
  eventtitle = {{{CHI}} '16},
  isbn = {978-1-4503-3362-7},
  keywords = {agent,collaborative interaction,creativity support tools,design,mobile music},
  file = {/Users/ben/Zotero/storage/QFRX2GJP/martin_et_al_2016_intelligent_agents_and_networked_buttons_improve_free-improvised_ensemble.pdf}
}

@inproceedings{martinMetatravelsMetalonsdaleIpad2014,
  title = {Metatravels and {{Metalonsdale}}: {{Ipad Apps}} for {{Percussive Improvisation}}},
  shorttitle = {Metatravels and {{Metalonsdale}}},
  booktitle = {{{CHI}} '14: {{Extended Abstracts}} of the 2014 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Martin, Charles and Gardner, Henry and Swift, Ben},
  date = {2014},
  series = {{{CHI EA}} '14},
  pages = {547--550},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2559206.2574805},
  url = {http://doi.acm.org/10.1145/2559206.2574805},
  urldate = {2019-01-23},
  abstract = {Percussionists are unique among instrumentalists in that their artistic practice is defined by an approach to interaction rather than their instruments. While percussionists are accustomed to exploring non-traditional objects to create music, these objects have yet to encompass touch-screen computing devices to any great extent. The proliferation and popularity of these devices now presents an opportunity to explore their use in combining computer-generated sound together with percussive interaction in a musical ensemble. This interactivity demonstration presents two iPad-instruments developed in collaboration with Ensemble Metatone, a group formed to explore the "infiltration" of iPad apps into a free-improvisation percussion ensemble. The apps encourage the performers' exploration through percussive gestures and use network features to support cohesive improvisation.},
  eventtitle = {{{CHI}} '14},
  isbn = {978-1-4503-2474-8},
  keywords = {artistic research,expression,gesture,multitouch,music,percussion,user experience},
  file = {/Users/ben/Zotero/storage/7RVXPLGR/martin_et_al_2014_metatravels_and_metalonsdale.pdf}
}

@inproceedings{martinMusic18Performances2015,
  title = {Music of 18 Performances: {{Evaluating}} Apps and Agents with Free Improvisation},
  booktitle = {{{ACMC}} '15: {{Proceedings}} of the 2015  {{Australasian Computer Music Conference}}},
  author = {Martin, Charles and Gardner, Henry and Swift, Ben and Martin, Michael},
  date = {2015},
  publisher = {{The Australasian Computer Music Association}},
  url = {https://computermusic.org.au/conferences/acmc-2015/},
  eventtitle = {{{ACMC}} 15},
  isbn = {1448-7780},
  file = {/Users/ben/Zotero/storage/S8N62FYD/martin_et_al_2015_music_of_18_performances.pdf}
}

@inproceedings{martinTrackingEnsemblePerformance2015,
  title = {Tracking Ensemble Performance on Touch-Screens with Gesture Classification and Transition Matrices},
  booktitle = {{{NIME}} '15: {{Proceedings}} of the {{International Conference}} on {{New Interfaces}} for {{Musical Expression}}},
  author = {Martin, Charles and Gardner, Henry and Swift, Ben},
  date = {2015},
  series = {{{NIME}} 2015},
  pages = {359--364},
  publisher = {{The School of Music and the Center for Computation and Technology (CCT), Louisiana State University}},
  location = {{Baton Rouge, Louisiana, USA}},
  doi = {10.5555/2993778.2993870},
  url = {https://dl.acm.org/doi/10.5555/2993778.2993870},
  eventtitle = {{{NIME}} '15},
  isbn = {978-0-692-49547-6},
  numpages = {6},
  venue = {Baton Rouge, Louisiana, USA},
  file = {/Users/ben/Zotero/storage/4BP755NV/martin_et_al_2015_tracking_ensemble_performance_on_touch-screens_with_gesture_classification_and.pdf}
}

@inproceedings{purcellVisualisingLiveCoding2014,
  title = {Visualising a {{Live Coding Arts Process}}},
  booktitle = {{{OZCHI}} '14: {{Proceedings}} of the 26th {{Australasian Conference}} on {{Computer-Human Interaction}}},
  author = {Purcell, Arrian and Gardner, Henry and Swift, Ben},
  date = {2014},
  series = {{{OzCHI}} '14},
  pages = {141--144},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2686612.2686634},
  url = {http://doi.acm.org/10.1145/2686612.2686634},
  urldate = {2019-01-23},
  abstract = {This paper describes an empirical study of source code visualisation as a means to communicate the programming process in "live coding" computer music performances. Following an exploratory field study of a live-coding performance at an arts festival, two different interaction-driven visualisation techniques were incorporated into a live coding system. We then performed a more controlled laboratory study to evaluate the visualisations' contributions to the audience experience, with emphasis on the (self-reported) experiential dimensions of understanding and enjoyment. Both software visualisation techniques enhanced audience enjoyment, while the effect on audience understanding was more complex. We conclude by suggesting how these visualisation techniques may be used to enhance the audience experience of live coding.},
  eventtitle = {{{OZCHI}} '14},
  isbn = {978-1-4503-0653-9},
  keywords = {live coding,musical performance,software visualisation},
  file = {/Users/ben/Zotero/storage/Q2HB99PG/purcell_et_al_2014_visualising_a_live_coding_arts_process.pdf}
}

@article{sorensenManyMeaningsLive2014,
  title = {The {{Many Meanings}} of {{Live Coding}}},
  author = {Sorensen, Andrew and Swift, Ben and Riddell, Alistair},
  date = {2014-03-01},
  journaltitle = {Computer Music Journal},
  shortjournal = {Computer Music Journal},
  volume = {38},
  number = {1},
  pages = {65--76},
  issn = {0148-9267},
  doi = {10.1162/COMJ_a_00230},
  url = {https://doi.org/10.1162/COMJ_a_00230},
  urldate = {2019-01-23},
  abstract = {The ten-year anniversary of TOPLAP presents a unique opportunity for reflection and introspection. In this essay we ask the question, what is the meaning of live coding? Our goal is not to answer this question, in absolute terms, but rather to attempt to unpack some of live coding's many meanings. Our hope is that by exploring some of the formal, embodied, and cultural meanings surrounding live-coding practice, we may help to stimulate a conversation that will resonate within the live-coding community for the next ten years.},
  file = {/Users/ben/Zotero/storage/2G22IU54/sorensen_et_al_2014_the_many_meanings_of_live_coding.pdf}
}

@inproceedings{swiftBecomingsoundAffectAssemblage2012,
  title = {Becoming-Sound: {{Affect}} and {{Assemblage}} in {{Improvisational Digital Music Making}}},
  shorttitle = {Becoming-Sound},
  booktitle = {{{CHI}} '12: {{Proceedings}} of the 2012 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Swift, Ben},
  date = {2012},
  series = {{{CHI}} '12},
  pages = {1815--1824},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2207676.2208315},
  url = {http://doi.acm.org/10.1145/2207676.2208315},
  urldate = {2019-01-23},
  abstract = {The concepts of affect and assemblage proposed by thinkers such as Gilles Deleuze and Brian Massumi can help us to understand the interaction between users and artefacts in interactive systems, particularly in the context of computer-supported improvisation and creativity. In this paper I provide an introduction to affect and assemblage theory for HCI practitioners. I then use a case study of Viscotheque, an iOS-based interface for group musical collaboration, to demonstrate the application of affective analysis in making sense of improvisational group music making.},
  eventtitle = {{{CHI}} '12},
  isbn = {978-1-4503-1015-4},
  keywords = {affect,assemblage,improvisation,music making},
  file = {/Users/ben/Zotero/storage/GJ87V5RI/swift_2012_becoming-sound.pdf}
}

@incollection{swiftChasingFeelingExperience2013,
  title = {Chasing a {{Feeling}}: {{Experience}} in {{Computer Supported Jamming}}},
  booktitle = {Music and {{Human-Computer Interaction}}},
  author = {Swift, Ben},
  editor = {Holland, Simon and Wilkie, Katie and Mulholland, Paul and Seago, Allan},
  date = {2013},
  pages = {85--99},
  publisher = {{Springer London}},
  location = {{London}},
  doi = {10.1007/978-1-4471-2990-5_5},
  url = {https://doi.org/10.1007/978-1-4471-2990-5_5},
  abstract = {Improvisational group music-making, informally known as ‘jamming’, has its own cultures and conventions of musical interaction. One characteristic of this interaction is the primacy of the experience over the musical artefact—in some sense the sound created is not as important as the feeling of being ‘in the groove’. As computing devices infiltrate creative, open-ended task domains, what can Human-Computer Interaction (HCI) learn from jamming? How do we design systems where the goal is not an artefact but a felt experience? This chapter examines these issues in light of an experiment involving ‘Viscotheque’, a novel group music-making environment based on the iPhone.},
  isbn = {978-1-4471-2990-5},
  file = {/Users/ben/Zotero/storage/8ARS87WI/swift_2013_chasing_a_feeling.pdf}
}

@inproceedings{swiftChasingWindExperience2011,
  title = {A {{Chasing After}} the {{Wind}}: {{Experience}} in {{Computer-supported Group Musicmaking}}},
  booktitle = {Proceedings of "{{When}} Words Fail: {{What}} Can Music Interaction Tell Us about {{HCI}}?" Workshop at {{British HCI}}},
  author = {Swift, Ben and Gardner, Henry and Riddell, Alistair},
  date = {2011-05-08},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.460.5123},
  eventtitle = {British {{HCI Group Annual Conference}} on {{People}} and {{Computers}}},
  file = {/Users/ben/Zotero/storage/URQ7F4R2/swift_et_al_2011_a_chasing_after_the_wind.pdf}
}

@inproceedings{swiftCodingLivecoding2014,
  title = {Coding {{Livecoding}}},
  booktitle = {{{CHI}} '14: {{Proceedings}} of the 2014 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Swift, Ben and Sorensen, Andrew and Martin, Michael and Gardner, Henry},
  date = {2014},
  series = {{{CHI}} '14},
  pages = {1021--1024},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2556288.2557049},
  url = {http://doi.acm.org/10.1145/2556288.2557049},
  urldate = {2019-01-23},
  abstract = {Livecoding is an artistic programming practice in which an artist's low-level interaction can be observed with sufficiently high fidelity to allow for transcription and analysis. This paper presents the first reported "coding" of livecoding videos. From an identified corpus of videos available on the web, we coded performances of two different livecoding artists, recording both the (textual) programming edit events and the musical effect of these edits. Our analysis includes a novel, transition-matrix visualisation of the textual and musical dimensions of this data to create a "performer fingerprint". We show how detailed transcriptions of livecoding videos can be made which, we hope, will provide a foundation for further research into describing and understanding livecoding.},
  eventtitle = {{{CHI}} '14},
  isbn = {978-1-4503-2473-1},
  keywords = {creativity support tools,end-user programming},
  file = {/Users/ben/Zotero/storage/8XGUHV48/swift_et_al_2014_coding_livecoding.pdf}
}

@article{swiftDesignSmartphonebasedDigital2012,
  title = {The Design of a Smartphone-Based Digital Musical Instrument for Jamming},
  author = {Swift, Ben},
  date = {2012},
  journaltitle = {ANU Library Open Research},
  volume = {PhD dissertations},
  doi = {10.25911/5d5157b18b58f},
  url = {https://openresearch-repository.anu.edu.au/handle/1885/151432},
  urldate = {2019-09-20},
  abstract = {Open-ended human-computer interactions, such as those in interactive digital art and music, are an increasingly popular area of study in HCI. They provide an opportunity to examine playfulness, creativity and expression and challenge conventional HCI notions of quality, evaluation and how to measure success.  Jamming-improvisational group music making-is often held up as an example of open-ended creativity. This thesis describes the development of Viscotheque, an iPhone- based digital musical instrument (DMI) designed for jamming, over three major design-test cycles. Over these three iterations the interface evolved from a very simple 'process control'  interface in v1 to a more expressive multi-touch sample manipulation tool in v3. At each stage of the design process, open-ended jam sessions held with local musicians suggested that the potential was there for the interface to support rich jamming experiences. Version 3 of the interface and the associated v3 jam session was the most in-depth of the three phases of the experiment, with the most expressive interface and also the most comprehensive field trial (using a multi-session longitudinal study of jamming musicians rather than the single jam sessions of v1 and v2).  Situating the qualitative results of these experiments within the broader context of third wave HCI, this thesis discusses 'affect' in a guise perhaps unfamiliar to readers of mainstream HCI discourse. The jam sessions were characterised by intense sonic atmospheres, and the post-jam interviews reveal a complicated picture of agency in the interaction of the musician and their sound. The thesis also presents a detailed analysis of the quantitative log data, including the results of a Machine Learning (ML) approach to looking for patterns in this data.  Finally, the thesis discusses the implications of the Viscotheque design process for HCI more broadly, including the powerful affective atmospheres which characterise musical interaction and an approach to data analysis which leverages the mathematical sophistication of modern ML techniques while remaining sensitive to the difficulties surrounding the measurement of experience. -- provided by Candidate.},
  langid = {australian},
  file = {/Users/ben/Zotero/storage/JX4PQ6AW/swift_2012_the_design_of_a_smartphone-based_digital_musical_instrument_for_jamming.pdf}
}

@inproceedings{swiftDistributedPerformanceLive2009,
  title = {Distributed {{Performance}} in {{Live Coding}}},
  booktitle = {{{ACMC}} '09: {{Proceedings}} of the 2009  {{Australasian Computer Music Conference}}},
  author = {Swift, Ben and Gardner, Henry and Riddell, Alistair},
  date = {2009-07},
  pages = {1--6},
  url = {https://computermusic.org.au/conferences/acmc2009/},
  eventtitle = {{{ACMC}} '09},
  file = {/Users/ben/Zotero/storage/3KI47PYM/swift_et_al_2009_distributed_performance_in_live_coding.pdf}
}

@inproceedings{swiftEngagementNetworksSocial2010,
  title = {Engagement {{Networks}} in {{Social Music-making}}},
  booktitle = {{{OZCHI}} '10: {{Proceedings}} of the 22nd {{Australasian Conference}} on {{Computer-Human Interaction}}},
  author = {Swift, Ben and Gardner, Henry and Riddell, Alistair},
  date = {2010},
  series = {{{OZCHI}} '10},
  pages = {104--111},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1952222.1952244},
  url = {http://doi.acm.org/10.1145/1952222.1952244},
  urldate = {2019-01-23},
  abstract = {Social music-making systems offer the possibility of accessible and engaging group experiences. In this paper we explore questions concerning the notion of 'engagement' in social music-making. In a recent user study of Viscotheque, an iPhone-based environment for group musical creativity, three different types of engagement were observed: individual, unilateral and bilateral. These results indicate that network-based approaches may be useful in analysing engagement relationships amongst participants in group music-making.},
  eventtitle = {{{OZCHI}} '10},
  isbn = {978-1-4503-0502-0},
  keywords = {engagement,mutual},
  file = {/Users/ben/Zotero/storage/K7CQUSEK/swift_et_al_2010_engagement_networks_in_social_music-making.pdf}
}

@article{swiftImpishGrooves2011,
  title = {Impish {{Grooves}}},
  author = {Swift, Ben},
  date = {2011-01-01},
  journaltitle = {Computer Music Journal},
  shortjournal = {Computer Music Journal},
  volume = {35},
  number = {4},
  pages = {119--137},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/COMJ_e_00098}
}

@inproceedings{swiftIntroductionMusicmakingExtempore2020,
  title = {Introduction to Music-Making in {{Extempore}} (Workshop)},
  shorttitle = {{{ICLC}}'20},
  booktitle = {{{ICLC}}'20: {{Proceedings}} of the 2020 {{International Conference}} on {{Live Coding}}},
  author = {Swift, Ben},
  date = {2020},
  pages = {197},
  publisher = {{University of Limerick}},
  location = {{Limerick, Ireland}},
  doi = {10.5281/zenodo.3939527},
  url = {https://zenodo.org/record/3939527#.X6M9jy8RphF},
  isbn = {978-1-911620-23-5},
  file = {/Users/ben/Zotero/storage/4TBPUVHP/swift_2020_introduction_to_music-making_in_extempore_(workshop).pdf}
}

@article{swiftLiveProgrammingScientific2016,
  title = {Live {{Programming}} in {{Scientific Simulation}}},
  author = {Swift, Ben and Sorensen, Andrew and Gardner, Henry and Davis, Peter and Decyk, Viktor K.},
  date = {2016-03-07},
  journaltitle = {Supercomputing Frontiers and Innovations},
  volume = {2},
  number = {4},
  pages = {4-15-15},
  issn = {2313-8734},
  doi = {10.14529/jsfi150401},
  url = {http://superfri.org/superfri/article/view/72},
  urldate = {2019-01-23},
  abstract = {We demonstrate that a live-programming environment can be used to harness and add run-time interactivity to scientific simulation codes. Through a set of examples using a Particle-In-Cell (PIC) simulation framework we show how the real-time, human-in-the-loop interactivity of live programming can be incorporated into a traditional, “offline”, development workflow. We discuss how live programming tools and techniques can be productively integrated into the existing HPC landscape to increase productivity and enhance exploration and discovery.},
  langid = {english},
  file = {/Users/ben/Zotero/storage/IGLQLS6W/swift_et_al_2016_live_programming_in_scientific_simulation.pdf}
}

@inproceedings{swiftMindmodulatedMusicMind2007,
  title = {Mind-Modulated {{Music}} in the {{Mind Attention Interface}}},
  booktitle = {{{OZCHI}} '07: {{Proceedings}} of the 19th {{Australasian Conference}} on {{Computer-Human Interaction}}},
  author = {Swift, Ben and Sheridan, James and Zhen, Yang and Gardner, Henry},
  date = {2007},
  series = {{{OZCHI}} '07},
  pages = {83--86},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1324892.1324907},
  url = {http://doi.acm.org/10.1145/1324892.1324907},
  urldate = {2019-01-23},
  eventtitle = {{{OZCHI}} '07},
  isbn = {978-1-59593-872-5},
  keywords = {brain computer interface,computer music,EEG,functional connectivity},
  file = {/Users/ben/Zotero/storage/P7NEGBWQ/swift_et_al_2007_mind-modulated_music_in_the_mind_attention_interface.pdf}
}

@inproceedings{swiftNetworkedLivecodingVL2014,
  title = {Networked Livecoding at {{VL}}/{{HCC}} 2013},
  booktitle = {{{VL}}/{{HCC}} '14: {{IEEE Symposium}} on {{Visual Languages}} and {{Human-Centric Computing}}},
  author = {Swift, Ben and Gardner, Henry and Sorensen, Andrew},
  date = {2014-07},
  pages = {221--222},
  doi = {10.1109/VLHCC.2014.6883065},
  url = {https://doi.ieeecomputersociety.org/10.1109/VLHCC.2014.6883065},
  urldate = {2019-01-23},
  abstract = {Network connectivity offers the potential for a group of musicians to play together over the network. This paper describes a trans-Atlantic networked musical livecoding performance between Andrew Sorensen in Germany (at the Schloss Daghstuhl conference on Collaboration and Learning through Live Coding) and Ben Swift in San Jose (at YL/HCC) in September 2013. In this paper we describe the infrastructure developed to enable this performance.},
  eventtitle = {{{VL}}/{{HCC}} '14},
  isbn = {978-1-4799-4035-6},
  keywords = {Australia,Bandwidth,Collaboration,Computer architecture,Educational institutions,Encoding,Servers},
  file = {/Users/ben/Zotero/storage/LSZBNL2P/swift_et_al_2014_networked_livecoding_at_vl-hcc_2013.pdf}
}

@inproceedings{swiftThoughtsANULaptop2018,
  title = {Thoughts on an {{ANU Laptop Ensemble}}: Where Do New Media Creators Fit in the Modern University?},
  booktitle = {Code2k18: {{A Media Conference}} of {{Platforms}}, {{Devices}} and {{Screens}}},
  author = {Swift, Ben and Hunter, Alexander},
  date = {2018-11-19},
  location = {{Swinburne University, Melbourne, Australia}},
  url = {http://www.code2k18.com},
  abstract = {Computer Science student numbers are exploding around the country. Economic realities and future employment prospects are certainly part of this trend, as well as parental \& societal pressure and many other factors. However, not all of these CS students want to go to Silicon Valley post-graduation and earn a billion dollars – some want to be cultural creators, indeed many already are (arts, music, memes, etc.). The core CS curriculum is still fairly traditional, and "creative code" activities usually fall into elective courses at best (or, indeed, are forced into spare time as hobbies).   In addition, CS has well-documented diversity problems. This is a wicked problem which requires addressing at many levels, but most would agree that part of the solution is having ways to attract these new media creators (e.g. STEAM rather than just STEM).   On the other side of campus, prevailing forces over the last couple of decades have not been kind to the creative departments (Schools of Music, Schools of Art, and the humanities in general-–-with a few notable exceptions). University funding is governed by economic realities, and these art \& craft-based disciplines, which by their very nature require high student to teacher ratios, are finding themselves increasingly difficult to justify in that context.   There is a lot of rhetoric about the importance of cross-campus and interdisciplinary collaboration, including in the creative arts/technology space. However, most of the energy and excitement surrounding cross-campus collaboration seems to be in "innovation/entrepreneurship" programs. While these programs often do lead to positive outcomes, there is usually a strong commercial imperative in an innovation context, so it's not always easy to see where innovative creative applications of technology fit into this world.   Our provocation is that the modern university has a huge untapped potential for new media content creation, but departmental silos stifle its flourishing, and that current 'de-silofication' initiatives (e.g. innovation/entrepreneurship) are too crassly-commercial to support many forms of collaborative content creation. The question is: what pragmatic, short-lead-time activities can we do support to release this untapped potential?   Over the last few years, we, two young academics-–-one, a live-coder and lecturer in Computer Science, the other, a composer/improviser from Music-–-have been finding ways to bring our students together in meaningful collaborative multimedia projects. Recently we have formalised these previously extracurricular activities in the new ANU Laptop Ensemble.   This group of \textasciitilde 10 students, from Computer Science, Art and Music, are currently working together as a pilot program using an existing "group project course" infrastructure primarily designed for industry client-based tech entrepreneurship projects for later-year CS students. In this we assume the roles (client and tutor) usually filled by industry mentors, helping students find their way through the seemingly infinite possibilities of digital performance that lie waiting inside every laptop and mobile device. The client's brief: to provide a group of undergraduate students the space to explore the creative potential of the technology they use every day-–-to make, break, mend, hack, learn and unlearn.   With the current rate of development in technology, we can't sit around and wait for budget, interest and motivation to align perfectly, or the creation of new, multi-million dollar facilities. By the time they're built, we've moved on. We need to create more spaces for our students, and ourselves, to be creative and nimble, moving with and ahead of trends and innovations.   New Media departments (either inside the Design school or elsewhere) can't be just another silo. The more we speak to students from different parts of the university, the more we realise that the next generation of new media creators are spread across the whole campus. They are a complex group with a wide range of goals, interests, skills, identities and backgrounds. Our aim is to create a space in which these curious minds can explore freedom from the academic silos they inherited from previous generations and sculpt their own technological landscape.},
  eventtitle = {Code2k18: {{A Media Conference}} of {{Platforms}}, {{Devices}} and {{Screens}}}
}

@inproceedings{swiftTwoPerspectivesRebooting2019,
  title = {Two {{Perspectives}} on {{Rebooting Computer Music Education}}: {{Composition}} and {{Computer Science}}},
  booktitle = {{{ACMC}} '19: {{Proceedings}} of the 2019  {{Australasian Computer Music Conference}}},
  author = {Swift, Ben and Martin, Charles Patrick and Hunter, Alexander},
  date = {2019},
  pages = {53--57},
  publisher = {{Australasian Computer Music Association}},
  location = {{Melbourne, Australia}},
  doi = {10.25911/5e37e8d92ff89},
  url = {http://computermusic.org.au/conferences/acmc-2019/},
  eventtitle = {{{ACMC}} '19},
  isbn = {1448-7780},
  file = {/Users/ben/Zotero/storage/IRQPLTWL/swift_et_al_2019_two_perspectives_on_rebooting_computer_music_education.pdf}
}

@inproceedings{swiftVisualCodeAnnotations2013,
  title = {Visual Code Annotations for Cyberphysical Programming},
  booktitle = {{{LIVE}} '13: 1st {{International Workshop}} on {{Live Programming}} at {{ICSE}}},
  author = {Swift, Ben and Sorensen, Andrew and Gardner, Henry and Hosking, John},
  date = {2013-05},
  pages = {27--30},
  doi = {10.1109/LIVE.2013.6617345},
  url = {https://ieeexplore.ieee.org/document/6617345},
  abstract = {User interfaces for source code editing are a crucial component in any software development environment, and in many editors visual annotations (overlaid on the textual source code) are used to provide important contextual information to the programmer. This paper focuses on the real-time programming activity of `cyberphysical' programming, and considers the type of visual annotations which may be helpful in this programming context.},
  eventtitle = {{{LIVE}} '13},
  keywords = {Context,contextual information,cyberphysical programming,interactive programming,Production,program compilers,Programming,programming context,real-time programming activity,Robots,Software,software development environment,source code editing,System-on-chip,textual source code,user interfaces,visual code annotations,visual programming,Visualization},
  file = {/Users/ben/Zotero/storage/QNMKPMGZ/swift_et_al_2013_visual_code_annotations_for_cyberphysical_programming.pdf}
}

@inproceedings{wangAnalysisVisualizationNarrative2019,
  title = {Analysis and {{Visualization}} of {{Narrative}} in {{Shanhaijing Using Linked Data}}},
  booktitle = {{{DH}} '19: {{Proceedings}} of the {{International Conference}} of the {{Alliance}} of {{Digital Humanities Organizations}}},
  author = {Wang, Qian and Nurmikko-Fuller, Terhi and Swift, Ben},
  date = {2019},
  location = {{Utrecht}},
  url = {https://dev.clariah.nl/files/dh2019/boa/0462.html},
  eventtitle = {{{DH}} '19},
  file = {/Users/ben/Zotero/storage/FG3E26RF/wang_et_al_2019_analysis_and_visualization_of_narrative_in_shanhaijing_using_linked_data.pdf}
}

@article{wuHighResolutionLabeling2021,
  title = {High {{Resolution}} and {{Labeling Free Studying}} the {{3D Microstructure}} of the {{Pars Tensa-Annulus Unit}} of {{Mice}}},
  author = {Wu, Jian-Ping and Yang, Xiaojie and Wang, Yilin and Swift, Ben and Adamson, Robert and Zheng, Yongchang and Zhang, Rongli and Zhong, Wen and Chen, Fangyi},
  date = {2021},
  journaltitle = {Frontiers in Cell and Developmental Biology},
  volume = {9},
  pages = {2441},
  issn = {2296-634X},
  doi = {10.3389/fcell.2021.720383},
  url = {https://www.frontiersin.org/article/10.3389/fcell.2021.720383},
  urldate = {2021-10-11},
  abstract = {Hearing loss is a serious illness affecting people’s normal life enormously. The acoustic properties of a tympanic membrane play an important role in hearing, and highly depend on its geometry, composition, microstructure and connection to the surrounding annulus. While the conical geometry of the tympanic membrane is critical to the sound propagation in the auditory system, it presents significant challenges to the study of the 3D microstructure of the tympanic membrane using traditional 2D imaging techniques. To date, most of our knowledge about the 3D microstructure and composition of tympanic membranes is built from 2D microscopic studies, which precludes an accurate understanding of the 3D microstructure, acoustic behaviors and biology of the tissue. Although the tympanic membrane has been reported to contain elastic fibers, the morphological characteristic of the elastic fibers and the spatial arrangement of the elastic fibers with the predominant collagen fibers have not been shown in images. We have developed a 3D imaging technique for the three-dimensional examination of the microstructure of the full thickness of the tympanic membranes in mice without requiring tissue dehydration and stain. We have also used this imaging technique to study the 3D arrangement of the collagen and elastic fibrillar network with the capillaries and cells in the pars tensa-annulus unit at a status close to the native. The most striking findings in the study are the discovery of the 3D form of the elastic and collagen network, and the close spatial relationships between the elastic fibers and the elongated fibroblasts in the tympanic membranes. The 3D imaging technique has enabled to show the 3D waveform contour of the collagen and elastic scaffold in the conical tympanic membrane. Given the close relationship among the acoustic properties, composition, 3D microstructure and geometry of tympanic membranes, the findings may advance the understanding of the structure—acoustic functionality of the tympanic membrane. The knowledge will also be very helpful in the development of advanced cellular therapeutic technologies and 3D printing techniques to restore damaged tympanic membranes to a status close to the native.},
  file = {/Users/ben/Zotero/storage/PIIPEKWV/wu_et_al_2021_high_resolution_and_labeling_free_studying_the_3d_microstructure_of_the_pars.pdf}
}

@article{wuHighresolutionStudy3D2017,
  title = {High-resolution study of the 3D collagen fibrillary matrix of Achilles tendons without tissue labelling and dehydrating},
  author = {Wu, Jian-Ping and Swift, Ben and Becker, Thomas and Squelch, Andrew and Wang, Allan and Zheng, Yong-Chang and Zhao, Xuelin and Xu, Jiake and Xue, Wei and Zheng, Minghao and Lloyd, David and Kirk, Thomas Brett},
  date = {2017},
  journaltitle = {Journal of Microscopy},
  volume = {266},
  number = {3},
  pages = {273--287},
  issn = {1365-2818},
  doi = {10.1111/jmi.12537},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jmi.12537},
  urldate = {2019-01-23},
  abstract = {Knowledge of the collagen structure of an Achilles tendon is critical to comprehend the physiology, biomechanics, homeostasis and remodelling of the tissue. Despite intensive studies, there are still uncertainties regarding the microstructure. The majority of studies have examined the longitudinally arranged collagen fibrils as they are primarily attributed to the principal tensile strength of the tendon. Few studies have considered the structural integrity of the entire three-dimensional (3D) collagen meshwork, and how the longitudinal collagen fibrils are integrated as a strong unit in a 3D domain to provide the tendons with the essential tensile properties. Using second harmonic generation imaging, a 3D imaging technique was developed and used to study the 3D collagen matrix in the midportion of Achilles tendons without tissue labelling and dehydration. Therefore, the 3D collagen structure is presented in a condition closely representative of the in vivo status. Atomic force microscopy studies have confirmed that second harmonic generation reveals the internal collagen matrix of tendons in 3D at a fibril level. Achilles tendons primarily contain longitudinal collagen fibrils that braid spatially into a dense rope-like collagen meshwork and are encapsulated or wound tightly by the oblique collagen fibrils emanating from the epitenon region. The arrangement of the collagen fibrils provides the longitudinal fibrils with essential structural integrity and endows the tendon with the unique mechanical function for withstanding tensile stresses. A novel 3D microscopic method has been developed to examine the 3D collagen microstructure of tendons without tissue dehydrating and labelling. The study also provides new knowledge about the collagen microstructure in an Achilles tendon, which enables understanding of the function of the tissue. The knowledge may be important for applying surgical and tissue engineering techniques to tendon reconstruction.},
  langid = {french},
  keywords = {3D collagen structure,Achilles tendons,computer imaging analysis,SHG imaging},
  file = {/Users/ben/Zotero/storage/GHQSAZG2/wu_et_al_2017_high-resolution_study_of_the_3d_collagen_fibrillary_matrix_of_achilles_tendons.pdf}
}

@article{wuTextureAnalysis3D2015,
  title = {Texture Analysis of the {{3D}} Collagen Network and Automatic Classification of the Physiology of Articular Cartilage {{AU}}  - {{Duan}}, {{Xiaojuan}}},
  author = {Wu, Jianping and Swift, Ben and Kirk, Thomas Brett},
  date = {2015-07-04},
  journaltitle = {Computer Methods in Biomechanics and Biomedical Engineering},
  shortjournal = {Computer Methods in Biomechanics and Biomedical Engineering},
  volume = {18},
  number = {9},
  pages = {931--943},
  issn = {1025-5842},
  doi = {10.1080/10255842.2013.864284},
  url = {https://doi.org/10.1080/10255842.2013.864284},
  file = {/Users/ben/Zotero/storage/2D2IABJ8/wu_et_al_2015_texture_analysis_of_the_3d_collagen_network_and_automatic_classification_of_the.pdf}
}


