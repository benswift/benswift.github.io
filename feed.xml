<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://benswift.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://benswift.me/" rel="alternate" type="text/html" /><updated>2023-01-13T11:01:35+11:00</updated><id>https://benswift.me/feed.xml</id><title type="html">benswift.me</title><subtitle>livecoder &amp; researcher homepage - code, creativity, culture</subtitle><author><name>Ben Swift</name></author><entry><title type="html">rclone: exclude all git repos</title><link href="https://benswift.me/blog/2023/01/11/rclone-exclude-all-git-repos/" rel="alternate" type="text/html" title="rclone: exclude all git repos" /><published>2023-01-11T00:00:00+11:00</published><updated>2023-01-11T00:00:00+11:00</updated><id>https://benswift.me/blog/2023/01/11/rclone-exclude-all-git-repos</id><content type="html" xml:base="https://benswift.me/blog/2023/01/11/rclone-exclude-all-git-repos/"><![CDATA[<p>For a long time I’ve used <a href="https://rclone.org">rclone</a> for remote backups and
it’s awesome. I have a script which syncs all the files I care about on my
laptop to cloudstor (which, being on the Australian university network has the
benefit that I get 1Gbps upload when I’m on the ANU network). My institution
(the ANU)</p>

<p>However, it turns out that
<a href="https://support.aarnet.edu.au/hc/en-us/articles/5697089309711">cloudstor</a> is
shutting down at the end of this year, so I need to find a new home for my
backups. I’ve been told that the “replacement” is to use my institutional
OneDrive account.</p>

<p>My feelings about closing down useful (local) infrastructure and centralising
things on one of the big cloud providers notwithstanding, this should be an easy
change—<a href="https://rclone.org/onedrive/">rclone supports OneDrive</a>, so it’s a
simple config change to move over.</p>

<p>However, I figured I’d take the opportunity to fix something which had long
bothered me. I have a <em>lot</em> of stuff in git repos which don’t need to be backed
up with rclone, and they just slow down the backup process. I wanted to get
rclone to ignore all git repos, and although it has an <a href="https://rclone.org/filtering/">include/exclude
filtering</a> system it’s not easy to get it to
filter all git repos (trust me, <a href="https://forum.rclone.org/t/ignore-all-git-repos-when-syncing/33023">I
checked</a>).</p>

<p>Now that it’s summertime I had a chance to make it work, and since that forum
thread is now locked I’m posting it here to save you some time, dear reader, if
you ever want to do this for yourself. Feel free to use/modify it (MIT Licence)
if it’s helpful.</p>

<pre><code class="language-bash">#!/bin/bash

EXCLUDE_FROM_FILE="/tmp/rclone-excludes.txt"
CLONE_ROOT_DIR=~/Documents
REMOTE=anu-onedrive

## find all git repo enclosing folders (including trailing slash), munge them
## into the form that rclone expects for its "exclude from" file
cd $CLONE_ROOT_DIR &amp;&amp; find . -type d -name .git -exec  dirname {} \; | sed -e 's/$/\//' -e 's/^.//' &gt; $EXCLUDE_FROM_FILE

## add a few extra excludes
echo ".DS_Store" &gt;&gt; $EXCLUDE_FROM_FILE

## sync to remote
rclone sync --progress --exclude-from=$EXCLUDE_FROM_FILE $CLONE_ROOT_DIR $REMOTE:mitch-rclone/$CLONE_ROOT_DIR
</code></pre>]]></content><author><name>Ben Swift</name></author><category term="tools" /><summary type="html"><![CDATA[For a long time I’ve used rclone for remote backups and it’s awesome. I have a script which syncs all the files I care about on my laptop to cloudstor (which, being on the Australian university network has the benefit that I get 1Gbps upload when I’m on the ANU network). My institution (the ANU)]]></summary></entry><entry><title type="html">A School of Cybernetics strategy meeting with ChatGPT</title><link href="https://benswift.me/blog/2022/12/07/a-school-of-cybernetics-strategy-meeting-with-chatgpt/" rel="alternate" type="text/html" title="A School of Cybernetics strategy meeting with ChatGPT" /><published>2022-12-07T00:00:00+11:00</published><updated>2022-12-07T00:00:00+11:00</updated><id>https://benswift.me/blog/2022/12/07/a-school-of-cybernetics-strategy-meeting-with-chatgpt</id><content type="html" xml:base="https://benswift.me/blog/2022/12/07/a-school-of-cybernetics-strategy-meeting-with-chatgpt/"><![CDATA[<p>Back at my desk after a couple of days off I finally got a chance to fire up the
<a href="https://chat.openai.com">OpenAI ChatGPT</a> AI language model that everyone’s
talking about.</p>

<p>Here’s—and this is the honest truth—the first conversation I had with it.
The initial paragraph was from the <a href="https://cybernetics.anu.edu.au/about/">ANU School of Cybernetics
website</a>, and from there we just
freewheeled. My inputs are in regular type, the ChatGPT responses are in the
coloured boxes.</p>

<hr />

<p><em>begin chat transcript</em></p>

<p>The 3A Institute sits within the School of Cybernetics with the mission to build
a new branch of engineering to take AI-enabled cyber-physical systems to scale
and to create a new generation of practitioners with the skills and knowledge we
need to help shape this future safely, sustainably and responsibly.</p>

<p>As people invent, commercialise and operationalise new technologies, new
practitioners emerge to manage them. Just as the invention of programmable
computers led to the professionalisation of software engineers, cyber-physical
systems will lead to a new type of engineer.</p>

<p>We don’t know what this new type of engineering will be called yet, but we know
we need it if we are to build a safe, sustainable and responsible future.</p>

<p>Give 5 concrete examples of how ideas from cybernetics can help:</p>

<div class="hl-para">

  <ol>
    <li>Cybernetics can help with the design and optimization of complex systems,
such as transportation networks or power grids, by using feedback loops and
control systems to ensure efficient and reliable operation.</li>
    <li>Cybernetic principles can be applied to the development of intelligent
machines and autonomous systems, enabling them to learn from their
environment and adapt to changing conditions.</li>
    <li>Cybernetics can be used to improve human-machine interactions, such as
through the design of user-friendly interfaces or the integration of wearable
technologies that enhance our physical abilities.</li>
    <li>Cybernetic concepts can be applied to the study of biological systems, such
as the nervous system, to gain a better understanding of how they function
and how they can be treated or improved.</li>
    <li>Cybernetics can be used to address societal challenges, such as promoting
sustainable development or addressing inequality, by providing a framework
for analyzing and addressing complex systems-level problems.</li>
  </ol>

</div>

<p>Can you try again, but this time using the 19th Century Australian Overland Telegraph as a framing device?</p>

<div class="hl-para">

  <p>Sure, here are five examples of how ideas from cybernetics can help using the
19th Century Australian Overland Telegraph as a framing device:</p>

  <ol>
    <li>The Overland Telegraph relied on a complex network of telegraph lines,
stations, and operators to transmit messages across the vast Australian
continent. Cybernetics can help optimize the design and operation of such
networks by using feedback loops and control systems to ensure efficient and
reliable communication.</li>
    <li>The Overland Telegraph used complex coding systems to encode and decode
messages, and cybernetic principles can be applied to the development of
intelligent machines and algorithms that can learn and adapt to improve the
efficiency and accuracy of such coding systems.</li>
    <li>The Overland Telegraph required skilled operators to manage the transmission
and receipt of messages, and cybernetics can be used to improve human-machine
interactions through the design of user-friendly interfaces and tools that
enable operators to work more efficiently and effectively.</li>
    <li>The Overland Telegraph relied on a network of towers and poles to support the
telegraph lines, and cybernetic concepts can be applied to the study of such
infrastructure to optimize its design and maintenance for long-term
reliability and durability.</li>
    <li>The Overland Telegraph played a crucial role in connecting remote parts of
Australia and facilitating economic and social development, and cybernetics
can be used to address societal challenges by providing a framework for
analyzing and addressing complex systems-level problems and developing
solutions that promote sustainable and equitable growth.</li>
  </ol>

</div>

<p>Ok, that’s better—I think it’s important that we provide a uniquely Australian
viewpoint on what cybernetics is and how it can help tackle the challenges we
face in the 21st century. As a new School of Cybernetics, then, what should our
top priorities be?</p>

<div class="hl-para">

  <p>As a new School of Cybernetics, some top priorities could include:</p>

  <ol>
    <li>Developing a strong research program focused on applying cybernetic
principles to real-world challenges facing Australia, such as improving the
resilience and sustainability of critical infrastructure, enhancing the
security and privacy of digital systems, or addressing complex societal
issues such as inequality and climate change.</li>
    <li>Providing high-quality education and training programs that enable students
to gain a deep understanding of cybernetic concepts and techniques, and
develop the skills and knowledge needed to become leaders in the field.</li>
    <li>Building partnerships and collaborations with other academic institutions,
industry partners, and government agencies to facilitate the exchange of
ideas, knowledge, and expertise, and to support the development and
implementation of innovative cybernetic solutions.</li>
    <li>Playing a key role in promoting public awareness and understanding of
cybernetics, and addressing common misconceptions and concerns about the
field, in order to foster a positive and informed dialogue about its
potential benefits and limitations.</li>
    <li>Establishing a strong and supportive community of scholars, practitioners,
and stakeholders within the field of cybernetics, and providing opportunities
for networking, collaboration, and professional development.</li>
  </ol>

</div>

<p><em>end chat transcript</em></p>

<hr />

<p>Well, wow. This required no re-prompting or editing on my part.</p>

<p>I should say that I share all of the skepticism (and fears!) of folks who are
pointing out that these language models don’t actually understand things. In
some cases this means the output of these models is benign but incorrect, and
it’s easy to think of situations where real harm could be caused.</p>

<p>Upon close reading, I don’t think the above responses capture the full breadth
of what we’re trying to do here in the School of Cybernetics. But I’d be lying
if I wasn’t impressed.</p>

<p>As I <a href="https://twitter.com/benswift/status/1600265214338867200">tweeted earlier this
morning</a>:</p>

<blockquote>
  <p>all this ChatGPT stuff means that we as a society are gonna have to get a
<strong>lot</strong> better at drinking from the bullshit firehose, and fast</p>
</blockquote>

<p>I’ll pen some more thoughts about what it all means when I’ve had some more time
to digest things.</p>]]></content><author><name>Ben Swift</name></author><category term="ai" /><category term="cybernetics" /><summary type="html"><![CDATA[Back at my desk after a couple of days off I finally got a chance to fire up the OpenAI ChatGPT AI language model that everyone’s talking about.]]></summary></entry><entry><title type="html">AI art installations and livecoding gigs in Nov/Dec</title><link href="https://benswift.me/blog/2022/12/06/ai-art-installations-and-livecoding-gigs-in-nov-dec/" rel="alternate" type="text/html" title="AI art installations and livecoding gigs in Nov/Dec" /><published>2022-12-06T00:00:00+11:00</published><updated>2022-12-06T00:00:00+11:00</updated><id>https://benswift.me/blog/2022/12/06/ai-art-installations-and-livecoding-gigs-in-nov-dec</id><content type="html" xml:base="https://benswift.me/blog/2022/12/06/ai-art-installations-and-livecoding-gigs-in-nov-dec/"><![CDATA[<p>If you <a href="https://twitter.com/benswift">follow me on twitter</a> you’ve seen this
already, but over the last couple of months I’ve been hard at work on an AI art
installation called Panic. This was a collaboration with <a href="https://cybernetics.anu.edu.au/people/adrian-schmidt/">Adrian
Schmidt</a>—I did the
software, he did the hardware (including the cool, 6” diameter <em>Panic!</em> button).</p>

<div style="width:100%; margin: 0px auto; margin-bottom: 1rem;">
  <div style="position: relative; padding-bottom: 62.5%; height: 0px;">
	<iframe style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%;" src="https://www.youtube.com/embed/wpOkUX_LmQo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
	</iframe>
  </div>
</div>

<p>I wrote <a href="https://cybernetics.anu.edu.au/news/2022/11/22/panic-a-serendipity-engine/">an essay on the School of Cybernetics
website</a>
which goes into more detail about how it works and what it all means (I mean,
it’s an artwork, so mostly I built it because I could, but some folks wanna know
“what’s the practical application of this stuff” so I wrote some words to give
plausible deniability).</p>

<p>Here’s a 90min video of every single stable-diffusion image generated over the
full 2 weeks of the exhibition. Not necessarily the sort of thing you’d sit down
and watch from start to finish, but fun to dip in and out of. Even at 8fps it’s
still possible to watch the gestalt change over time (and it’s also interesting
to see which prompts the humans in the room put in).</p>

<div style="width:100%; margin: 0px auto; margin-bottom: 1rem;">
  <div style="position: relative; padding-bottom: 62.5%; height: 0px;">
	<iframe style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%;" src="https://www.youtube.com/embed/_9gLWebIeeY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
	</iframe>
  </div>
</div>

<p>On the livecoding front, Ushini &amp; I did another <a href="https://www.ozchi.org/2022/creative_4.html">livecoding set at the OzCHI
2022 conference</a>—pics &amp; video on
the <a href="/livecoding/2022-12-01-ozchi-22.html">livecoding page</a>.</p>]]></content><author><name>Ben Swift</name></author><category term="livecoding" /><category term="ai" /><summary type="html"><![CDATA[If you follow me on twitter you’ve seen this already, but over the last couple of months I’ve been hard at work on an AI art installation called Panic. This was a collaboration with Adrian Schmidt—I did the software, he did the hardware (including the cool, 6” diameter Panic! button).]]></summary></entry><entry><title type="html">NGA Un-tour: The Hidden Gallery</title><link href="https://benswift.me/blog/2022/10/12/nga-un-tour-the-hidden-gallery/" rel="alternate" type="text/html" title="NGA Un-tour: The Hidden Gallery" /><published>2022-10-12T00:00:00+11:00</published><updated>2022-10-12T00:00:00+11:00</updated><id>https://benswift.me/blog/2022/10/12/nga-un-tour-the-hidden-gallery</id><content type="html" xml:base="https://benswift.me/blog/2022/10/12/nga-un-tour-the-hidden-gallery/"><![CDATA[<p>For the last year or so my team has been collaborating with the good folks at
<a href="https://www.bestfestivalever.com.au">BOHO Interactive</a> and the National Gallery
of Australia on something which launches tonight—the <em>NGA Un-Tour</em>.</p>

<p>From the <a href="https://nga.gov.au/events/un-tour-the-hidden-gallery/">NGA website</a>:</p>

<blockquote>
  <p>Experience an immersive cybernetic walking tour of the National Gallery after
dark. Travel through familiar and behind-the scenes spaces and investigate the
dynamic relationships between humans, technology and the built environment
found within works of art and the Gallery itself. Wear your walking shoes and
join us as we explore previously hidden parts of the building and gardens.</p>

  <p>Held of over one week during the Gallery’s 40th birthday celebrations. This
special event is co-presented by the National Gallery and the ANU School of
Cybernetics.</p>
</blockquote>

<p>I’m not sure if there are any tickets left, but if there are you should
definitely check it out. You won’t get another chance to experience one of
Canberra’s cultural institutions in this way for a long time.</p>]]></content><author><name>Ben Swift</name></author><category term="cybernetics" /><summary type="html"><![CDATA[For the last year or so my team has been collaborating with the good folks at BOHO Interactive and the National Gallery of Australia on something which launches tonight—the NGA Un-Tour.]]></summary></entry><entry><title type="html">So this is how it feels when the robots come for your job: what GitHub’s Copilot ‘AI assistant’ means for coders</title><link href="https://benswift.me/blog/2022/06/30/what-githubs-copilot-ai-assistant-means-for-coders/" rel="alternate" type="text/html" title="So this is how it feels when the robots come for your job: what GitHub’s Copilot ‘AI assistant’ means for coders" /><published>2022-06-30T00:00:00+10:00</published><updated>2022-06-30T00:00:00+10:00</updated><id>https://benswift.me/blog/2022/06/30/what-githubs-copilot-ai-assistant-means-for-coders</id><content type="html" xml:base="https://benswift.me/blog/2022/06/30/what-githubs-copilot-ai-assistant-means-for-coders/"><![CDATA[<p>I wrote <a href="https://theconversation.com/so-this-is-how-it-feels-when-the-robots-come-for-your-job-what-githubs-copilot-ai-assistant-means-for-coders-185957">a piece for the Conversation</a> about GitHub’s new
<em>Copilot</em> AI programming assistant. You can <a href="https://theconversation.com/so-this-is-how-it-feels-when-the-robots-come-for-your-job-what-githubs-copilot-ai-assistant-means-for-coders-185957">head over there</a> to
read it if you like.</p>]]></content><author><name>Ben Swift</name></author><category term="tools" /><category term="ai" /><summary type="html"><![CDATA[I wrote a piece for the Conversation about GitHub’s new Copilot AI programming assistant. You can head over there to read it if you like.]]></summary></entry><entry><title type="html">Redacting craiyon prompts with imagemagick</title><link href="https://benswift.me/blog/2022/06/22/redacting-craiyon-prompts-with-imagemagick/" rel="alternate" type="text/html" title="Redacting craiyon prompts with imagemagick" /><published>2022-06-22T00:00:00+10:00</published><updated>2022-06-22T00:00:00+10:00</updated><id>https://benswift.me/blog/2022/06/22/redacting-craiyon-prompts-with-imagemagick</id><content type="html" xml:base="https://benswift.me/blog/2022/06/22/redacting-craiyon-prompts-with-imagemagick/"><![CDATA[<p>I’ve been messing around with <a href="https://www.craiyon.com">craiyon</a> (formerly
<a href="https://huggingface.co/spaces/dalle-mini/dalle-mini">DALL-E mini</a>), because who
<em>hasn’t</em> been doing that recently.</p>

<p>As part of a workshop I’m running soon at the <a href="https://cybernetics.anu.edu.au">School of
Cybernetics</a> I need to provide “redacted”
versions of the classic 3x3 craiyon output image—and I need to do it for quite
a few outputs.</p>

<p>Because it’s tedious to do that by hand, here’s what I came up with:</p>

<ol>
  <li>
    <p>input a prompt and generate the craiyon output as normal</p>
  </li>
  <li>
    <p>use the <em>📷 Screenshot</em> button to get a nice, clean screenshot</p>
  </li>
  <li>
    <p>run this <a href="https://imagemagick.org">imagemagick</a> command (in my case the
downloaded screenshot name was <code>craiyon_2022-6-22_17-21-5.png</code>, yours will be
similar but with a different timestamp at the end)</p>

    <pre><code class="language-sh">convert craiyon_2022-6-22_17-21-5.png -fill red -draw 'rectangle 30, 240, 1320, 320' -fill white -pointsize 50 -gravity north -annotate +0+250 'REDACTED' craiyon_2022-6-22_17-21-5-redacted.png
</code></pre>
  </li>
  <li>
    <p>(bonus round) if you want to loop over a bunch of files and do it in batch, I
did that in Emacs with:</p>
    <pre><code class="language-lisp">(--each
    (f-entries "." (lambda (s) (s-ends-with? "png" s)))
  (shell-command (format "convert %s -fill red -draw 'rectangle 30, 240, 1320, 320' -fill white -pointsize 50 -gravity north -annotate +0+250 'REDACTED' redacted-%s.jpg"
                         it
                         (f-base it))))
</code></pre>
  </li>
</ol>

<h2 id="an-example">An example</h2>

<p>Here’s an example screenshot:</p>

<p><img src="/assets/images/posts/craiyon/craiyon_2022-6-22_17-21-5.png" alt="Grid of AI image outputs generated in response to the prompt &quot;redacting the prompt from a DALL-E image output with imagemagick&quot;" /></p>

<p>and the same output, after the redaction command has been run:</p>

<p><img src="/assets/images/posts/craiyon/redacted-craiyon_2022-6-22_17-21-5.jpg" alt="Grid of AI image outputs generated in response to the prompt, which has been redacted" /></p>

<p>If you need to do the same, then hopefully I’ve saved you a bit of time ☺</p>]]></content><author><name>Ben Swift</name></author><category term="tools" /><category term="ai" /><summary type="html"><![CDATA[I’ve been messing around with craiyon (formerly DALL-E mini), because who hasn’t been doing that recently.]]></summary></entry><entry><title type="html">Pulling apart Zoom attendance csv dumps in tidy R</title><link href="https://benswift.me/blog/2022/05/23/pulling-apart-zoom-attendance-csv-dumps-in-tidy-r/" rel="alternate" type="text/html" title="Pulling apart Zoom attendance csv dumps in tidy R" /><published>2022-05-23T00:00:00+10:00</published><updated>2022-05-23T00:00:00+10:00</updated><id>https://benswift.me/blog/2022/05/23/pulling-apart-zoom-attendance-csv-dumps-in-tidy-r</id><content type="html" xml:base="https://benswift.me/blog/2022/05/23/pulling-apart-zoom-attendance-csv-dumps-in-tidy-r/"><![CDATA[<p>My team ran some Zoom training last week and today I needed to figure out who
actually attended across all the days, and for how long.</p>

<p>Zoom can give you a csv dump of all attendees but doesn’t provide the
aggregations I was after, so I hacked up a little script (in <a href="https://www.tidyverse.org">tidy
R</a>) to do it. If you ever want to do something
similar, feel free to use it (<a href="https://mit-license.org">MIT Licence</a>).</p>

<pre><code class="language-R">read_zoom_attendance_csv = function(filename){
  read_csv(filename, show_col_types = FALSE) %&gt;%
    # I only needed the date, not the actual start time, so I didn't bother parsing
    # the full datetime + timezone string that Zoom gives
    mutate(date = parse_date(str_sub(`Join Time`, end = 10), format = "%m/%d/%Y")) %&gt;%
    # this isn't necessary, but I like shorter column names
    rename(name = `Name (Original Name)`, email = `User Email`, duration = `Duration (Minutes)` ) %&gt;%
    # I'm only intrested in these columns
    select(name, email, date, duration) %&gt;%
    # this isn't necessary, but handy if individuals have signed in with
    # slightly different names on different days (requires eyeballing the data)
    mutate(name = recode(name,
                         "JS" = "Joanna Smith",
                         "Louise" = "Louise Jones"
}
</code></pre>

<p>Then you can read the Zoom csv file like so:</p>

<pre><code class="language-R">df = read_zoom_attendance_csv("zoom-call.csv")
</code></pre>

<p>And to visualise in <a href="https://ggplot2.tidyverse.org">ggplot2</a> (which was my
reason for using R in the first place) you could try something like:</p>

<pre><code class="language-R">df %&gt;%
  group_by(name, date) %&gt;%
  # this was the key summarisation I was after - total time 
  # in-call across multiple connects/re-connects
  summarise(duration = sum(duration)) %&gt;%
  ungroup() %&gt;%
  ggplot(aes(name, duration)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~date, nrow = 1) +
  labs(
    title = "Zoom call attendance",
    x = "participant name",
    y = "duration on the call (minutes)"
  )
</code></pre>

<p>Enjoy!</p>]]></content><author><name>Ben Swift</name></author><category term="tools" /><summary type="html"><![CDATA[My team ran some Zoom training last week and today I needed to figure out who actually attended across all the days, and for how long.]]></summary></entry><entry><title type="html">Scar trees &amp;amp; living typefaces</title><link href="https://benswift.me/blog/2022/05/02/scar-trees-living-typefaces/" rel="alternate" type="text/html" title="Scar trees &amp;amp; living typefaces" /><published>2022-05-02T00:00:00+10:00</published><updated>2022-05-02T00:00:00+10:00</updated><id>https://benswift.me/blog/2022/05/02/scar-trees-living-typefaces</id><content type="html" xml:base="https://benswift.me/blog/2022/05/02/scar-trees-living-typefaces/"><![CDATA[<picture style="position: relative;">
  <img alt="Yellow box tree on ANU campus. 📸 ANU Photography, 2016" src="/assets/images/posts/20190103_H1P3978_scar_tree.jpg" />

  
</picture>

<p>I wrote something up for the ANU School of Cybernetics blog (where <a href="https://cybernetics.anu.edu.au/people/ben-swift/">I
work</a>) which has the same
energy as some of the stuff I post on here. If you’re interested, <a href="https://cybernetics.anu.edu.au/2022/05/02/scar-trees-living-typefaces/">go check it
out</a> ☺</p>]]></content><author><name>Ben Swift</name></author><category term="cybernetics" /><category term="web" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Teaching the world to Cybernetics</title><link href="https://benswift.me/blog/2022/03/07/teaching-the-world-to-cybernetics/" rel="alternate" type="text/html" title="Teaching the world to Cybernetics" /><published>2022-03-07T00:00:00+11:00</published><updated>2022-03-07T00:00:00+11:00</updated><id>https://benswift.me/blog/2022/03/07/teaching-the-world-to-cybernetics</id><content type="html" xml:base="https://benswift.me/blog/2022/03/07/teaching-the-world-to-cybernetics/"><![CDATA[<p>If you’re the sort of person who lurks on people’s LinkedIn accounts, you may
have noticed that I’ve recently become the <a href="https://cybernetics.anu.edu.au/people/ben-swift/">lead of the <em>Experiences</em>
Team</a> at the School of
Cybernetics. My team’s mission is:</p>

<blockquote>
  <p>to take Cybernetics to the world by providing encounters with cybernetics to
individuals and organisations who are (currently) not in our Masters/PhD
programs.</p>

  <p>…and to find ways to do it sustainably.</p>
</blockquote>

<p><img src="/assets/images/posts/alina-grubnyak-ZiQkhI7417A-unsplash.jpg" alt="a bunch of wires connected together in interesting geometric shapes" /></p>

<p>That might strike you as a pretty broad mission, for a couple of reasons:</p>

<ul>
  <li>what counts as an <em>encounter</em>? couldn’t that be anything?</li>
  <li>the world is a big and diverse place</li>
</ul>

<p>…and you’d be right. If you follow the rest of the communications from the
School of Cybernetics you’ll know that we’re not shy about painting a grand,
totalising vision of how the world can/might work and what part cybernetics has
to play in it. And while I’m a bit daunted by the task, I’m also pretty excited.</p>

<p>Here are some of the things my team will be working on over the next weeks/months/years:</p>

<ul>
  <li>
    <p>site-specific experiences based around specific places; uncovering the
cybernetic histories of some of the places we inhabit on the ANU campus and
beyond, and creating narrative experiences to share those histories with
others</p>
  </li>
  <li>
    <p>articulating many different answers to the question “what is cybernetics?” for
different audiences &amp; mediums (online/in-person, talks/workshops/happenings,
prosaic/poetic)</p>
  </li>
  <li>
    <p>creating a curriculum for interested partners to understand how the key ideas
of cybernetics can help them untagle (or at least manage) the complexity in
their businesses and other organisations</p>
  </li>
</ul>

<p>What I can say is that I want the things we make to be <strong>weird</strong>. If I ever turn
up in a tie and a blue suit and deliver an hours-long bullet-pointed PowerPoint
presentation<sup id="fnref:ppt" role="doc-noteref"><a href="#fn:ppt" class="footnote" rel="footnote">1</a></sup> on this stuff then you have my permission to point me back to
this post and ask me where it all went wrong. Otherwise, I look forward to
crossing paths with you and sharing an educational experience about cybernetics.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:ppt" role="doc-endnote">

      <p>Don’t get me wrong, PowerPoint (well, visual aids in general) can be super
effective as part of a multimedia presentation strategy, but the
<a href="https://twitter.com/add_hawk/status/1489001635779018754">affordances of the tool are such that it makes it hard to avoid the
pitfalls</a>. <a href="#fnref:ppt" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="cybernetics" /><summary type="html"><![CDATA[If you’re the sort of person who lurks on people’s LinkedIn accounts, you may have noticed that I’ve recently become the lead of the Experiences Team at the School of Cybernetics. My team’s mission is:]]></summary></entry><entry><title type="html">Running an AI neural style transfer model under Singularity</title><link href="https://benswift.me/blog/2022/02/01/running-an-ai-neural-style-transfer-model-under-singularity/" rel="alternate" type="text/html" title="Running an AI neural style transfer model under Singularity" /><published>2022-02-01T00:00:00+11:00</published><updated>2022-02-01T00:00:00+11:00</updated><id>https://benswift.me/blog/2022/02/01/running-an-ai-neural-style-transfer-model-under-singularity</id><content type="html" xml:base="https://benswift.me/blog/2022/02/01/running-an-ai-neural-style-transfer-model-under-singularity/"><![CDATA[<p>I’ve recently been given access to a beefy AI server (6x RTX3090s!) which is
managed via <a href="https://sylabs.io/singularity/">SingularityCE</a>, whose homepage
boldly asks and then forgets to answer the question: “What is SingularityCE?”</p>

<p>If you <a href="https://sylabs.io/guides/latest/user-guide/introduction.html">dig further into the
documentation</a>
it’s a little less coy:</p>

<blockquote>
  <p>SingularityCE is a container platform. It allows you to create and run
containers that package up pieces of software in a way that is portable and
reproducible. You can build a container using SingularityCE on your laptop,
and then run it on many of the largest HPC clusters in the world, local
university or company clusters, a single server, in the cloud, or on a
workstation down the hall. Your container is a single file, and you don’t have
to worry about how to install all the software you need on each different
operating system.</p>
</blockquote>

<p>I want to fire up my new GPUs and run one of Katherine Crowson’s awesome pytorch
scripts to do some <a href="https://github.com/crowsonkb/style-transfer-pytorch">neural style
transfer</a>. I’m very
familiar with Docker, but new to this Singularity thing, so here are some of the
hurdles I encountered (and cleared) along the way.</p>

<h2 id="finding-a-base-image">Finding a base image</h2>

<p>Looking in the style transfer repo’s
<a href="https://github.com/crowsonkb/style-transfer-pytorch/blob/master/setup.py#L23"><code>setup.py</code></a>,
it looks like <a href="https://pytorch.org">torch</a> v1.7.1 or later is required. Having
done this sort of thing before, I know that these deep learning frameworks
change a fair bit even between minor versions, so the safest option is to pick
the exact version that it was designed for—in this case v1.7.1 (or at least
v1.7.x).</p>

<p>So, the challenge is to find a Singularity image with that version of torch
installed. The singularity docs <a href="https://sylabs.io/guides/latest/user-guide/quick_start.html#download-pre-built-images">suggest using the search
command</a>
like so:</p>

<pre><code class="language-shell">$ singularity search torch

Found 34 container images for amd64 matching "torch":

	library://adalisan81/default/pytorch:latest

	library://aday651/default/pytorch-geometric-gpu:latest

	library://aphoh/default/pytorch-20.11-py3:v-1

	library://aradeva24/default/ar_pytorch_21.06-py3.sif:latest
		PyTorch NGC container with CUDA11.0, where PyTorch and apex are installed

	library://calebh/hpccm-test/faircluster-pytorch-1.10-cuda11.3:sha256.7c63a6c1f6f125b8d3e14fa10203965536ec7173d50e85b8c9ecf6ee0bff2ba7

	library://claytonm/default/ubuntu18_torch_torchvision_opencv_cuda10:latest

	library://dxtr/default/hpc-pytorch:0.1

	library://guoweihe/default/pytorch:hz1

	library://guoweihe/default/pytorch:v1.2

	library://guoweihe/default/torch:deep-ed

	library://guoweihe/default/torch:sha256.ff32c85ade2c8f6a1d34bd500de1b7bd11cdac16461aeef4d7cbd16ab129d8a7

	library://guoweihe/default/torchgpipe:master

	library://guoweihe/default/torchgpipe:sha256.a6ea5d732cba07c043e2f06cccbe541d28da6a8d9e5a3d18872d58af288dbc62

	library://ipa/medimgproc/pytorch:latest

	library://jamiechang917/default/pytorch:sha256.9c60c9825f20626cc0d6e69ac61d862bfec927e82d86becee73f853d657f2425

	library://lamastex/default/pytorch_21.03.sif:berzelius-20211027

	library://lamastex/default/pytorch_21.07.sif:berzelius-20211027

	library://lev-hpc/ml/pytorch_gpu:jupyter

	library://lev-hpc/ml/torch_tf_jupyter:latest

	library://mbalazs/default/pytorch:latest

	library://mbalazs/default/pytorch_cuda110:latest

	library://mike_holcomb/pytorchvision/v0.1.0:latest,v0.1.0

	library://oscartang/default/pytorch_translation:latest

	library://ottino8/default/pytorch:first

	library://pauldhein/hpc-deep-learning/torch-base:latest

	library://sina-ehsani/default/transformer-googlecrawl-torch-opencv:latest

	library://sina-ehsani/default/transformer-googlecrawl-torch1.10:latest

	library://skykiny/default/pytorch_skykiny:latest

	library://tmyoda/default/cuda-torch-pyenv:latest

	library://tru/default/c7-conda-pytorch-10.0:2019-07-12-2053,latest

	library://ufscar/hpc/cuda_pytorch:latest

	library://uvarc/default/pytorch:1.4.0-py37

	library://yboget/default/pytorch_rdkit_visdom:sha256.d97f221ef1294a8ef57d40cf7994d05d4955abc7cf39e3ce42faafd59fe3151a

	library://zhengtang/default/torch_translation:latest
</code></pre>

<p>Hmm. It’s hard to know which is official, which ones are going to work (a few of
them mention torch versions, but none of them are v1.7.x) and which ones might
even be malicious? That’s a worry.</p>

<p>Looking a bit deeper into the Singularity docs I find that one can <a href="https://sylabs.io/guides/latest/user-guide/singularity_and_docker.html">also use
Docker/OCI
images</a>,
and Singularity can pull them straight from Docker hub. That’s good news,
because NVIDIA do maintain <a href="https://hub.docker.com/r/pytorch/pytorch/tags">official Docker
images</a> for using torch with
NVIDIA graphics cards (like the 3090), so I find the <a href="https://hub.docker.com/layers/pytorch/pytorch/1.7.1-cuda11.0-cudnn8-runtime/images/sha256-db6086be92f439b918c96dc002f4cf40239e247f0b1b6c32e3fb36de70032bf9?context=explore">specific container image
for torch
v1.7.1</a>
and pull it down with:</p>

<pre><code class="language-shell">singularity pull docker://pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
</code></pre>

<h2 id="running-the-style_transfer-python-script">Running the <code>style_transfer</code> python script</h2>

<p>I’d already cloned the neural style transfer repo, so I can follow the
instructions in that
<a href="https://github.com/crowsonkb/style-transfer-pytorch/blob/master/README.md">README.md</a></p>

<pre><code class="language-shell">$ singularity shell pytorch_1.7.0-cuda11.0-cudnn8-runtime.sif 
Singularity&gt; cd style-transfer-pytorch/
Singularity&gt; pip install --user .
</code></pre>

<p>I got a bunch of warnings about certain things not being on the <code>$PATH</code>, but it
seems to finish installing everthing ok.</p>

<p>Continuing on with the instructions in the README, let’s try running this thing
(I’d downloaded a couple of image files to use as my <em>content</em> and <em>style</em> images).</p>

<pre><code class="language-shell">Singularity&gt; style_transfer ben.jpg tiger.jpg -o ben-tiger.jpg
bash: style_transfer: command not found
</code></pre>

<p>Hmm, looks like those <code>$PATH</code> warnings were prescient. Looking back, the exact
warning was:</p>

<pre><code class="language-shell">WARNING: The script normalizer is installed in '/home/users/ben/.local/bin' which is not on PATH
</code></pre>

<p>The quickest &amp; dirtiest fix for this is to add that <code>/bin</code> directory to my path
and try and re-run the script.</p>

<pre><code class="language-shell">Singularity&gt; PATH="$PATH:~/.local/bin" style_transfer ben.jpg tiger.jpg -o ben-tiger.jpg
</code></pre>

<p>And away it went! Several minutes later, it was done. Here are the original two images:</p>

<p><img src="/assets/images/headshots/headshot.jpg" alt="Original ben.jpg image" />
<img src="/assets/images/posts/tiger.jpg" alt="Original tiger.jpg" /></p>

<p>and here’s the output:</p>

<p><img src="/assets/images/posts/ben-tiger.jpg" alt="style-transferred ben-tiger.jpg" /></p>

<p>Success…ish. Clearly I need to keep tweaking parameters &amp; input images to come
up with an output that’s actually <em>good</em>, but at least that journey can now
begin.</p>

<h2 id="but-is-it-fast">But is it <em>fast</em>?</h2>

<p>Actually, that declaration of success is a bit premature. At the top of the
output I noticed that the script was running on the CPU, not the GPU.</p>

<pre><code class="language-shell">Singularity&gt; PATH="$PATH:~/.local/bin" style_transfer ben.jpg tiger.jpg -o ben-tiger.jpg
~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from https://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() &gt; 0
Using devices: cpu
CPU threads: 128
Loading model...
</code></pre>

<p>That’s really not ok—the whole point of running on this machine is to take
advantage of the GPUs. There could be lots of reasons for this, but I have a
hunch it has something to do with Singularity not allowing the script access to
the hardware. Sure enough, looking through the <a href="https://sylabs.io/guides/3.7/user-guide/gpu.html">Singularity GPU support
documentation</a> it turns out
there’s a magic <code>--nv</code> flag which must be passed when starting up the
Singularity session, so let’s do that.</p>

<pre><code class="language-shell">$ singularity shell --nv pytorch_1.7.1-cuda11.0-cudnn8-runtime.sif 
Singularity&gt; PATH="$PATH:~/.local/bin" style_transfer ben.jpg tiger.jpg -o ben-tiger.jpg
Using devices: cuda:0
~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:143: UserWarning: 
NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.
If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(incompatible_device_warn.format(device_name, capability, " ".join(arch_list), device_name))
GPU 0 type: NVIDIA GeForce RTX 3090 (compute 8.6)
GPU 0 RAM: 24268 MB
Loading model...

*error traceback intensifies*
</code></pre>

<p>Well, that’s progress. Looking through the output I can see</p>

<pre><code class="language-shell">Using devices: cuda:0
GPU 0 type: NVIDIA GeForce RTX 3090 (compute 8.6)
GPU 0 RAM: 24268 MB
</code></pre>

<p>so torch can now see the GPUs. However, the error message in the middle of that
output is now the problem:</p>

<blockquote>
  <p>NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the
current PyTorch installation.</p>

  <p>The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60
sm_70.</p>

  <p>If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check
the instructions at <a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></p>
</blockquote>

<p>Like I said earlier, torch/tensorflow/CUDA and deep learning frameworks in
general are really finnicky about versions. It’s tricky to get things up and
running so that (i) all the versions work together and (ii) the changes you make
don’t break the delicate version relationships between other deep learning
projects you want to run on the same system<sup id="fnref:singularity-isolation" role="doc-noteref"><a href="#fn:singularity-isolation" class="footnote" rel="footnote">1</a></sup>.</p>

<p>After a web search, it seems <a href="https://github.com/pytorch/vision/issues/4886">like others
</a> <a href="https://discuss.pytorch.org/t/geforce-rtx-3090-with-cuda-capability-sm-86-is-not-compatible-with-the-current-pytorch-installation/123499">have
had</a>
<a href="https://github.com/crowsonkb/style-transfer-pytorch/issues/1#issuecomment-769701949">similar
issues</a>,
although I tried all the approaches listed there and none of them worked.</p>

<h2 id="using-a-pytorch-image-from-nvidias-container-registry">Using a pytorch image from NVIDIA’s container registry</h2>

<p>Changing tack a bit (after a suggestion from a colleague) I decided to try using
a (Docker) <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch">container image from the NVIDIA
registry</a>, rather
than the official pytorch channel on Docker Hub.</p>

<pre><code class="language-shell">$ singularity pull docker://nvcr.io/nvidia/pytorch:22.01-py3
$ singularity shell --nv pytorch_22.01-py3.sif 
Singularity&gt; pip install --user .
</code></pre>

<p>Now, let’s try running the <code>style_trasfer</code> script one more time:</p>

<pre><code class="language-shell">Singularity&gt; PATH="$PATH:~/.local/bin" style_transfer ben.jpg tiger.jpg -o ben-tiger.jpg
Using devices: cuda:0
GPU 0 type: NVIDIA GeForce RTX 3090 (compute 8.6)
GPU 0 RAM: 24268 MB
Loading model...
Processing content image (128x85)...
</code></pre>

<p>Hooray! It works, and runs, like 10000x faster on the GPU.</p>

<h2 id="open-questions">Open questions</h2>

<p>I really was just “hacking it until it worked” during this process, so I have a
few open questions.</p>

<ul>
  <li>
    <p>What’s the “persistance” story with the singularity images (<code>*.sif</code> files)? Is
it like docker, where I <code>singularity shell</code> in, do some things, but then any
changes I make in the shell (container?) don’t persist? It doesn’t seem like
that… but need to have a better mental model of how singularity images work.</p>
  </li>
  <li>
    <p>I didn’t use <a href="https://virtualenv.pypa.io/en/latest/">venvs</a> or
<a href="https://docs.conda.io/en/latest/">conda</a> or
<a href="https://python-poetry.org/docs/">poetry</a> or any of the things I’d usually use
when python-ing on my own machine, partially because of my above questions
about how the whole singularity shell thing actually works. I just did <code>pip
install --user .</code> and hoped it didn’t break anything else. Is that ok? Or
should I still use venvs in the singularity image?</p>
  </li>
</ul>

<p>I will return and try and better understand these things later, but right now
this isn’t on the critical path for me so I’ll have to park it. This blog post
is really just me opening a ticket for myself to return to later. I share it so
that you, dear reader, can also benefit from my mistakes (and if you know of
better ways to do any of this then do <a href="mailto:ben.swift@anu.edu.au">drop me a
line</a>.</p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:singularity-isolation" role="doc-endnote">

      <p>I had hoped that Singularity might help with the “isolation” part of this,
but I’m not sure I understand it well enough yet to know how to do it. <a href="#fnref:singularity-isolation" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="tools" /><category term="ai" /><summary type="html"><![CDATA[I’ve recently been given access to a beefy AI server (6x RTX3090s!) which is managed via SingularityCE, whose homepage boldly asks and then forgets to answer the question: “What is SingularityCE?”]]></summary></entry></feed>