<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://benswift.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://benswift.me/" rel="alternate" type="text/html" /><updated>2025-09-18T10:18:53+10:00</updated><id>https://benswift.me/feed.xml</id><title type="html">benswift.me</title><subtitle>livecoder &amp; researcher homepage - code, creativity, culture</subtitle><author><name>Ben Swift</name></author><entry><title type="html">The great 2025 email yak-shave: O365 + mbsync + mu + neomutt + msmtp</title><link href="https://benswift.me/blog/2025/09/12/the-great-2025-email-yak-shave-o365-mbsync-mu-neomutt-msmtp/" rel="alternate" type="text/html" title="The great 2025 email yak-shave: O365 + mbsync + mu + neomutt + msmtp" /><published>2025-09-12T00:00:00+10:00</published><updated>2025-09-12T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/09/12/the-great-2025-email-yak-shave-o365-mbsync-mu-neomutt-msmtp</id><content type="html" xml:base="https://benswift.me/blog/2025/09/12/the-great-2025-email-yak-shave-o365-mbsync-mu-neomutt-msmtp/"><![CDATA[<p>For years I was a happy user of
<a href="https://www.djcbsoftware.nl/code/mu/mu4e.html">mu4e</a> in Emacs. But then a few
years ago my employer turned off password-based IMAP auth and broke my (Office
365-based) work email, so I had to make alternative email arrangements.</p>

<p>I’ve recently rebuilt my entire email setup around neomutt (in Zed’s built-in
terminal). I always knew that there was <em>some</em> way to do the Office365 OAuth2
dance and hook things back up, so I took the plug and shaved the email yak
again. And here, dear reader, are the results—may you not waste as many hours
messing around as I did.</p>

<h2 id="the-moving-parts">The moving parts</h2>

<p>The new setup consists of:</p>

<ul>
  <li><strong>mbsync</strong> (built from source with SASL support) for IMAP sync with OAuth2</li>
  <li><strong>cyrus-sasl-xoauth2</strong> mbsync plugin to handle the OAuth dance</li>
  <li><strong>mu</strong> for fast email search and indexing</li>
  <li><strong>neomutt</strong> as the email client</li>
  <li><strong>msmtp</strong> for SMTP sending</li>
  <li><strong>macOS Keychain</strong> for secure token storage</li>
</ul>

<p>Each tool does one thing well, which is the Unix way—even if it means more
configuration files to maintain.</p>

<h2 id="oauth2-the-tricky-bit">OAuth2: the tricky bit</h2>

<p>Getting OAuth2 working with Office365 was the gnarliest part. You need to use
the <code>mutt_oauth2.py</code> script with Thunderbird’s client ID
(<code>9e5f94bc-e8a4-4e73-b8be-63364c29d753</code>) and the devicecode flow, since
localhostauthcode doesn’t work with the way my O365 exchange server is set up.</p>

<p>Here’s a snippet from my mbsyncrc showing how the OAuth token gets passed:</p>

<pre><code>IMAPAccount anu
Host outlook.office365.com
Port 993
AuthMech XOAUTH2
User ben.swift@anu.edu.au
PassCmd "/Users/ben/.dotfiles/mail/mutt_oauth2.py \
  --decryption-pipe 'security find-generic-password -a ben.swift@anu.edu.au -s mutt_oauth2_anu -w' \
  --encryption-pipe '/Users/ben/.dotfiles/mail/keychain-store.sh ben.swift@anu.edu.au mutt_oauth2_anu' \
  /Users/ben/.dotfiles/mail/anu_oauth2_keychain_stub"
</code></pre>

<p>The <code>keychain-store.sh</code> wrapper script ensures tokens are stored securely in
macOS Keychain rather than sitting around in plaintext files. If you’re on Linux
you can switch my macOS-specific approach with suitable <code>pass</code> or <code>gpg</code>
invocations.</p>

<p class="hl-para">I needed to build mbsync and the cyrus-sasl-xoauth2 plugin from source with
XOAUTH2 support (something I plan to upstream to the homebrew formula when I get
a chance).</p>

<h2 id="running-in-zed">Running in Zed</h2>

<p>Since I’m a Zed user, I run neomutt in a fullscreen terminal task (same approach
as my <a href="/blog/2025/07/23/running-claude-code-within-zed/">Claude Code
setup</a>). Add
this to your tasks.json:</p>

<pre><code class="language-json">{
  "label": "mutt",
  "command": "neomutt",
  "reveal": "always",
  "use_new_terminal": true,
  "allow_concurrent_runs": false
}
</code></pre>

<p>I bind this task to a keyboard shortcut, then I’m one key command away from a
fullscreen email client with all the Zed terminal niceties.</p>

<h2 id="the-payoff">The payoff</h2>

<p>Yes, it was a yak-shave. But now I have:</p>

<ul>
  <li>full control over my email workflow</li>
  <li>lightning-fast search with mu (I tried notmuch, but the mu setup allows me to
use normal IMAP folders—and that’s important because I have to check my
email from multiple devices)</li>
  <li>OAuth2 working seamlessly with Office365</li>
  <li>everything running in my preferred editor</li>
  <li>the tantalising prospect of replacing <em>all</em> the email parts of my job with a
series of shell scripts (and claude code invocations)</li>
</ul>

<p>For the full config files and detailed setup instructions, check out
<a href="https://github.com/benswift/.dotfiles/tree/main/mail/">my full email config on GitHub</a>.
Fair warning: you’ll probably need to tweak things for your specific setup, but
that’s half the fun.</p>]]></content><author><name>Ben Swift</name></author><category term="tools" /><summary type="html"><![CDATA[For years I was a happy user of mu4e in Emacs. But then a few years ago my employer turned off password-based IMAP auth and broke my (Office 365-based) work email, so I had to make alternative email arrangements.]]></summary></entry><entry><title type="html">Running Claude Code within Zed</title><link href="https://benswift.me/blog/2025/07/23/running-claude-code-within-zed/" rel="alternate" type="text/html" title="Running Claude Code within Zed" /><published>2025-07-23T00:00:00+10:00</published><updated>2025-07-23T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/07/23/running-claude-code-within-zed</id><content type="html" xml:base="https://benswift.me/blog/2025/07/23/running-claude-code-within-zed/"><![CDATA[<p>After several months using Zed’s built-in
<a href="https://zed.dev/docs/ai/agent-panel">Agent panel</a> I’ve switched to Claude Code.
The main reason is cost. I’m still using the same models—still primarily
Sonnet (and occasionally Opus) 4—but using them via the Agent panel incurs
per-token billing via the Anthropic API. I was racking up a lot of costs (around
100USD/week) even with a <em>bit</em> of restraint, although still using it fairly
heavily during all work days.</p>

<p>Claude Code allows one to pay a flat-fee subscription (I’m now nearly through my
first month of <a href="https://www.anthropic.com/max">MAX 20x</a>, so I’m already ahead).
And it’s <em>really nice</em> to have the flat fee; it does change the way you use the
agent (and sub-agents).</p>

<p>One downside is that you have to use their
<a href="https://docs.anthropic.com/en/docs/claude-code/overview"><code>claude</code> CLI tool</a>.
Since I still want to stay in Zed (even if I can’t use the Agent panel) I’ve
added a task which gets me pretty close to the same workflow I had before. Add
this to your zed <code>tasks.json</code>:</p>

<pre><code class="language-json">{
  "label": "claude",
  "command": "claude --dangerously-skip-permissions",
  "reveal": "always",
  "use_new_terminal": true,
  "allow_concurrent_runs": false
}
</code></pre>

<p>Anectodally, it also seems like Claude Code is a bit more token-efficient than
using the same models via the Zed Agent panel. To my eyes (watching the agent at
work) it seems like it’s more parsimonious in only reading sections of files
into context, and whole files only when absolutely necessary. But I don’t have
hard data to back that up.</p>

<p>And the workflow is really almost as good, with only two things that I’m really
missing from my old workflow.</p>

<ol>
  <li>
    <p>You can’t trivially switch providers/models like you can when you’re using
the Zed Agent panel. But the Anthropic models are pretty good—at least
equal to best-in-class for the sort of work I do.</p>
  </li>
  <li>
    <p>Claude code no longer has direct access to the LSP diagnostics—because that
stuff in general Just Works^TM in Zed, it was nice to not have to set up
extra MCPs to get access to the language-aware tooling stuff. I used to be
able to just say “fix the warnings in this project” and it’d do what I meant,
but now I have to either set up a MCP server, or hope that Claude (possibly
with a hint in <code>CLAUDE.md</code>) knows how to run the CLI tools to get the same
information.</p>
  </li>
</ol>

<p>The thing that puts me at ease with this sort of (big) workflow chnage is that
there’s really not too much lock-in; if the pricing landscape changes in the
future I’d happily switch to another provider. The main skill—context
management, and figuring out how to communicate to an LLM what needs to be done
in clear, concise language—is unchanged.</p>

<p>If this helps you get the most out of your setup, then happy Claude-in-Zedding.</p>

<h2 id="update-2025-09-18">Update (2025-09-18)</h2>

<p>I’ve since wrapped the Claude CLI in a tmux session to make the workflow even
smoother. This means:</p>

<ul>
  <li>each project gets its own persistent Claude session that keeps running after
Zed is closed (especially useful for remote Zed sessions where you want the
agent to keep working)</li>
  <li>you can easily reconnect to an existing Claude conversation when switching
between projects</li>
  <li>the session names automatically match your project directory names</li>
</ul>

<p>The wrapper script (<code>claude-tmux</code>) and updated Zed task configuration are
<a href="https://github.com/benswift/.dotfiles">in my dotfiles</a> if you want to steal
them.</p>]]></content><author><name>Ben Swift</name></author><category term="ai" /><category term="tools" /><category term="zed" /><summary type="html"><![CDATA[After several months using Zed’s built-in Agent panel I’ve switched to Claude Code. The main reason is cost. I’m still using the same models—still primarily Sonnet (and occasionally Opus) 4—but using them via the Agent panel incurs per-token billing via the Anthropic API. I was racking up a lot of costs (around 100USD/week) even with a bit of restraint, although still using it fairly heavily during all work days.]]></summary></entry><entry><title type="html">Agentic AI: LLMs with stones</title><link href="https://benswift.me/blog/2025/07/17/agentic-ai-llms-with-stones/" rel="alternate" type="text/html" title="Agentic AI: LLMs with stones" /><published>2025-07-17T00:00:00+10:00</published><updated>2025-07-17T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/07/17/agentic-ai-llms-with-stones</id><content type="html" xml:base="https://benswift.me/blog/2025/07/17/agentic-ai-llms-with-stones/"><![CDATA[<blockquote>
  <p>Sticks and stones may break my bones, but words will never hurt me.</p>
</blockquote>

<p>There’s a truth to that proverb, even if you feel (as I do) the temptation to
“well akshually…” make several very valid points about how words <em>can</em> be
hurtful. For most of the Large Language Model (LLM) era—since the public
release of ChatGPT in November 2022—we’ve been in turns amazed, disgusted and
now kindof “meh” about the way that LLMs can take the <strong>words</strong> we give them and
produce <strong>more words</strong> in response.</p>

<p>Working as I do as an
<a href="https://cybernetics.anu.edu.au/people/ben-swift/">academic computer scientist</a>
(with a research background in AI) who regularly runs executive education
courses on AI for a diverse range of educated and intelligent folks, I’m getting
more and more questions about “agentic AI”. And while definitions and
descriptions change pretty quick in this field at the moment, I want to
demistify some things about this term in particular.</p>

<p>Agentic AI (as concieved and talked about in this present moment) is about
connecting LLMs—“pure” input/output text sausage machines—to the world with
<strong>tools</strong>. These tools they can use do stuff beyond just returning words in a
text box on a web page. To return to the “sticks and stones” aphorism above:
agentic AI means giving an LLM a stone.</p>

<p>Here’s how it works in practice:</p>

<ul>
  <li>you put in a prompt as normal which is sent to the LLM</li>
  <li>in addition to that prompt, though, the LLM is sent a list of tools that you
have access to (including human-readable descriptions of what they can do),
for example:
    <ul>
      <li><code>calendar</code>: add, update, or delete events in your calendar</li>
      <li><code>weather</code>: check the weather forecast for a given location</li>
      <li><code>fire-ze-missiles</code>: launch a missile at a target location</li>
    </ul>
  </li>
  <li>instead of only being able to respond with text, the LLM can
respond<sup id="fnref:tool-call-llm"><a href="#fn:tool-call-llm" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> with a “tool call” instruction, to continue the
example:
    <ul>
      <li>use tool <code>calendar</code> to “add a meeting with John at 10am tomorrow”</li>
      <li>use tool <code>weather</code> to “check the weather forecast for Sydney next Tuesday”</li>
      <li>use tool <code>fire-ze-missiles</code> to “launch a missile at coordinates 40.7128,
-74.0060” . it can say “add a meeting to the calendar at 10am tomorrow using
the <code>calendar</code> tool”</li>
    </ul>
  </li>
</ul>

<p>If the LLM requests a tool call, the user doesn’t need to do anything; the
system will use the tool as requested by the LLM and return the results (usually
the fact that this is happening is communicated to the user via some sort of
visual feedback in the interface, although this isn’t a requirement).</p>

<p>Vocab-wise, this all started with OpenAI introducing “function calling” to GPT
models in
<a href="https://openai.com/index/function-calling-and-other-api-updates/">June 2023</a>.
However it wasn’t until late 2024 that the term “agentic” really took off,
coinciding with Anthropic’s release of the
<a href="https://www.anthropic.com/news/model-context-protocol">Model Context Protocol</a>,
a standardised and interoperable way for other parties (not just the LLM
providers) to create tools which all LLMs could use. That term is just riffing
on the “agency” sense of the world, where LLMs are given the means of acting in
the world. But agentic AI, tools, and tool/function-calling LLMs—it’s all the
same general idea.</p>

<p>In the last couple of years there have been a proliferation of such tools. Some
of them are really general, e.g. “search the web for …”. Some of them might be
really specific to your company, e.g. a tool that maps names to phone numbers in
your company’s database. In this case they’re useful because they’re <em>not</em>
LLM-powered (and so they don’t just make stuff up if they don’t know the
answer).</p>

<p>Software developers (including me) in particular have found ways to use tools to
help them write code. There have been many recent blog posts of various
developers describing how they set up their agentic AI (tool-calling LLM)
systems—from
<a href="https://www.philschmid.de/context-engineering">Phil Schmid’s “Context Engineering”</a>
to
<a href="https://fly.io/blog/youre-all-nuts/">Thomas Ptacek’s “My AI Skeptic Friends Are All Nuts”</a>
to countless Hacker News threads debating whether this is all just hype.</p>

<h2 id="so-is-this-a-big-deal">So is this a big deal?</h2>

<p>From a cybernetic perspective, this isn’t quite as big a change as you might
think. Because even the original ChatGPT could “do stuff in the world” by
telling <em>you</em> (the human user) to do it. Sometimes that was as benign as having
you copying text into an email. We’d still colloquially refer to this as
“answering my emails with ChatGPT”, but actually all ChatGPT was doing was
giving you <strong>words</strong> to type into your email client and hit “send”. Sometimes
the LLM’s words told us to do more life-impacting things, like
<a href="https://www.vice.com/en/article/we-asked-chatgpt-how-to-break-up-with-someone/">break up with your partner</a>,
or worse. Whenever LLMs are used by humans they have the (indirect) ability to
affect the world.</p>

<p>In my opinion the best way to think about this shift isn’t that LLMs can now
influence the world; it’s that now they can do it without asking—and this
tightens the feedback loop. This means:</p>

<ol>
  <li>first, there’s now no longer a human in the loop (so now there’s no human to
say “hey, that’s a dumb idea” and refuse to do it)</li>
  <li>as a consequence of #1, LLMs can now run/iterate without intervention for
much longer (minutes, maybe even hours…)</li>
</ol>

<p>The second point is the bigger deal (and it’s a point that Anthropic, the makers
of the Claude LLM which is one of the big players these days, makes in their
<a href="https://www.anthropic.com/engineering/building-effective-agents">recent whitepaper about agentic AI</a>).</p>

<p>Humans were always a) able to do things in the environment, and b) the
bottleneck in any LLM system (time-wise, at least). But by gaining the former
capability, agentic AI removes the latter bottleneck.</p>

<p>So if you’re going to allow your LLMs to use tools, you <strong>must</strong> be certain that
you’re comfortable with what things the LLM can do with them. Both in theory,
i.e. in the sense of what’s possible, but also in practice, i.e. through testing
the way that your particular LLM tends to use your tools given specific prompts
or other context. How exactly you do that is a topic for another blog post (just
kidding, it’s a much bigger question than that that depends on a whole bunch of
things). But I think that’s the right question to be asking when it comes to
agentic AI.</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:tool-call-llm">

      <p>The LLM needs to be specially trained to support this, but all of the main
ones do these days. <a href="#fnref:tool-call-llm" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="ai" /><summary type="html"><![CDATA[Sticks and stones may break my bones, but words will never hurt me.]]></summary></entry><entry><title type="html">Automated RPi Web Kiosk Setup in 2025</title><link href="https://benswift.me/blog/2025/07/16/automated-rpi-web-kiosk-setup-in-2025/" rel="alternate" type="text/html" title="Automated RPi Web Kiosk Setup in 2025" /><published>2025-07-16T00:00:00+10:00</published><updated>2025-07-16T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/07/16/automated-rpi-web-kiosk-setup-in-2025</id><content type="html" xml:base="https://benswift.me/blog/2025/07/16/automated-rpi-web-kiosk-setup-in-2025/"><![CDATA[<p>As part of a <a href="https://github.com/anucybernetics/panic">recent art installation</a>
I’ve needed to set up lots (well, dozens) of Raspberry 5s to run as fullscreen
Chromium “kiosks” with a pre-set URL (network connected, but with no
keyboard/mouse).</p>

<p>They’ve all needed slightly different kiosk URLs, and I <em>hate</em> doing this sort
of busy-work by hand. So I’ve spent longer than I’d like to admit<sup id="fnref:time"><a href="#fn:time" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> putting
together a fully scripted burn-and-boot process. My non-negotiables were:</p>

<ul>
  <li>script needs to be fully automated (though it turns out it only works on Linux
now due to the tools required)</li>
  <li>once the SD card is put into an rpi and plugged in, everything should be
automatic (join network, install and configure software and OS)</li>
  <li>I need a way to remotely access the rpis once “in the field” (in particular,
sometimes I need to change their kiosk URL)</li>
</ul>

<p>I <em>thought</em> that this would be a pretty common thing that others would have
done—rpis are cheap and seem like a good fit for this type of “web browser as
installation display” thing. But I kept running into dead ends.</p>

<p>I finally got it fully working with
<a href="https://www.raspberrypi.com/software/">Raspberry Pi OS</a> (Bookworm) as the OS
and <a href="https://github.com/labwc/labwc">labwc</a> as the compositor (which is the
default on RPi OS). To save you the trouble, dear reader, I’ve packaged it all
up into a script (which works as of the date of this post—July 2025) and put
it <a href="https://github.com/ANUcybernetics/panic/tree/main/rpi">here</a>. From that
folder’s <code>README.md</code>:</p>

<blockquote>
  <p>This directory contains scripts for setting up Raspberry Pi 5 devices as
browser kiosks that boot directly into fullscreen Chromium displaying a
specified URL.</p>

  <p>The setup process uses:</p>

  <ul>
    <li><strong>install-sdm.sh</strong> - Installs SDM (SD Card Image Management tool) - one-time
setup</li>
    <li><strong>pi-setup.sh</strong> - Creates customized Raspberry Pi OS SD cards with kiosk
mode</li>
  </ul>

  <p>The resulting installation:</p>

  <ul>
    <li>boots directly into GPU-accelerated kiosk mode using labwc Wayland
compositor (RPi OS default)</li>
    <li>automatically joins your Tailscale network</li>
    <li>supports native display resolutions including 4K at 60Hz</li>
    <li>configures WiFi (WPA2 and enterprise 802.1X)</li>
    <li>includes HDMI audio support</li>
    <li>optimized specifically for Raspberry Pi 5</li>
    <li>provides a <code>kiosk-set-url</code> utility for easy URL changes</li>
    <li>uses official Raspberry Pi OS Bookworm (64-bit)</li>
  </ul>
</blockquote>

<p>I hope this saves you some time. Use it to hug your loved ones.</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:time">
      <p>I thought it’d take a day, it’s taken about a week, on and off :( <a href="#fnref:time" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="tools" /><summary type="html"><![CDATA[As part of a recent art installation I’ve needed to set up lots (well, dozens) of Raspberry 5s to run as fullscreen Chromium “kiosks” with a pre-set URL (network connected, but with no keyboard/mouse).]]></summary></entry><entry><title type="html">DIYChatGPT Short Course for ANU Undergraduates</title><link href="https://benswift.me/blog/2025/07/07/diychatgpt-short-course-for-anu-undergraduates/" rel="alternate" type="text/html" title="DIYChatGPT Short Course for ANU Undergraduates" /><published>2025-07-07T00:00:00+10:00</published><updated>2025-07-07T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/07/07/diychatgpt-short-course-for-anu-undergraduates</id><content type="html" xml:base="https://benswift.me/blog/2025/07/07/diychatgpt-short-course-for-anu-undergraduates/"><![CDATA[<p>If you’re an ANU undergraduate student you can enrol in an upcoming (next week!)
<a href="https://mccuskerinstitute.anu.edu.au/study/knots/diy-chatgpt-llms-as-information-processing-machines/">upcoming 3hr course</a>
I’ve created called <em>DIY ChatGPT: LLMs as Information Processing Machines</em>. It’s
running for the first time <strong>next Thursday 17 July 10am–1pm</strong> (although it will
run again later in the semester).</p>

<p>Here’s the blurb:</p>

<blockquote>
  <p>In this hands-on workshop you will train and use your own language model from
scratch - with just pen and paper and a bit of dice rolling. Through
interactive exercises and guided discussions, you’ll see how language models
(even large language models like ChatGPT) are fundamentally information
processing machines, turning language inputs into language outputs. The
workshop builds from basic principles to more complex applications through an
exploration of language modeling as a probabilistic process of predicting
“what token comes next” in a sequence, and ends with a poetry slam (for real).</p>
</blockquote>

<p>This course is a
<a href="https://mccuskerinstitute.anu.edu.au/study/knots_explained/">“Know the Nature Of Things” KNoT</a>
offered as part of the ANU’s
<a href="https://mccuskerinstitute.anu.edu.au/">McCusker Institute</a>. They’re for-credit
courses run by different folks all across campus, and a great way to explore
some of the cool stuff going on at ANU.</p>]]></content><author><name>Ben Swift</name></author><summary type="html"><![CDATA[If you’re an ANU undergraduate student you can enrol in an upcoming (next week!) upcoming 3hr course I’ve created called DIY ChatGPT: LLMs as Information Processing Machines. It’s running for the first time next Thursday 17 July 10am–1pm (although it will run again later in the semester).]]></summary></entry><entry><title type="html">Agentic Elixir superpowers: Zed + Tidewave + AshAI</title><link href="https://benswift.me/blog/2025/06/06/agentic-elixir-superpowers-zed-tidewave-ashai/" rel="alternate" type="text/html" title="Agentic Elixir superpowers: Zed + Tidewave + AshAI" /><published>2025-06-06T00:00:00+10:00</published><updated>2025-06-06T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/06/06/agentic-elixir-superpowers-zed-tidewave-ashai</id><content type="html" xml:base="https://benswift.me/blog/2025/06/06/agentic-elixir-superpowers-zed-tidewave-ashai/"><![CDATA[<p>For a few years now whenever I need to build any sort of networked interactive
experience, I reach for Elixir (with Phoenix LiveView and Ash). It’s an
<em>extremely</em> productive combination, especially when you want to do more complex
client/server information flows than the standard request/response UX. I’ve also
(for the last year or two) ditched Emacs for <a href="https://zed.dev">Zed</a>. Again, if
you take the time to master these tools I think they’re excellent.</p>

<p>Like every indie hacker and their wombat I’m experimenting with LLM Agents as
part of my software development workflow. The new frontier (which I’m excited
about, but ask me in a few months how it’s gone) is putting all of the above
together. If you’d like to do that too, and want to see a real-world example of
how these things can be plugged into one another, here’s my setup.</p>

<ul>
  <li>Zed as the text editor/MCP host</li>
  <li>both <a href="https://hexdocs.pm/tidewave/zed.html">Tidewave</a> and
<a href="https://hexdocs.pm/ash_ai/readme.html">AshAI</a> as MCP servers</li>
</ul>

<p>Then, my Zed settings file has this MCP server configuration:</p>

<pre><code class="language-json">"context_servers": {
  "ash_ai": {
    "command": {
      "path": "mcp-proxy",
      "args": ["http://localhost:4000/ash_ai/mcp"],
      "env": {}
    },
    "settings": {}
  },
  "tidewave": {
    "command": {
      "path": "mcp-proxy",
      "args": ["http://localhost:4000/tidewave/mcp"],
      "env": {}
    },
    "settings": {}
  }
},
</code></pre>

<p>Even after following the instructions for those tools, you’ll need a way to set
up all the proxies and pipe everything together. I wrote
<a href="https://github.com/benswift/.dotfiles/blob/master/bin/tidewave-proxy.sh">this script</a>,
which you’re free to use (MIT Licence) if it helps.</p>

<p>Put it on your <code>~PATH</code> and then (in your Phoenix project root) run it like so:</p>

<pre><code>[16:03] daysy:panic $ tidewave-proxy.sh
🚀 MCP Proxy Development Environment
====================================
Configuration:
  Host: localhost
  Port: 4000
  Ash AI MCP: http://localhost:4000/ash_ai/mcp
  Tidewave MCP: http://localhost:4000/tidewave/mcp

🔗 Starting mcp-proxy instances...
Starting mcp-proxy for ash_ai...
✅ mcp-proxy for ash_ai started (PID: 88815)
   Log file: /tmp/mcp_ash_ai.log
Starting mcp-proxy for tidewave...
✅ mcp-proxy for tidewave started (PID: 88816)
   Log file: /tmp/mcp_tidewave.log

🎉 MCP Proxy environment is ready!
=====================================

Available MCP Servers:
  📊 Ash AI MCP:    http://localhost:4000/ash_ai/mcp
  🌊 Tidewave MCP:  http://localhost:4000/tidewave/mcp

MCP Proxy Commands:
  For Ash AI:    mcp-proxy http://localhost:4000/ash_ai/mcp
  For Tidewave:  mcp-proxy http://localhost:4000/tidewave/mcp

Next Steps:
1. Configure your MCP client (Zed, Claude Desktop, etc.)
2. Use the endpoints above with your MCP client
3. Test with: curl -H 'Accept: application/json' &lt;endpoint&gt;

Log Files:
  Ash AI Proxy: /tmp/mcp_ash_ai.log
  Tidewave Proxy: /tmp/mcp_tidewave.log

Press Ctrl+C to stop all services and clean up

🔥 Starting Phoenix development server with REPL...
==================================================
Erlang/OTP 27 [erts-15.2.5] [source] [64-bit] [smp:16:16] [ds:16:16:10] [async-threads:1] [jit]

[info] Migrations already up
[info] Running PanicWeb.Endpoint with Bandit 1.7.0 at 127.0.0.1:4000 (http)
[info] Access PanicWeb.Endpoint at http://localhost:4000
Interactive Elixir (1.18.4) - press Ctrl+C to exit (type h() ENTER for help)
[watch] build finished, watching for changes...

Rebuilding...

Done in 196ms.
</code></pre>

<p>Then, to test, in the Agent panel try something like:</p>

<pre><code>Use the tidewave project eval tool to add 10+15 in Elixir.
</code></pre>

<p>If everything’s hooked up right, you’ll get the answer.</p>

<p>One other tip: make sure you’re using OTP27 rather than the (newest)
OTP28—there’s an error on one of the AshAI deps which stops it compiling on
the latest OTP. I suspect it’ll be fixed soon, though.</p>]]></content><author><name>Ben Swift</name></author><category term="elixir" /><category term="web" /><category term="zed" /><summary type="html"><![CDATA[For a few years now whenever I need to build any sort of networked interactive experience, I reach for Elixir (with Phoenix LiveView and Ash). It’s an extremely productive combination, especially when you want to do more complex client/server information flows than the standard request/response UX. I’ve also (for the last year or two) ditched Emacs for Zed. Again, if you take the time to master these tools I think they’re excellent.]]></summary></entry><entry><title type="html">LLM Parlour Games for Overeducated Wankers</title><link href="https://benswift.me/blog/2024/11/07/llm-parlour-games-for-overeducated-wankers/" rel="alternate" type="text/html" title="LLM Parlour Games for Overeducated Wankers" /><published>2024-11-07T00:00:00+11:00</published><updated>2024-11-07T00:00:00+11:00</updated><id>https://benswift.me/blog/2024/11/07/llm-parlour-games-for-overeducated-wankers</id><content type="html" xml:base="https://benswift.me/blog/2024/11/07/llm-parlour-games-for-overeducated-wankers/"><![CDATA[<p><em>Note: this stuff is the workshop content for an alumni workshop in November
2024 hosted by the Cybernetic Studio at the ANU School of Cybernetics.</em></p>

<h2 id="abstract">Abstract</h2>

<p>In this interactive design session participants will design and prototype their
own language-model-based parlour game. We’ll think critically about what
language models are (and aren’t) and what they’re good for (and rubbish at).
You’ll interact with other humans and design systems with goals and guardrails,
and think about what it means to give input to (and understand output from) LLMs
and genAI systems.</p>

<p>Prerequisites: ability to lounge around and use big words to impress your
friends in games of no stakes whatsoever. Self-satisfied smugness about said
loquaciousness is helpful but not essential.</p>

<h2 id="tech-note">Tech note</h2>

<p>This workshop requires access to a chat-based LLM (e.g. ChatGPT). If you’ve got
a laptop (or even a phone, although you’ll be typing on your janky little phone
keyboard) you can head to <a href="https://chatgpt.com">https://chatgpt.com</a> (no sign-up required). But if
you’ve got a different favourite chat-based LLM, feel free to use that instead.</p>

<h2 id="outline">Outline</h2>

<ul>
  <li>intro</li>
  <li>play 20 questions
    <ul>
      <li>shareback</li>
    </ul>
  </li>
  <li>adventures in amphibology
    <ul>
      <li>shareback</li>
    </ul>
  </li>
  <li>design your own LLM parlour game
    <ul>
      <li>shareback</li>
    </ul>
  </li>
</ul>

<h2 id="thesis-statement">Thesis statement</h2>

<blockquote>
  <p>the killer app for genAI is parlour games for overeducated wankers</p>
</blockquote>

<p>(this is a demographic I know well, because I am one)</p>

<p>To break it down:</p>

<ul>
  <li>
    <p><strong>parlour</strong>: involving co-located humans. Bored. Night outside, drinks and a
warm fire inside… and so</p>
  </li>
  <li>
    <p><strong>games</strong>: to entertain ourselves and each other</p>
  </li>
</ul>

<p>…for…</p>

<ul>
  <li>
    <p><strong>overeducated</strong>: word games, word play, we are masters of language and we
love to show off</p>
  </li>
  <li>
    <p><strong>wankers</strong>: (literally) self-indulgent. Not about a bigger goal, or doing
good in the world, it’s just for the heck of it.</p>
  </li>
</ul>

<h2 id="mechanics-of-a-parlour-game">Mechanics of a parlour game</h2>

<pre><code class="language-text">10 someone says something
20 someone(s) says something in response
30 GOTO 20
</code></pre>

<p>As an example, consider the game of
<a href="https://en.wikipedia.org/wiki/Twenty_questions">Twenty Questions</a>.</p>

<p><em>Ben scrawls on the whiteboard for 5mins</em></p>

<h2 id="play-llm-augmented-20-questions">Play: <em>LLM-augmented 20 Questions</em></h2>

<p>Here are the prompts (you can copy-paste them automatically with the widget at
the top right of the text).</p>

<p>To start, let’s have the LLM be the “answerer” and you (and other human players)
be the “questioner”, although you’ll need to have someone at a laptop be the LLM
surrogate. Have them use this (or similar) prompt, and don’t show the output to
the questioner(s).</p>

<pre><code class="language-markdown">You're playing Twenty Questions, so you need to choose an object and tell me
what it is (I won't tell the other players).
</code></pre>

<p>Then, the questioner(s) can ask the LLM questions (via the surrogate) like:</p>

<pre><code class="language-markdown">Is the object you're thinking of bigger than a car?
</code></pre>

<p>The surrogate continues to mediate between the LLM and the questioners until
there’s a winner (either a questioner guesses correctly, or there have been 20
questions).</p>

<h3 id="shareback">Shareback</h3>

<ul>
  <li>what’s the funnest/funniest moment, and why?</li>
  <li>what parts sucked?</li>
  <li>how did your play/strategy/behaivour change over time?</li>
  <li>did you tweak the game rules at all? if so, how?</li>
</ul>

<h2 id="play-adventures-in-amphibology">Play: <em>Adventures in Amphibology</em></h2>

<p>Every player thinks of a word or (short) phrase, writes it down on a piece of
paper and puts it in a hat.</p>

<p>These words/phrases are all fed into an LLM using the following prompt template:</p>

<pre><code class="language-text">You are an expert in the use of language, and you have been given the following words/phrases:

- &lt;phrase one&gt;
- &lt;phrase two&gt;
- &lt;phrase three&gt;
- etc.

Which of these words/phrases is the most ambiguous, and why?
</code></pre>

<p>The player who wrote the thing that the LLM chooses scores 1 point. But they’re
not the winner just yet.</p>

<p>Using the same words/phrases (i.e. without starting a new ChatGPT session) ask
the LLM to pick again, based on successively different criteria:</p>

<pre><code class="language-markdown">Which of these words/phrases best describes your perfect date?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases is the purplest?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases most evokes the experience of a crisp winter sunrise in Canberra?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the best title for a sci-fi movie?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases is the most nihilistic?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases has the most assonance?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases is the most disrespectful?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the best name for a pet cat?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases sounds most like a boy band track from the 90s?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the easiest to explain to a toddler?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the best password for a secret underground antifascist network?</code></pre>

<p>At the end, the player with the most points wins. You can play as many times as
you like, with new words/phrases and/or new “judging” criteria.</p>

<h3 id="shareback-1">Shareback</h3>

<ul>
  <li>what’s the funnest/funniest moment, and why?</li>
  <li>what parts sucked?</li>
  <li>how did your play/strategy/behaivour change over time?</li>
  <li>did you tweak the game rules at all? if so, how?</li>
</ul>

<h2 id="play-design-your-own-llm-parlour-game">Play: design your own LLM parlour game</h2>

<p>Write/draw it up in such a way that a different group (in this workshop) could
play it without you being there to help them out.</p>

<h3 id="shareback-and-remember">Shareback, and remember</h3>

<p>There’s no higher purpose here. It’s all just parlour games for overeducated
wankers.</p>]]></content><author><name>Ben Swift</name></author><category term="teaching" /><category term="ai" /><summary type="html"><![CDATA[Note: this stuff is the workshop content for an alumni workshop in November 2024 hosted by the Cybernetic Studio at the ANU School of Cybernetics.]]></summary></entry><entry><title type="html">Format Markdown on save in Zed using Prettier</title><link href="https://benswift.me/blog/2024/09/20/format-markdown-on-save-in-zed-using-prettier/" rel="alternate" type="text/html" title="Format Markdown on save in Zed using Prettier" /><published>2024-09-20T00:00:00+10:00</published><updated>2024-09-20T00:00:00+10:00</updated><id>https://benswift.me/blog/2024/09/20/format-markdown-on-save-in-zed-using-prettier</id><content type="html" xml:base="https://benswift.me/blog/2024/09/20/format-markdown-on-save-in-zed-using-prettier/"><![CDATA[<p>One thing I’ve particularly enjoyed since switching to Zed is format-on-save,
which is turned on by default in most programming modes. However, it’s not
turned on by default for Markdown files.</p>

<p>Since I like my Markdown files formatted with <a href="https://prettier.io">Prettier</a>,
including the “wrap to 80 cols” <code>--prose-wrap always</code> option, I set that command
as an “external” formatter for Markdown files and turned on <code>format_on_save</code>.
Here’s the config if you want to do the same:</p>

<pre><code class="language-json">"languages": {
    "Markdown": {
      "format_on_save": "on",
      "formatter": {
        "external": {
          "command": "prettier",
          "arguments": [
            "--prose-wrap",
            "always",
            "--stdin-filepath",
            "{buffer_path}"
          ]
        }
      }
    }
  }
</code></pre>]]></content><author><name>Ben Swift</name></author><category term="tools" /><category term="zed" /><summary type="html"><![CDATA[One thing I’ve particularly enjoyed since switching to Zed is format-on-save, which is turned on by default in most programming modes. However, it’s not turned on by default for Markdown files.]]></summary></entry><entry><title type="html">Hosting a genAI trivia night</title><link href="https://benswift.me/blog/2024/08/12/hosting-a-genai-trivia-night/" rel="alternate" type="text/html" title="Hosting a genAI trivia night" /><published>2024-08-12T00:00:00+10:00</published><updated>2024-08-12T00:00:00+10:00</updated><id>https://benswift.me/blog/2024/08/12/hosting-a-genai-trivia-night</id><content type="html" xml:base="https://benswift.me/blog/2024/08/12/hosting-a-genai-trivia-night/"><![CDATA[<p>I was recently tasked with organising a trivia night, and decided to generate
all the questions (and answers) with a large language model (I used
<a href="https://claude.ai/">Claude</a>, although obviously this would work with any model.</p>

<p>Here’s the initial prompt I used:</p>

<blockquote>
  <p>Write a set of questions (10 rounds, 5 questions per round) for a trivia
night, including answers. Each round must have a different theme, including
rounds on the topics of “<em>insert list of rounds here</em>”. You must provide
questions which have a single, unambiguous correct answer. Include a mix of
easy and difficult questions, such that a graduate-level audience would get
approximately 50% of the answers correct.</p>
</blockquote>

<p>Looking over the answers, they looked a little too easy, so I provided a
follow-up:</p>

<blockquote>
  <p>Those questions are all too easy. Try again, and dial up the difficulty.</p>
</blockquote>

<p>which gave questions which looked (to my eyes) to be around the right level.</p>

<p>Now, LLMs are notorious for hallucinating/making up facts, and I couldn’t be
bothered to check that all the answers were correct. So I incorporated this “is
the LLM making stuff up?” dynamic into the rules. As well as the usual trivia
night procedure:</p>

<ul>
  <li>1 pt per question</li>
  <li>each question will be read <strong>twice</strong> (no more than that)</li>
  <li>we’ll give answers and tally scores after each round</li>
  <li>no cheating (internet <em>or</em> AI models, inc. self-hosted ones)</li>
</ul>

<p>there was an additional rule: at the end of each round, each team can challenge
any answer(s) they think the LLM got wrong. For each challenge, the trivia hosts
would investigate (using the internet, or whatever) to see what the correct
answer is.</p>

<ul>
  <li>if the LLM’s answer was wrong, <em>all teams</em> have that question re-marked with
the correct answer</li>
  <li>if the LLM’s answer was ambiguous (i.e. it was correct, but there are other
answers that were equally correct) then <em>all teams</em> have that question
re-marked, accepting any of the correct answers</li>
  <li>if the LLM’s answer was correct (or if we can’t find a reliable answer in an
appropriate timeframe) then the question is not re-marked, and the challenging
team receives an additional one-point penalty</li>
</ul>

<p>As for whether the LLM was correct/ambiguous/wrong, all decisions by the trivia
hosts were final.</p>

<p>And how’d it go? Pretty well, overall. In the end the questions were a bit too
tricky. Turns out it’s really hard to eyeball questions <em>with</em> answers to guess
how many you’d get correct, so if you’re going to do that make sure you do it
without looking at the answers.</p>

<p>There was one successful challenge on the night, but overall there didn’t seem
to be too many hallucinations. In some ways it would have been more fun if there
were.</p>

<p>Anyway if you need to organise a trivia night and don’t want to do any
painstaking research, then give the above prompts a try.</p>]]></content><author><name>Ben Swift</name></author><category term="ai" /><summary type="html"><![CDATA[I was recently tasked with organising a trivia night, and decided to generate all the questions (and answers) with a large language model (I used Claude, although obviously this would work with any model.]]></summary></entry><entry><title type="html">Livecoding set ICLC’24 - Shanghai Concert Hall</title><link href="https://benswift.me/blog/2024/05/30/livecoding-set-iclc-24-shanghai-concert-hall/" rel="alternate" type="text/html" title="Livecoding set ICLC’24 - Shanghai Concert Hall" /><published>2024-05-30T00:00:00+10:00</published><updated>2024-05-30T00:00:00+10:00</updated><id>https://benswift.me/blog/2024/05/30/livecoding-set-iclc-24-shanghai-concert-hall</id><content type="html" xml:base="https://benswift.me/blog/2024/05/30/livecoding-set-iclc-24-shanghai-concert-hall/"><![CDATA[<p>I’m currently in Shanghai tomorrow night I’ll be performing at the <a href="https://www.shanghaiconcerthall.org/">Shanghai Concert Hall</a> as part of <a href="https://iclc.toplap.org/2024/program.html#table-day2">ICLC ‘24</a>. I’m told tickets are already sold out, but if you’ve already got one and you’re coming along then come say hi after the set.</p>

<p>I’ll try and put up a video of the performance (I’ll be doing music, with LXT
and Beverly Edwards on visuals) once it’s all done.</p>]]></content><author><name>Ben Swift</name></author><summary type="html"><![CDATA[I’m currently in Shanghai tomorrow night I’ll be performing at the Shanghai Concert Hall as part of ICLC ‘24. I’m told tickets are already sold out, but if you’ve already got one and you’re coming along then come say hi after the set.]]></summary></entry></feed>