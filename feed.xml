<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://benswift.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://benswift.me/" rel="alternate" type="text/html" /><updated>2025-07-17T17:04:32+10:00</updated><id>https://benswift.me/feed.xml</id><title type="html">benswift.me</title><subtitle>livecoder &amp; researcher homepage - code, creativity, culture</subtitle><author><name>Ben Swift</name></author><entry><title type="html">Agentic AI: LLMs with stones</title><link href="https://benswift.me/blog/2025/07/17/agentic-ai-llms-with-stones/" rel="alternate" type="text/html" title="Agentic AI: LLMs with stones" /><published>2025-07-17T00:00:00+10:00</published><updated>2025-07-17T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/07/17/agentic-ai-llms-with-stones</id><content type="html" xml:base="https://benswift.me/blog/2025/07/17/agentic-ai-llms-with-stones/"><![CDATA[<blockquote>
  <p>Sticks and stones may break my bones, but words will never hurt me.</p>
</blockquote>

<p>There’s a truth to that proverb, even if you feel (as I do) the temptation to
“well akshually…” make several very valid points about how words <em>can</em> be
hurtful. For most of the Large Language Model (LLM) era—since the public
release of ChatGPT in November 2022—we’ve been in turns amazed, disgusted and
now kindof “meh” about the way that LLMs can take the <strong>words</strong> we give them and
produce <strong>more words</strong> in response.</p>

<p>Working as I do as an
<a href="https://cybernetics.anu.edu.au/people/ben-swift/">academic computer scientist</a>
(with a research background in AI) who regularly runs “executive education”
courses on AI for a diverse range of educated and intelligent folks, I’m getting
more and more questions about “agentic AI”. And while definitions and
descriptions change pretty quick in this field at the moment, I want to
demistify some things about this term in particular.</p>

<p>Agentic AI (as concieved and talked about in this present moment) is about
connecting LLMs—“pure” input/output text sausage machines—to the world with
<strong>tools</strong>. To return to the “sticks and stones” aphorism above; Agentic AI is an
an LLM holding a stone. Because while OpenAI introduced “function calling” to
GPT models in
<a href="https://openai.com/index/function-calling-and-other-api-updates/">June 2023</a>,
it wasn’t until late 2024 that the term “agentic” really took off—coinciding
with Anthropic’s release of their
<a href="https://www.anthropic.com/news/3-5-models-and-computer-use">computer use capabilities</a>
and a general industry shift toward thinking of LLMs as autonomous actors rather
than just conversational partners.</p>

<p>From a cybernetic perspective, this isn’t quite as big a change as you might
think. Because even the original ChatGPT could “do stuff in the world” by
telling <em>you</em> (the human user) to do it. Sometimes that was as benign as having
you copying text into an email—and we’d still casually refer to this as
“answering email with ChatGPT”, but actually all ChatGPT was doing was giving
you <strong>words</strong> to type into your email client and send out. Sometimes those words
told us to do more life-impacting things, like
<a href="https://www.vice.com/en/article/we-asked-chatgpt-how-to-break-up-with-someone/">break up with your partner</a>,
or worse. So whenever LLMs are used by humans they have the (indirect) ability
to affect the world.</p>

<p>The change with agentic AI is that the LLMs are provided access to specific
tools which they can use do stuff beyond just returning words in a text box on a
web page. In the last couple of years there have been a proliferation of tools
(particularly using a standard interface called the
<a href="https://modelcontextprotocol.io/introduction">Model Context Protocol</a>). Some
classic examples of tools are “search the web”, “add/remove meetings from my
calendar”. There are domain-specialised tools depending on what you’re doing
with the LLM too—software developers in particular have found ways to use
tools to help them write code. There have been many recent blog posts of various
developers describing how they set up their agentic AI (tool-calling LLM)
systems—from
<a href="https://simonwillison.net/2025/Apr/19/claude-code-best-practices/">Simon Willison’s explorations</a>
to
<a href="https://fly.io/blog/youre-all-nuts/">Thomas Ptacek’s “My AI Skeptic Friends Are All Nuts”</a>
to countless HackerNews threads debating whether this is all just hype.</p>

<p>Here’s how it works in practice:</p>

<ul>
  <li>you put in a prompt as normal which is sent to the LLM</li>
  <li>in addition to that prompt, though, the LLM is sent a list of tools that you
have (including human-readable descriptions of what they can do)</li>
  <li>instead of only being able to respond with text, the LLM can respond with a
“tool call” instruction; e.g. it can say “add a meeting to the calendar at
10am tomorrow using the <code>calendar</code> tool”</li>
  <li>in the latter case, the user doesn’t need to do anything; the system will use
the tool as requested by the LLM and return the results (usually the fact that
this is happening is communicated to the user via some sort of visual feedback
in the interface, although this isn’t a requirement)</li>
</ul>

<h2 id="implications">Implications</h2>

<p>In my opinion the best way to think about this shift isn’t that LLMs can now
influence the world; it’s that now they can do it without asking. This means:</p>

<ol>
  <li>first, there’s now no longer a human in the loop (so now there’s no human to
say “hey, that’s a dumb idea” and refuse to do it)</li>
  <li>as a consequence of #1, LLMs can now run/iterate without intervention for
much longer (minutes, maybe even hours…)</li>
</ol>

<p>The second point is the bigger deal (and it’s a point that Anthropic, the makers
of the Claude LLM which is one of the big players these days, makes in their
<a href="https://www.anthropic.com/engineering/building-effective-agents">recent whitepaper about agentic AI</a>).</p>

<p>Humans were always a) able to do things in the environment, and b) the
bottleneck in any LLM system (time-wise, at least). But by gaining the former
capability, agentic AI removes the latter bottleneck.</p>

<p>So if you’re going to allow your LLMs to use tools, you <strong>must</strong> be certain that
a) you’re comfortable with what things the LLM can do with them (both in theory,
i.e. in the sense of what’s possible, but also in practice, i.e. through testing
the way that your particular LLM tends to use your tools given specific prompts
or other context).</p>]]></content><author><name>Ben Swift</name></author><category term="ai" /><summary type="html"><![CDATA[Sticks and stones may break my bones, but words will never hurt me.]]></summary></entry><entry><title type="html">Automated RPi Web Kiosk Setup in 2025</title><link href="https://benswift.me/blog/2025/07/16/automated-rpi-web-kiosk-setup-in-2025/" rel="alternate" type="text/html" title="Automated RPi Web Kiosk Setup in 2025" /><published>2025-07-16T00:00:00+10:00</published><updated>2025-07-16T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/07/16/automated-rpi-web-kiosk-setup-in-2025</id><content type="html" xml:base="https://benswift.me/blog/2025/07/16/automated-rpi-web-kiosk-setup-in-2025/"><![CDATA[<p>As part of a <a href="https://github.com/anucybernetics/panic">recent art installation</a>
I’ve needed to set up lots (well, dozens) of Raspberry 5s to run as fullscreen
Chromium “kiosks” with a pre-set URL (network connected, but with no
keyboard/mouse).</p>

<p>They’ve all needed slightly different kiosk URLs, and I <em>hate</em> doing this sort
of busy-work by hand. So I’ve spent longer than I’d like to admit<sup id="fnref:time" role="doc-noteref"><a href="#fn:time" class="footnote" rel="footnote">1</a></sup> putting
together a fully scripted burn-and-boot process. My non-negotiables were:</p>

<ul>
  <li>script needs to run on macOS (since my MBP is the only computer I have with an
SD card reader/writer)</li>
  <li>once the SD card is put into an rpi and plugged in, everything should be
automatic (join network, install and configure software and OS)</li>
  <li>I need a way to remotely access the rpis once “in the field” (in particular,
sometimes I need to change their kiosk URL)</li>
</ul>

<p>I <em>thought</em> that this would be a pretty common thing that others would have
done—rpis are cheap and seem like a good fit for this type of “web browser as
installation display” thing. But I kept running into dead ends.</p>

<p>I finally got it fully working with <a href="https://dietpi.com/">DietPi</a> as the OS and
<a href="https://www.hjdskes.nl/projects/cage/">Cage/Wayland</a> as the compositor. To save
you the trouble, dear reader, I’ve packaged it all up into a script (which works
as of the date of this post—July 2025) and put it
<a href="https://github.com/ANUcybernetics/panic/tree/main/rpi">here</a>. From that
folder’s <code>README.md</code>:</p>

<blockquote>
  <p>This directory contains a script to set up Raspberry Pi 5 devices as browser
kiosks that boot directly into fullscreen Chromium displaying a specified URL.</p>

  <p>The <code>pi-setup.sh</code> script creates a fully automated
<a href="https://dietpi.com">DietPi</a> installation that:</p>

  <ul>
    <li>boots directly into GPU-accelerated kiosk mode using Cage Wayland compositor</li>
    <li>automatically joins your Tailscale network</li>
    <li>supports native display resolutions including 4K at 60Hz</li>
    <li>configures WiFi (WPA2 and enterprise 802.1X)</li>
    <li>includes HDMI audio support</li>
    <li>optimized specifically for Raspberry Pi 5 with 8GB RAM</li>
    <li>provides a <code>kiosk-set-url</code> utility for easy URL changes</li>
  </ul>
</blockquote>

<p>I hope this savees you some time. Use it to hug your loved ones.</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:time" role="doc-endnote">
      <p>I thought it’d take a day, it’s taken about a week, on and off :( <a href="#fnref:time" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="tools" /><summary type="html"><![CDATA[As part of a recent art installation I’ve needed to set up lots (well, dozens) of Raspberry 5s to run as fullscreen Chromium “kiosks” with a pre-set URL (network connected, but with no keyboard/mouse).]]></summary></entry><entry><title type="html">DIYChatGPT Short Course for ANU Undergraduates</title><link href="https://benswift.me/blog/2025/07/07/diychatgpt-short-course-for-anu-undergraduates/" rel="alternate" type="text/html" title="DIYChatGPT Short Course for ANU Undergraduates" /><published>2025-07-07T00:00:00+10:00</published><updated>2025-07-07T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/07/07/diychatgpt-short-course-for-anu-undergraduates</id><content type="html" xml:base="https://benswift.me/blog/2025/07/07/diychatgpt-short-course-for-anu-undergraduates/"><![CDATA[<p>If you’re an ANU undergraduate student you can enrol in an upcoming (next week!)
<a href="https://mccuskerinstitute.anu.edu.au/study/knots/diy-chatgpt-llms-as-information-processing-machines/">upcoming 3hr course</a>
I’ve created called <em>DIY ChatGPT: LLMs as Information Processing Machines</em>. It’s
running for the first time <strong>next Thursday 17 July 10am–1pm</strong> (although it will
run again later in the semester).</p>

<p>Here’s the blurb:</p>

<blockquote>
  <p>In this hands-on workshop you will train and use your own language model from
scratch - with just pen and paper and a bit of dice rolling. Through
interactive exercises and guided discussions, you’ll see how language models
(even large language models like ChatGPT) are fundamentally information
processing machines, turning language inputs into language outputs. The
workshop builds from basic principles to more complex applications through an
exploration of language modeling as a probabilistic process of predicting
“what token comes next” in a sequence, and ends with a poetry slam (for real).</p>
</blockquote>

<p>This course is a
<a href="https://mccuskerinstitute.anu.edu.au/study/knots_explained/">“Know the Nature Of Things” KNoT</a>
offered as part of the ANU’s
<a href="https://mccuskerinstitute.anu.edu.au/">McCusker Institute</a>. They’re for-credit
courses run by different folks all across campus, and a great way to explore
some of the cool stuff going on at ANU.</p>]]></content><author><name>Ben Swift</name></author><summary type="html"><![CDATA[If you’re an ANU undergraduate student you can enrol in an upcoming (next week!) upcoming 3hr course I’ve created called DIY ChatGPT: LLMs as Information Processing Machines. It’s running for the first time next Thursday 17 July 10am–1pm (although it will run again later in the semester).]]></summary></entry><entry><title type="html">Agentic Elixir superpowers: Zed + Tidewave + AshAI</title><link href="https://benswift.me/blog/2025/06/06/agentic-elixir-superpowers-zed-tidewave-ashai/" rel="alternate" type="text/html" title="Agentic Elixir superpowers: Zed + Tidewave + AshAI" /><published>2025-06-06T00:00:00+10:00</published><updated>2025-06-06T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/06/06/agentic-elixir-superpowers-zed-tidewave-ashai</id><content type="html" xml:base="https://benswift.me/blog/2025/06/06/agentic-elixir-superpowers-zed-tidewave-ashai/"><![CDATA[<p>For a few years now whenever I need to build any sort of networked interactive
experience, I reach for Elixir (with Phoenix LiveView and Ash). It’s an
<em>extremely</em> productive combination, especially when you want to do more complex
client/server information flows than the standard request/response UX. I’ve also
(for the last year or two) ditched Emacs for <a href="https://zed.dev">Zed</a>. Again, if
you take the time to master these tools I think they’re excellent.</p>

<p>Like every indie hacker and their wombat I’m experimenting with LLM Agents as
part of my software development workflow. The new frontier (which I’m excited
about, but ask me in a few months how it’s gone) is putting all of the above
together. If you’d like to do that too, and want to see a real-world example of
how these things can be plugged into one another, here’s my setup.</p>

<ul>
  <li>Zed as the text editor/MCP host</li>
  <li>both <a href="https://hexdocs.pm/tidewave/zed.html">Tidewave</a> and
<a href="https://hexdocs.pm/ash_ai/readme.html">AshAI</a> as MCP servers</li>
</ul>

<p>Then, my Zed settings file has this MCP server configuration:</p>

<pre><code class="language-json">"context_servers": {
  "ash_ai": {
    "command": {
      "path": "mcp-proxy",
      "args": ["http://localhost:4000/ash_ai/mcp"],
      "env": {}
    },
    "settings": {}
  },
  "tidewave": {
    "command": {
      "path": "mcp-proxy",
      "args": ["http://localhost:4000/tidewave/mcp"],
      "env": {}
    },
    "settings": {}
  }
},
</code></pre>

<p>Even after following the instructions for those tools, you’ll need a way to set
up all the proxies and pipe everything together. I wrote
<a href="https://github.com/benswift/.dotfiles/blob/master/bin/tidewave-proxy.sh">this script</a>,
which you’re free to use (MIT Licence) if it helps.</p>

<p>Put it on your <code>~PATH</code> and then (in your Phoenix project root) run it like so:</p>

<pre><code>[16:03] daysy:panic $ tidewave-proxy.sh
🚀 MCP Proxy Development Environment
====================================
Configuration:
  Host: localhost
  Port: 4000
  Ash AI MCP: http://localhost:4000/ash_ai/mcp
  Tidewave MCP: http://localhost:4000/tidewave/mcp

🔗 Starting mcp-proxy instances...
Starting mcp-proxy for ash_ai...
✅ mcp-proxy for ash_ai started (PID: 88815)
   Log file: /tmp/mcp_ash_ai.log
Starting mcp-proxy for tidewave...
✅ mcp-proxy for tidewave started (PID: 88816)
   Log file: /tmp/mcp_tidewave.log

🎉 MCP Proxy environment is ready!
=====================================

Available MCP Servers:
  📊 Ash AI MCP:    http://localhost:4000/ash_ai/mcp
  🌊 Tidewave MCP:  http://localhost:4000/tidewave/mcp

MCP Proxy Commands:
  For Ash AI:    mcp-proxy http://localhost:4000/ash_ai/mcp
  For Tidewave:  mcp-proxy http://localhost:4000/tidewave/mcp

Next Steps:
1. Configure your MCP client (Zed, Claude Desktop, etc.)
2. Use the endpoints above with your MCP client
3. Test with: curl -H 'Accept: application/json' &lt;endpoint&gt;

Log Files:
  Ash AI Proxy: /tmp/mcp_ash_ai.log
  Tidewave Proxy: /tmp/mcp_tidewave.log

Press Ctrl+C to stop all services and clean up

🔥 Starting Phoenix development server with REPL...
==================================================
Erlang/OTP 27 [erts-15.2.5] [source] [64-bit] [smp:16:16] [ds:16:16:10] [async-threads:1] [jit]

[info] Migrations already up
[info] Running PanicWeb.Endpoint with Bandit 1.7.0 at 127.0.0.1:4000 (http)
[info] Access PanicWeb.Endpoint at http://localhost:4000
Interactive Elixir (1.18.4) - press Ctrl+C to exit (type h() ENTER for help)
[watch] build finished, watching for changes...

Rebuilding...

Done in 196ms.
</code></pre>

<p>Then, to test, in the Agent panel try something like:</p>

<pre><code>Use the tidewave project eval tool to add 10+15 in Elixir.
</code></pre>

<p>If everything’s hooked up right, you’ll get the answer.</p>

<p>One other tip: make sure you’re using OTP27 rather than the (newest)
OTP28—there’s an error on one of the AshAI deps which stops it compiling on
the latest OTP. I suspect it’ll be fixed soon, though.</p>]]></content><author><name>Ben Swift</name></author><category term="elixir" /><category term="web" /><category term="zed" /><summary type="html"><![CDATA[For a few years now whenever I need to build any sort of networked interactive experience, I reach for Elixir (with Phoenix LiveView and Ash). It’s an extremely productive combination, especially when you want to do more complex client/server information flows than the standard request/response UX. I’ve also (for the last year or two) ditched Emacs for Zed. Again, if you take the time to master these tools I think they’re excellent.]]></summary></entry><entry><title type="html">LLM Parlour Games for Overeducated Wankers</title><link href="https://benswift.me/blog/2024/11/07/llm-parlour-games-for-overeducated-wankers/" rel="alternate" type="text/html" title="LLM Parlour Games for Overeducated Wankers" /><published>2024-11-07T00:00:00+11:00</published><updated>2024-11-07T00:00:00+11:00</updated><id>https://benswift.me/blog/2024/11/07/llm-parlour-games-for-overeducated-wankers</id><content type="html" xml:base="https://benswift.me/blog/2024/11/07/llm-parlour-games-for-overeducated-wankers/"><![CDATA[<p><em>Note: this stuff is the workshop content for an alumni workshop in November
2024 hosted by the Cybernetic Studio at the ANU School of Cybernetics.</em></p>

<h2 id="abstract">Abstract</h2>

<p>In this interactive design session participants will design and prototype their
own language-model-based parlour game. We’ll think critically about what
language models are (and aren’t) and what they’re good for (and rubbish at).
You’ll interact with other humans and design systems with goals and guardrails,
and think about what it means to give input to (and understand output from) LLMs
and genAI systems.</p>

<p>Prerequisites: ability to lounge around and use big words to impress your
friends in games of no stakes whatsoever. Self-satisfied smugness about said
loquaciousness is helpful but not essential.</p>

<h2 id="tech-note">Tech note</h2>

<p>This workshop requires access to a chat-based LLM (e.g. ChatGPT). If you’ve got
a laptop (or even a phone, although you’ll be typing on your janky little phone
keyboard) you can head to <a href="https://chatgpt.com">https://chatgpt.com</a> (no sign-up required). But if
you’ve got a different favourite chat-based LLM, feel free to use that instead.</p>

<h2 id="outline">Outline</h2>

<ul>
  <li>intro</li>
  <li>play 20 questions
    <ul>
      <li>shareback</li>
    </ul>
  </li>
  <li>adventures in amphibology
    <ul>
      <li>shareback</li>
    </ul>
  </li>
  <li>design your own LLM parlour game
    <ul>
      <li>shareback</li>
    </ul>
  </li>
</ul>

<h2 id="thesis-statement">Thesis statement</h2>

<blockquote>
  <p>the killer app for genAI is parlour games for overeducated wankers</p>
</blockquote>

<p>(this is a demographic I know well, because I am one)</p>

<p>To break it down:</p>

<ul>
  <li>
    <p><strong>parlour</strong>: involving co-located humans. Bored. Night outside, drinks and a
warm fire inside… and so</p>
  </li>
  <li>
    <p><strong>games</strong>: to entertain ourselves and each other</p>
  </li>
</ul>

<p>…for…</p>

<ul>
  <li>
    <p><strong>overeducated</strong>: word games, word play, we are masters of language and we
love to show off</p>
  </li>
  <li>
    <p><strong>wankers</strong>: (literally) self-indulgent. Not about a bigger goal, or doing
good in the world, it’s just for the heck of it.</p>
  </li>
</ul>

<h2 id="mechanics-of-a-parlour-game">Mechanics of a parlour game</h2>

<pre><code class="language-text">10 someone says something
20 someone(s) says something in response
30 GOTO 20
</code></pre>

<p>As an example, consider the game of
<a href="https://en.wikipedia.org/wiki/Twenty_questions">Twenty Questions</a>.</p>

<p><em>Ben scrawls on the whiteboard for 5mins</em></p>

<h2 id="play-llm-augmented-20-questions">Play: <em>LLM-augmented 20 Questions</em></h2>

<p>Here are the prompts (you can copy-paste them automatically with the widget at
the top right of the text).</p>

<p>To start, let’s have the LLM be the “answerer” and you (and other human players)
be the “questioner”, although you’ll need to have someone at a laptop be the LLM
surrogate. Have them use this (or similar) prompt, and don’t show the output to
the questioner(s).</p>

<pre><code class="language-markdown">You're playing Twenty Questions, so you need to choose an object and tell me
what it is (I won't tell the other players).
</code></pre>

<p>Then, the questioner(s) can ask the LLM questions (via the surrogate) like:</p>

<pre><code class="language-markdown">Is the object you're thinking of bigger than a car?
</code></pre>

<p>The surrogate continues to mediate between the LLM and the questioners until
there’s a winner (either a questioner guesses correctly, or there have been 20
questions).</p>

<h3 id="shareback">Shareback</h3>

<ul>
  <li>what’s the funnest/funniest moment, and why?</li>
  <li>what parts sucked?</li>
  <li>how did your play/strategy/behaivour change over time?</li>
  <li>did you tweak the game rules at all? if so, how?</li>
</ul>

<h2 id="play-adventures-in-amphibology">Play: <em>Adventures in Amphibology</em></h2>

<p>Every player thinks of a word or (short) phrase, writes it down on a piece of
paper and puts it in a hat.</p>

<p>These words/phrases are all fed into an LLM using the following prompt template:</p>

<pre><code class="language-text">You are an expert in the use of language, and you have been given the following words/phrases:

- &lt;phrase one&gt;
- &lt;phrase two&gt;
- &lt;phrase three&gt;
- etc.

Which of these words/phrases is the most ambiguous, and why?
</code></pre>

<p>The player who wrote the thing that the LLM chooses scores 1 point. But they’re
not the winner just yet.</p>

<p>Using the same words/phrases (i.e. without starting a new ChatGPT session) ask
the LLM to pick again, based on successively different criteria:</p>

<pre><code class="language-markdown">Which of these words/phrases best describes your perfect date?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases is the purplest?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases most evokes the experience of a crisp winter sunrise in Canberra?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the best title for a sci-fi movie?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases is the most nihilistic?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases has the most assonance?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases is the most disrespectful?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the best name for a pet cat?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases sounds most like a boy band track from the 90s?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the easiest to explain to a toddler?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the best password for a secret underground antifascist network?</code></pre>

<p>At the end, the player with the most points wins. You can play as many times as
you like, with new words/phrases and/or new “judging” criteria.</p>

<h3 id="shareback-1">Shareback</h3>

<ul>
  <li>what’s the funnest/funniest moment, and why?</li>
  <li>what parts sucked?</li>
  <li>how did your play/strategy/behaivour change over time?</li>
  <li>did you tweak the game rules at all? if so, how?</li>
</ul>

<h2 id="play-design-your-own-llm-parlour-game">Play: design your own LLM parlour game</h2>

<p>Write/draw it up in such a way that a different group (in this workshop) could
play it without you being there to help them out.</p>

<h3 id="shareback-and-remember">Shareback, and remember</h3>

<p>There’s no higher purpose here. It’s all just parlour games for overeducated
wankers.</p>]]></content><author><name>Ben Swift</name></author><category term="teaching" /><category term="ai" /><summary type="html"><![CDATA[Note: this stuff is the workshop content for an alumni workshop in November 2024 hosted by the Cybernetic Studio at the ANU School of Cybernetics.]]></summary></entry><entry><title type="html">Format Markdown on save in Zed using Prettier</title><link href="https://benswift.me/blog/2024/09/20/format-markdown-on-save-in-zed-using-prettier/" rel="alternate" type="text/html" title="Format Markdown on save in Zed using Prettier" /><published>2024-09-20T00:00:00+10:00</published><updated>2024-09-20T00:00:00+10:00</updated><id>https://benswift.me/blog/2024/09/20/format-markdown-on-save-in-zed-using-prettier</id><content type="html" xml:base="https://benswift.me/blog/2024/09/20/format-markdown-on-save-in-zed-using-prettier/"><![CDATA[<p>One thing I’ve particularly enjoyed since switching to Zed is format-on-save,
which is turned on by default in most programming modes. However, it’s not
turned on by default for Markdown files.</p>

<p>Since I like my Markdown files formatted with <a href="https://prettier.io">Prettier</a>,
including the “wrap to 80 cols” <code>--prose-wrap always</code> option, I set that command
as an “external” formatter for Markdown files and turned on <code>format_on_save</code>.
Here’s the config if you want to do the same:</p>

<pre><code class="language-json">"languages": {
    "Markdown": {
      "format_on_save": "on",
      "formatter": {
        "external": {
          "command": "prettier",
          "arguments": [
            "--prose-wrap",
            "always",
            "--stdin-filepath",
            "{buffer_path}"
          ]
        }
      }
    }
  }
</code></pre>]]></content><author><name>Ben Swift</name></author><category term="tools" /><category term="zed" /><summary type="html"><![CDATA[One thing I’ve particularly enjoyed since switching to Zed is format-on-save, which is turned on by default in most programming modes. However, it’s not turned on by default for Markdown files.]]></summary></entry><entry><title type="html">Hosting a genAI trivia night</title><link href="https://benswift.me/blog/2024/08/12/hosting-a-genai-trivia-night/" rel="alternate" type="text/html" title="Hosting a genAI trivia night" /><published>2024-08-12T00:00:00+10:00</published><updated>2024-08-12T00:00:00+10:00</updated><id>https://benswift.me/blog/2024/08/12/hosting-a-genai-trivia-night</id><content type="html" xml:base="https://benswift.me/blog/2024/08/12/hosting-a-genai-trivia-night/"><![CDATA[<p>I was recently tasked with organising a trivia night, and decided to generate
all the questions (and answers) with a large language model (I used
<a href="https://claude.ai/">Claude</a>, although obviously this would work with any model.</p>

<p>Here’s the initial prompt I used:</p>

<blockquote>
  <p>Write a set of questions (10 rounds, 5 questions per round) for a trivia
night, including answers. Each round must have a different theme, including
rounds on the topics of “<em>insert list of rounds here</em>”. You must provide
questions which have a single, unambiguous correct answer. Include a mix of
easy and difficult questions, such that a graduate-level audience would get
approximately 50% of the answers correct.</p>
</blockquote>

<p>Looking over the answers, they looked a little too easy, so I provided a
follow-up:</p>

<blockquote>
  <p>Those questions are all too easy. Try again, and dial up the difficulty.</p>
</blockquote>

<p>which gave questions which looked (to my eyes) to be around the right level.</p>

<p>Now, LLMs are notorious for hallucinating/making up facts, and I couldn’t be
bothered to check that all the answers were correct. So I incorporated this “is
the LLM making stuff up?” dynamic into the rules. As well as the usual trivia
night procedure:</p>

<ul>
  <li>1 pt per question</li>
  <li>each question will be read <strong>twice</strong> (no more than that)</li>
  <li>we’ll give answers and tally scores after each round</li>
  <li>no cheating (internet <em>or</em> AI models, inc. self-hosted ones)</li>
</ul>

<p>there was an additional rule: at the end of each round, each team can challenge
any answer(s) they think the LLM got wrong. For each challenge, the trivia hosts
would investigate (using the internet, or whatever) to see what the correct
answer is.</p>

<ul>
  <li>if the LLM’s answer was wrong, <em>all teams</em> have that question re-marked with
the correct answer</li>
  <li>if the LLM’s answer was ambiguous (i.e. it was correct, but there are other
answers that were equally correct) then <em>all teams</em> have that question
re-marked, accepting any of the correct answers</li>
  <li>if the LLM’s answer was correct (or if we can’t find a reliable answer in an
appropriate timeframe) then the question is not re-marked, and the challenging
team receives an additional one-point penalty</li>
</ul>

<p>As for whether the LLM was correct/ambiguous/wrong, all decisions by the trivia
hosts were final.</p>

<p>And how’d it go? Pretty well, overall. In the end the questions were a bit too
tricky. Turns out it’s really hard to eyeball questions <em>with</em> answers to guess
how many you’d get correct, so if you’re going to do that make sure you do it
without looking at the answers.</p>

<p>There was one successful challenge on the night, but overall there didn’t seem
to be too many hallucinations. In some ways it would have been more fun if there
were.</p>

<p>Anyway if you need to organise a trivia night and don’t want to do any
painstaking research, then give the above prompts a try.</p>]]></content><author><name>Ben Swift</name></author><category term="ai" /><summary type="html"><![CDATA[I was recently tasked with organising a trivia night, and decided to generate all the questions (and answers) with a large language model (I used Claude, although obviously this would work with any model.]]></summary></entry><entry><title type="html">Livecoding set ICLC’24 - Shanghai Concert Hall</title><link href="https://benswift.me/blog/2024/05/30/livecoding-set-iclc-24-shanghai-concert-hall/" rel="alternate" type="text/html" title="Livecoding set ICLC’24 - Shanghai Concert Hall" /><published>2024-05-30T00:00:00+10:00</published><updated>2024-05-30T00:00:00+10:00</updated><id>https://benswift.me/blog/2024/05/30/livecoding-set-iclc-24-shanghai-concert-hall</id><content type="html" xml:base="https://benswift.me/blog/2024/05/30/livecoding-set-iclc-24-shanghai-concert-hall/"><![CDATA[<p>I’m currently in Shanghai tomorrow night I’ll be performing at the <a href="https://www.shanghaiconcerthall.org/">Shanghai Concert Hall</a> as part of <a href="https://iclc.toplap.org/2024/program.html#table-day2">ICLC ‘24</a>. I’m told tickets are already sold out, but if you’ve already got one and you’re coming along then come say hi after the set.</p>

<p>I’ll try and put up a video of the performance (I’ll be doing music, with LXT
and Beverly Edwards on visuals) once it’s all done.</p>]]></content><author><name>Ben Swift</name></author><summary type="html"><![CDATA[I’m currently in Shanghai tomorrow night I’ll be performing at the Shanghai Concert Hall as part of ICLC ‘24. I’m told tickets are already sold out, but if you’ve already got one and you’re coming along then come say hi after the set.]]></summary></entry><entry><title type="html">Congrats Dr. Chenchen Xu</title><link href="https://benswift.me/blog/2023/06/01/congrats-dr-chenchen-xu/" rel="alternate" type="text/html" title="Congrats Dr. Chenchen Xu" /><published>2023-06-01T00:00:00+10:00</published><updated>2023-06-01T00:00:00+10:00</updated><id>https://benswift.me/blog/2023/06/01/congrats-dr-chenchen-xu</id><content type="html" xml:base="https://benswift.me/blog/2023/06/01/congrats-dr-chenchen-xu/"><![CDATA[<p>A big congratulations do my PhD student Chenchen for successfully completing his
PhD; his thesis title was <em>Weakly Supervised Vision and Language Representation
Learning in Sign Language Understanding</em>. It’s freelly available online from the
<a href="https://openresearch-repository.anu.edu.au/items/174662f9-0aba-4ed2-a340-eea1574969b4">ANU Library</a>.</p>

<p>Not only did he do excellent work, but (look at the date!) did a lot of it
alone, under lockdown. Really tough circumstances and a credit to you Chenchen
for coming out the other side.</p>]]></content><author><name>Ben Swift</name></author><category term="research" /><category term="ai" /><summary type="html"><![CDATA[A big congratulations do my PhD student Chenchen for successfully completing his PhD; his thesis title was Weakly Supervised Vision and Language Representation Learning in Sign Language Understanding. It’s freelly available online from the ANU Library.]]></summary></entry><entry><title type="html">Parental leave take 3</title><link href="https://benswift.me/blog/2023/05/13/parental-leave-take-3/" rel="alternate" type="text/html" title="Parental leave take 3" /><published>2023-05-13T00:00:00+10:00</published><updated>2023-05-13T00:00:00+10:00</updated><id>https://benswift.me/blog/2023/05/13/parental-leave-take-3</id><content type="html" xml:base="https://benswift.me/blog/2023/05/13/parental-leave-take-3/"><![CDATA[<p>Apologies for the radio silence over the last couple of months. I have the happy
opportunity to take one more round of parental leave. So I’ll be off doing Dad
stuff until March ‘24.</p>]]></content><author><name>Ben Swift</name></author><summary type="html"><![CDATA[Apologies for the radio silence over the last couple of months. I have the happy opportunity to take one more round of parental leave. So I’ll be off doing Dad stuff until March ‘24.]]></summary></entry></feed>