<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://benswift.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://benswift.me/" rel="alternate" type="text/html" /><updated>2022-02-04T13:17:13+11:00</updated><id>https://benswift.me/feed.xml</id><title type="html">benswift.me</title><subtitle>livecoder &amp; researcher homepage - code, creativity, culture</subtitle><author><name>Ben Swift</name></author><entry><title type="html">Running an AI neural style transfer model under Singularity</title><link href="https://benswift.me/blog/2022/02/01/running-an-ai-neural-style-transfer-model-under-singularity/" rel="alternate" type="text/html" title="Running an AI neural style transfer model under Singularity" /><published>2022-02-01T00:00:00+11:00</published><updated>2022-02-01T00:00:00+11:00</updated><id>https://benswift.me/blog/2022/02/01/running-an-ai-neural-style-transfer-model-under-singularity</id><content type="html" xml:base="https://benswift.me/blog/2022/02/01/running-an-ai-neural-style-transfer-model-under-singularity/"><![CDATA[<p>I’ve recently been given access to a beefy AI server (6x RTX3090s!) which is
managed via <a href="https://sylabs.io/singularity/">SingularityCE</a>, whose homepage
boldly asks and then forgets to answer the question: “What is SingularityCE?”</p>

<p>If you <a href="https://sylabs.io/guides/latest/user-guide/introduction.html">dig further into the
documentation</a>
it’s a little less coy:</p>

<blockquote>
  <p>SingularityCE is a container platform. It allows you to create and run
containers that package up pieces of software in a way that is portable and
reproducible. You can build a container using SingularityCE on your laptop,
and then run it on many of the largest HPC clusters in the world, local
university or company clusters, a single server, in the cloud, or on a
workstation down the hall. Your container is a single file, and you don’t have
to worry about how to install all the software you need on each different
operating system.</p>
</blockquote>

<p>I want to fire up my new GPUs and run one of Katherine Crowson’s awesome pytorch
scripts to do some <a href="https://github.com/crowsonkb/style-transfer-pytorch">neural style
transfer</a>. I’m very
familiar with Docker, but new to this Singularity thing, so here are some of the
hurdles I encountered (and cleared) along the way.</p>

<h2 id="finding-a-base-image">Finding a base image</h2>

<p>Looking in the style transfer repo’s
<a href="https://github.com/crowsonkb/style-transfer-pytorch/blob/master/setup.py#L23"><code>setup.py</code></a>,
it looks like <a href="https://pytorch.org">torch</a> v1.7.1 or later is required. Having
done this sort of thing before, I know that these deep learning frameworks
change a fair bit even between minor versions, so the safest option is to pick
the exact version that it was designed for—in this case v1.7.1 (or at least
v1.7.x).</p>

<p>So, the challenge is to find a Singularity image with that version of torch
installed. The singularity docs <a href="https://sylabs.io/guides/latest/user-guide/quick_start.html#download-pre-built-images">suggest using the search
command</a>
like so:</p>

<pre><code class="language-shell">$ singularity search torch

Found 34 container images for amd64 matching "torch":

	library://adalisan81/default/pytorch:latest

	library://aday651/default/pytorch-geometric-gpu:latest

	library://aphoh/default/pytorch-20.11-py3:v-1

	library://aradeva24/default/ar_pytorch_21.06-py3.sif:latest
		PyTorch NGC container with CUDA11.0, where PyTorch and apex are installed

	library://calebh/hpccm-test/faircluster-pytorch-1.10-cuda11.3:sha256.7c63a6c1f6f125b8d3e14fa10203965536ec7173d50e85b8c9ecf6ee0bff2ba7

	library://claytonm/default/ubuntu18_torch_torchvision_opencv_cuda10:latest

	library://dxtr/default/hpc-pytorch:0.1

	library://guoweihe/default/pytorch:hz1

	library://guoweihe/default/pytorch:v1.2

	library://guoweihe/default/torch:deep-ed

	library://guoweihe/default/torch:sha256.ff32c85ade2c8f6a1d34bd500de1b7bd11cdac16461aeef4d7cbd16ab129d8a7

	library://guoweihe/default/torchgpipe:master

	library://guoweihe/default/torchgpipe:sha256.a6ea5d732cba07c043e2f06cccbe541d28da6a8d9e5a3d18872d58af288dbc62

	library://ipa/medimgproc/pytorch:latest

	library://jamiechang917/default/pytorch:sha256.9c60c9825f20626cc0d6e69ac61d862bfec927e82d86becee73f853d657f2425

	library://lamastex/default/pytorch_21.03.sif:berzelius-20211027

	library://lamastex/default/pytorch_21.07.sif:berzelius-20211027

	library://lev-hpc/ml/pytorch_gpu:jupyter

	library://lev-hpc/ml/torch_tf_jupyter:latest

	library://mbalazs/default/pytorch:latest

	library://mbalazs/default/pytorch_cuda110:latest

	library://mike_holcomb/pytorchvision/v0.1.0:latest,v0.1.0

	library://oscartang/default/pytorch_translation:latest

	library://ottino8/default/pytorch:first

	library://pauldhein/hpc-deep-learning/torch-base:latest

	library://sina-ehsani/default/transformer-googlecrawl-torch-opencv:latest

	library://sina-ehsani/default/transformer-googlecrawl-torch1.10:latest

	library://skykiny/default/pytorch_skykiny:latest

	library://tmyoda/default/cuda-torch-pyenv:latest

	library://tru/default/c7-conda-pytorch-10.0:2019-07-12-2053,latest

	library://ufscar/hpc/cuda_pytorch:latest

	library://uvarc/default/pytorch:1.4.0-py37

	library://yboget/default/pytorch_rdkit_visdom:sha256.d97f221ef1294a8ef57d40cf7994d05d4955abc7cf39e3ce42faafd59fe3151a

	library://zhengtang/default/torch_translation:latest
</code></pre>

<p>Hmm. It’s hard to know which is official, which ones are going to work (a few of
them mention torch versions, but none of them are v1.7.x) and which ones might
even be malicious? That’s a worry.</p>

<p>Looking a bit deeper into the Singularity docs I find that one can <a href="https://sylabs.io/guides/latest/user-guide/singularity_and_docker.html">also use
Docker/OCI
images</a>,
and Singularity can pull them straight from Docker hub. That’s good news,
because NVIDIA do maintain <a href="https://hub.docker.com/r/pytorch/pytorch/tags">official Docker
images</a> for using torch with
NVIDIA graphics cards (like the 3090), so I find the <a href="https://hub.docker.com/layers/pytorch/pytorch/1.7.1-cuda11.0-cudnn8-runtime/images/sha256-db6086be92f439b918c96dc002f4cf40239e247f0b1b6c32e3fb36de70032bf9?context=explore">specific container image
for torch
v1.7.1</a>
and pull it down with:</p>

<pre><code class="language-shell">singularity pull docker://pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime
</code></pre>

<h2 id="running-the-style_transfer-python-script">Running the <code>style_transfer</code> python script</h2>

<p>I’d already cloned the neural style transfer repo, so I can follow the
instructions in that
<a href="https://github.com/crowsonkb/style-transfer-pytorch/blob/master/README.md">README.md</a></p>

<pre><code class="language-shell">$ singularity shell pytorch_1.7.0-cuda11.0-cudnn8-runtime.sif 
Singularity&gt; cd style-transfer-pytorch/
Singularity&gt; pip install --user .
</code></pre>

<p>I got a bunch of warnings about certain things not being on the <code>$PATH</code>, but it
seems to finish installing everthing ok.</p>

<p>Continuing on with the instructions in the README, let’s try running this thing
(I’d downloaded a couple of image files to use as my <em>content</em> and <em>style</em> images).</p>

<pre><code class="language-shell">Singularity&gt; style_transfer ben.jpg tiger.jpg -o ben-tiger.jpg
bash: style_transfer: command not found
</code></pre>

<p>Hmm, looks like those <code>$PATH</code> warnings were prescient. Looking back, the exact
warning was:</p>

<pre><code class="language-shell">WARNING: The script normalizer is installed in '/home/users/ben/.local/bin' which is not on PATH
</code></pre>

<p>The quickest &amp; dirtiest fix for this is to add that <code>/bin</code> directory to my path
and try and re-run the script.</p>

<pre><code class="language-shell">Singularity&gt; PATH="$PATH:~/.local/bin" style_transfer ben.jpg tiger.jpg -o ben-tiger.jpg
</code></pre>

<p>And away it went! Several minutes later, it was done. Here are the original two images:</p>

<p><img src="/assets/images/headshots/headshot.jpg" alt="Original ben.jpg image" />
<img src="/assets/images/posts/tiger.jpg" alt="Original tiger.jpg" /></p>

<p>and here’s the output:</p>

<p><img src="/assets/images/posts/ben-tiger.jpg" alt="style-transferred ben-tiger.jpg" /></p>

<p>Success…ish. Clearly I need to keep tweaking parameters &amp; input images to come
up with an output that’s actually <em>good</em>, but at least that journey can now
begin.</p>

<h2 id="but-is-it-fast">But is it <em>fast</em>?</h2>

<p>Actually, that declaration of success is a bit premature. At the top of the
output I noticed that the script was running on the CPU, not the GPU.</p>

<pre><code class="language-shell">Singularity&gt; PATH="$PATH:~/.local/bin" style_transfer ben.jpg tiger.jpg -o ben-tiger.jpg
~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() &gt; 0
Using devices: cpu
CPU threads: 128
Loading model...
</code></pre>

<p>That’s really not ok—the whole point of running on this machine is to take
advantage of the GPUs. There could be lots of reasons for this, but I have a
hunch it has something to do with Singularity not allowing the script access to
the hardware. Sure enough, looking through the <a href="https://sylabs.io/guides/3.7/user-guide/gpu.html">Singularity GPU support
documentation</a> it turns out
there’s a magic <code>--nv</code> flag which must be passed when starting up the
Singularity session, so let’s do that.</p>

<pre><code class="language-shell">$ singularity shell --nv pytorch_1.7.1-cuda11.0-cudnn8-runtime.sif 
Singularity&gt; PATH="$PATH:~/.local/bin" style_transfer ben.jpg tiger.jpg -o ben-tiger.jpg
Using devices: cuda:0
~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:143: UserWarning: 
NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.
If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(incompatible_device_warn.format(device_name, capability, " ".join(arch_list), device_name))
GPU 0 type: NVIDIA GeForce RTX 3090 (compute 8.6)
GPU 0 RAM: 24268 MB
Loading model...

*error traceback intensifies*
</code></pre>

<p>Well, that’s progress. Looking through the output I can see</p>

<pre><code class="language-shell">Using devices: cuda:0
GPU 0 type: NVIDIA GeForce RTX 3090 (compute 8.6)
GPU 0 RAM: 24268 MB
</code></pre>

<p>so torch can now see the GPUs. However, the error message in the middle of that
output is now the problem:</p>

<blockquote>
  <p>NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the
current PyTorch installation.</p>

  <p>The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60
sm_70.</p>

  <p>If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check
the instructions at <a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></p>
</blockquote>

<p>Like I said earlier, torch/tensorflow/CUDA and deep learning frameworks in
general are really finnicky about versions. It’s tricky to get things up and
running so that (i) all the versions work together and (ii) the changes you make
don’t break the delicate version relationships between other deep learning
projects you want to run on the same system<sup id="fnref:singularity-isolation" role="doc-noteref"><a href="#fn:singularity-isolation" class="footnote" rel="footnote">1</a></sup>.</p>

<p>After a web search, it seems <a href="https://github.com/pytorch/vision/issues/4886">like others
</a> <a href="https://discuss.pytorch.org/t/geforce-rtx-3090-with-cuda-capability-sm-86-is-not-compatible-with-the-current-pytorch-installation/123499">have
had</a>
<a href="https://github.com/crowsonkb/style-transfer-pytorch/issues/1#issuecomment-769701949">similar
issues</a>,
although I tried all the approaches listed there and none of them worked.</p>

<h2 id="using-a-pytorch-image-from-nvidias-container-registry">Using a pytorch image from NVIDIA’s container registry</h2>

<p>Changing tack a bit (after a suggestion from a colleague) I decided to try using
a (Docker) <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch">container image from the NVIDIA
registry</a>, rather
than the official pytorch channel on Docker Hub.</p>

<pre><code class="language-shell">$ singularity pull docker://nvcr.io/nvidia/pytorch:22.01-py3
$ singularity shell --nv pytorch_22.01-py3.sif 
Singularity&gt; pip install --user .
</code></pre>

<p>Now, let’s try running the <code>style_trasfer</code> script one more time:</p>

<pre><code class="language-shell">Singularity&gt; PATH="$PATH:~/.local/bin" style_transfer ben.jpg tiger.jpg -o ben-tiger.jpg
Using devices: cuda:0
GPU 0 type: NVIDIA GeForce RTX 3090 (compute 8.6)
GPU 0 RAM: 24268 MB
Loading model...
Processing content image (128x85)...
</code></pre>

<p>Hooray! It works, and runs, like 10000x faster on the GPU.</p>

<h2 id="open-questions">Open questions</h2>

<p>I really was just “hacking it until it worked” during this process, so I have a
few open questions.</p>

<ul>
  <li>
    <p>What’s the “persistance” story with the singularity images (<code>*.sif</code> files)? Is
it like docker, where I <code>singularity shell</code> in, do some things, but then any
changes I make in the shell (container?) don’t persist? It doesn’t seem like
that… but need to have a better mental model of how singularity images work.</p>
  </li>
  <li>
    <p>I didn’t use <a href="https://virtualenv.pypa.io/en/latest/">venvs</a> or
<a href="https://docs.conda.io/en/latest/">conda</a> or
<a href="https://python-poetry.org/docs/">poetry</a> or any of the things I’d usually use
when python-ing on my own machine, partially because of my above questions
about how the whole singularity shell thing actually works. I just did <code>pip
install --user .</code> and hoped it didn’t break anything else. Is that ok? Or
should I still use venvs in the singularity image?</p>
  </li>
</ul>

<p>I will return and try and better understand these things later, but right now
this isn’t on the critical path for me so I’ll have to park it. This blog post
is really just me opening a ticket for myself to return to later. I share it so
that you, dear reader, can also benefit from my mistakes (and if you know of
better ways to do any of this then do <a href="mailto:ben.swift@anu.edu.au">drop me a
line</a>.</p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:singularity-isolation" role="doc-endnote">

      <p>I had hoped that Singularity might help with the “isolation” part of this,
but I’m not sure I understand it well enough yet to know how to do it. <a href="#fnref:singularity-isolation" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="tools" /><category term="ai" /><summary type="html"><![CDATA[I’ve recently been given access to a beefy AI server (6x RTX3090s!) which is managed via SingularityCE, whose homepage boldly asks and then forgets to answer the question: “What is SingularityCE?”]]></summary></entry><entry><title type="html">Cybernetic futures explained (maybe)</title><link href="https://benswift.me/blog/2022/01/20/cybernetic-futures-explained/" rel="alternate" type="text/html" title="Cybernetic futures explained (maybe)" /><published>2022-01-20T00:00:00+11:00</published><updated>2022-01-20T00:00:00+11:00</updated><id>https://benswift.me/blog/2022/01/20/cybernetic-futures-explained</id><content type="html" xml:base="https://benswift.me/blog/2022/01/20/cybernetic-futures-explained/"><![CDATA[<p class="hl-para">One of my current projects at the ANU School of Cybernetics is to develop tools
&amp; procedures for futuring. This post is an attempt to get my head around how
these things fit together (spoiler: they do!).</p>

<h2 id="futures">Futures</h2>

<p>Futures/futuring<sup id="fnref:terminology" role="doc-noteref"><a href="#fn:terminology" class="footnote" rel="footnote">1</a></sup> is <em>a thing</em>—see <a href="https://www.howtofuture.com">Smith and
Ashby</a> for a practical guide or
<a href="https://www.press.uillinois.edu/books/?id=p084690">Powers</a> for a more critical
history and review. It’s the idea and practice of futuring as a verb, and this
video from the <a href="https://www.iftf.org">Institute for the Future</a> is a good
articulation of the “pitch”:</p>

<div style="width:100%; margin: 0px auto; margin-bottom: 1rem;">
  <div style="position: relative; padding-bottom: 62.5%; height: 0px;">
	<iframe style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%;" src="https://www.youtube.com/embed/5_EsLu4qydw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
	</iframe>
  </div>
</div>

<p>Just so I’m clear up-front: I think that futuring is a genuinely useful tool in
the toolbelt of any individual or organisation trying to figure out what success
looks like and how to achieve it.</p>

<p>One diagram which is often used in futuring is the <a href="https://thevoroscope.com/publications/foresight-primer/">futures
cone</a>, which helps
visualise the relationship between the <em>now</em> and the different potential
<em>futures</em> which might eventuate.</p>

<p><img src="/assets/images/posts/cybernetic-futures/futures-cone.png" alt="The futures cone" /></p>

<p>This diagram is just a visual aid—the future doesn’t really exist as a series
of concentric discs of soothing colours—but it helps to anchor discussions we
might have and predictions we might make about the future. In this sense, any
prediction or “vision” of the future is a single <strong>point</strong> in the futures cone,
and exactly where it falls in the space of possible, plausible, probable, or
preferable futures is part of the discussion. Think about it as <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">monte
carlo</a> of the future:</p>

<p><img src="/assets/images/posts/cybernetic-futures/futures-sampling.png" alt="An example distribution of potential futures in the futures cone" /></p>

<p>One thing to note here is that futuring is not about predicting the future. In
many ways it makes one less certain about the future; futuring requires a
healthy dose of epistemic humility, but that’s ok. It’s hard to expect the
unexpected and predict the unpredictable, so these sampled points (potential
futures) are really just a way of thinking about the current state of the world,
and especially how we might act and position ourselves in the world now to make
the most of future opportunities and steer towards the more desirable potential
futures.</p>

<h2 id="cybernetics">Cybernetics</h2>

<p><a href="https://en.wikipedia.org/wiki/Stafford_Beer">Stafford Beer</a> tells a joke about
defining cybernetics in <a href="https://www.emerald.com/insight/content/doi/10.1108/03684920210417283/">an address he gave at the University of
Valladolid</a>:</p>

<blockquote>
  <p>…it concerns three men who are about to be executed. The prison governor
calls them to his office, and explains that each will be granted a last
request. The first one confesses that he has led a sinful life, and would like
to see a priest. The governor says he thinks he can arrange that. And the
second man? The second man explains that he is a professor of cybernetics. His
last request is to deliver a final and definitive answer to the question: what
is cybernetics? The governor accedes to this request also. And the third man?
Well, he is a doctoral student of the professor—his request is to be
executed second.</p>
</blockquote>

<p>It’s a great joke, grounded in a deep truth. One way to define cybernetic
systems is as systems with (i) a purpose/goal and (ii) a mechanism for steering
towards<sup id="fnref:towards" role="doc-noteref"><a href="#fn:towards" class="footnote" rel="footnote">2</a></sup> said purpose. In some cases there’s a defined end state, where
upon attaining said purpose victory is declared and the job is done. However in
many cases what’s desirable is
<a href="https://en.wikipedia.org/wiki/Homeostasis">homeostasis</a>, i.e. a system which
can keep itself “in its happy place”, stable and resistant to peturbations.</p>

<p><img src="/assets/images/posts/cybernetic-futures/evgeni-tcherkasski-SHA85I0G8K4-unsplash.jpg" alt="A lighthouse on the shore" /></p>

<p>There are lots of potential illustrations of this idea, but one that many of my
cybernetic forebears liked is the one of using a lighthouse to steer a ship.
<a href="https://history-computer.com/the-complete-guide-to-cybernetics/">Here’s</a> a nice
explanation:</p>

<blockquote>
  <p>In ancient Greece, the
<a href="https://en.wikipedia.org/wiki/Cybernetics#Etymology"><em>Kubernetes</em></a>
[navigator/helmsperson] was in charge of controlling the Grecian longships.
The ships had to be steered through all kinds of unpredictable forces,
including wind, waves, storms, currents, and tides. The Greeks found that they
could ignore all of these and control the ship via a small tiller connected to
the ship’s larger rudder just by pointing the tiller toward a fixed object in
the distance, such as a lighthouse, and making adjustments in real-time.</p>
</blockquote>

<p>There’s a <a href="https://www.youtube.com/watch?v=iXmlbd86YGA">YouTube “What is
cybernetics?”</a> video which includes
a diagram like this:</p>

<p><img src="/assets/images/posts/cybernetic-futures/kubernetes-steering-procedure.png" alt="Diagram of the kubernetes' steering procedure" /></p>

<p>It’s not the particular position of the tiller at any one time that’s important,
is the way that the navigator watches the lighthouse and moves the tiller in
response (as the ship is affected by currents &amp; winds). If the ship’s bow is
pointing to one side of the lighthouse, then adjust the tiller in the opposite
direction until it does. If you keep up that simple procedure, you’ll get there
in the end, and with your ship in one piece.</p>

<p>It’s the navigator’s continued monitoring of the difference between the
purpose/goal (as indicated by the lighthouse) and the current state (as
indicated by the where the bow is pointing) which matters. Once you know how to
respond to that difference, by steering in the opposite direction of that
difference, then you’ve got a simple and reliable procedure for successful
sailing.</p>

<p>Here’s the key point: cybernetic systems don’t work by planning out a complex,
go-to-whoa list of actions to take and then mindlessly following them. Instead
they use a simpler process involving <em>feedback</em>: do a thing,
looking/listening/sense what happened, compare the new state of the world with
the goal, and then do another thing… and so on<sup id="fnref:all-encompassing" role="doc-noteref"><a href="#fn:all-encompassing" class="footnote" rel="footnote">3</a></sup>.</p>

<p><img src="/assets/images/posts/cybernetic-futures/remy-gieling-n_QECf2Qm4E-unsplash.jpg" alt="A radar dish" /></p>

<p>Here’s another example of a feedback-powered system. The development of radar in
WWII was deeply connected to the birth of cybernetics (as detailed by Thomas Rid
in <a href="https://wwnorton.com/books/Rise-of-the-Machines/">Rise of the Machines</a>
Chapter 1). Take it away, <a href="https://en.wikipedia.org/wiki/Radar">Wikipedia</a>:</p>

<blockquote>
  <p>A radar system consists of a
<a href="https://en.wikipedia.org/wiki/Transmitter" title="Transmitter">transmitter</a>
producing <a href="https://en.wikipedia.org/wiki/Electromagnetic_wave" title="Electromagnetic
wave">electromagnetic
waves</a> in the <a href="https://en.wikipedia.org/wiki/Radio_spectrum" title="Radio
spectrum">radio</a> or <a href="https://en.wikipedia.org/wiki/Microwave" title="Microwave">microwaves</a> domain, a transmitting
<a href="https://en.wikipedia.org/wiki/Antenna_(radio)" title="Antenna (radio)">antenna</a>, a
receiving antenna (often the same antenna is used for transmitting and
receiving) and a <a href="https://en.wikipedia.org/wiki/Radio_receiver" title="Radio receiver">receiver</a> and
<a href="https://en.wikipedia.org/wiki/Data_processing_system" title="Data
processing system">processor</a> to determine properties of the object(s). Radio waves
(pulsed or continuous) from the transmitter reflect off the object and return
to the receiver, giving information about the object’s location and speed.</p>
</blockquote>

<p>The radar sends out the pulses, which bounce (reflect) off the environment—and
these these reflections are sufficient (with some <a href="https://nato-us.org/analysis2000/papers/moran.pdf">tricky
maths</a>) to figure out what
the environment looks like. Often there’s some sort of visual representation of
the results, like the classic “beeping dots on concentric circles” radar sweep
interface you’ll know from the movies.</p>

<p>The radar example is different from the ship steering one in that lighthouses
don’t really move/change (although the currents in the water &amp; other
environmental factors do… hence the need for the feedback-powered steering
procedure). Using the radar properly requires sending an ongoing series of
pulses, because they each give an indication of what the environment looked like
when the pulses were reflected, but to track moving objects you need to monitor
the dots over time to see how the environment changes.</p>

<h2 id="cybernetic-futures">Cybernetic futures</h2>

<p>Returning to futuring; each future scenario you can imagine (regardless of where
it falls in the futures cone) is a way of projecting a potential “end state”.
That end state doesn’t have to be desirable—many potential futures
aren’t—but it gives a “concrete” thing against which to compare your current
state to inform your current actions. Even the more mundane acts of management
and oversight—strategy, tactics, contingency planning—when done well they
all involve articulating goals and thinking about ways to bring them about.
There’s clearly an echo of the “steering systems” thing here.</p>

<p>However, one pitfall of that sort of mental picture of the future is that
futuring looks like a three step process:</p>

<ol>
  <li>come up with (sample) a bunch of potential futures from the futures cone (the
points in the diagram above)</li>
  <li>pick one of the potential futures you like the most (from the <em>preferable</em>
area of the futures cone)</li>
  <li>steer towards<sup id="fnref:lighthouse-steering" role="doc-noteref"><a href="#fn:lighthouse-steering" class="footnote" rel="footnote">4</a></sup> it like a lighthouse (whatever that looks
like)</li>
</ol>

<p><img src="/assets/images/posts/cybernetic-futures/steering-towards-preferred-future.png" alt="Steering towards a preferred future" /></p>

<p>That’s an unhelpful picture of what futuring is because it implies that the
potential future you’re steering towards is solid &amp; stationary, but that’s just
not how potential futures work. Instead, I think that futuring works best when
it’s more like a radar:</p>

<ol>
  <li>come up with (sample) a bunch of potential futures from the futures cone
(send out radar pulses)</li>
  <li>look for their reflections in the present</li>
  <li>analyse these reflections to decide how to act</li>
  <li>goto step 1 (because things have now changed)</li>
</ol>

<p><img src="/assets/images/posts/cybernetic-futures/futures-as-radar.png" alt="Futuring as a radar" /></p>

<p>As well as giving a different mental model of what futuring is and isn’t, I
think there are two implications of this switch in perspective.</p>

<ol>
  <li>
    <p>Just like a ship drifts on the currents, our orientation to these potential
futures is constantly changing as we &amp; others act in the present—we’re
agents; we have agency. So we need to constantly re-examine our orientation
towards these multiple futures and reorient ourselves as a result. That’s a
“radar-like” model of futuring. Cybernetics doesn’t provide a “once you
measure &amp; model all the things you can predict the future” silver bullet
(although it’s not like some folks haven’t tried <a href="https://eujournalfuturesresearch.springeropen.com/articles/10.1007/s40309-013-0029-y">and
failed</a>),
but rather it’s a commitment to using a simple “sense-analyze-act” feedback
loop to keep the system on track.</p>
  </li>
  <li>
    <p>Don’t be fooled by the simplicity of the steering procedure in the
ship/lighthouse example; the radar example shows that making sense of the
feedback from the environment can require non-trivial analysis before it’s
useful.</p>
  </li>
</ol>

<p>To close, I want to stress that I’m not saying anything remotely new here about
the connection between futuring and cybernetics—they’re very often seen &amp;
discussed together. William Gibson, award-winning sci-fi novelist and sometime
futurist was <a href="http://www.nytimes.com/2007/08/19/magazine/19wwln-q4-t.html">steeped in cybernetic
lore</a> when he
<a href="https://www.themarginalian.org/2014/08/26/how-william-gibson-coined-cyberspace/">coined the term
“cyberspace”</a>.
My boss <a href="https://cybernetics.anu.edu.au/people/genevieve-bell/">Genevieve Bell
AO</a>, director of the <a href="https://cybernetics.anu.edu.au">ANU
School of Cybernetics</a>, has thought deeply and
written persuasively and generally projected big futuring energy for pretty much
her whole career.</p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:terminology" role="doc-endnote">
      <p>or foresight, or forecasting… there are a few terms which are used relatively interchangeably <a href="#fnref:terminology" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:towards" role="doc-endnote">

      <p>The system’s purpose can also be stated negatively, e.g. avoiding pain. So
towards/away from are equally valid when talking about purpose. <a href="#fnref:towards" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:all-encompassing" role="doc-endnote">

      <p>If that definition sounds all-encompassing, you’re not the first person to
notice that. Cybernetics <a href="https://www.pangaro.com/cybernetics-the-center-of-sciences-future.html">isn’t
shy</a>
about claiming all things as within its purview—Wiener was an ersatz
theologian. <a href="#fnref:all-encompassing" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:lighthouse-steering" role="doc-endnote">

      <p>Yeah, I know, you don’t steer towards the lighthouse exactly—they just
<a href="https://adventure.howstuffworks.com/lighthouse.htm">show you where the rocks and reefs
are</a>. Don’t @ me. <a href="#fnref:lighthouse-steering" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="cybernetics" /><summary type="html"><![CDATA[One of my current projects at the ANU School of Cybernetics is to develop tools &amp; procedures for futuring. This post is an attempt to get my head around how these things fit together (spoiler: they do!).]]></summary></entry><entry><title type="html">Livecoder-in-the-club as a system</title><link href="https://benswift.me/blog/2021/11/11/livecoder-in-the-club-as-a-system/" rel="alternate" type="text/html" title="Livecoder-in-the-club as a system" /><published>2021-11-11T00:00:00+11:00</published><updated>2021-11-11T00:00:00+11:00</updated><id>https://benswift.me/blog/2021/11/11/livecoder-in-the-club-as-a-system</id><content type="html" xml:base="https://benswift.me/blog/2021/11/11/livecoder-in-the-club-as-a-system/"><![CDATA[<div class="hl-para">

  <p>Here in the <a href="https://3ainstitute.org">School of Cybernetics</a> we are building our
capability in cybernetics—its histories and possibilities—and
working out how each of us will contribute to the new cybernetics for the 21st
Century.</p>

  <p>This blog post, written for a general audience, is part of a content development
sprint, written in reponse to the task of developing a short (1000 words)
persuasive argument about the role and value of cybernetics as an approach to
shape futures through and with technology.</p>

</div>

<p>If you follow this blog, you’ll know that I’m a <a href="/livecoding/index.html">livecoder</a>. I <em>code</em> (i.e. write computer programs) to make music in a
club setting, with an audience that just wants to dance and have a good
time<sup id="fnref:niche" role="doc-noteref"><a href="#fn:niche" class="footnote" rel="footnote">1</a></sup>. Between the code, the thumping music, the dancing humans, and all
the other glorious complexities of live entertainment, there’s certainly a lot
of different stuff going on. You might find watching a livecoder in action to be
entertaining, or impressive, or bewildering, or all of the above. Most of all,
when you see/attend a livecoding gig for the first time, I bet that your initial
feeling is one of <em>what is going on?</em></p>

<p>In this post I’m going to use some ideas from cybernetics to try and help you
make sense of a livecoder performing in a club, in part to help you understand
for what it <em>feels like</em> I’m doing when I do it. From there I want to think
about ways to make livecoding even better, that is, to figure out put on a
better show for the adoring crowds.</p>

<h2 id="livecoder-in-the-club-as-a-system">Livecoder-in-the-club as a system</h2>

<p><a href="https://www.youtube.com/watch?v=kx79HLLboT8">Cybernetics</a> is all about the
looking at and reasoning about <strong>systems</strong> with <em>goals</em>, interacting with and
connected to their <em>environment</em> via perception/action <em>feedback loops</em>. These
sorts of systems exist at all sorts of different scales (big/small, fast/slow,
old/new, cheap/expensive, etc.) and they’re <a href="https://www.goodreads.com/book/show/10698938-the-fractal-organization">fractal in
nature</a>—it
doesn’t matter what level of “magnification” you look at, each component of a
system is itself a system of interacting components, and each system is itself a
component interacting in a larger system. But since that’s all pretty abstract,
let’s return to the example of the livecoder-in-the-club. This is written in the
first person, but other livecoders may have similar understandings of their own
livecoder-in-the-club practice.</p>

<ul>
  <li>
    <p>I start with a full tank of <strong>brain juice</strong> which allows me to work on tricky
coding problems. But it’s mentally taxing. When I’m happy, rested &amp; in the
zone, I feel like I’ve got a full tank, but writing code takes mental energy,
and so writing the code in the performance drains my brain juice until I’m
cooked, and then I can’t write any more code (or at least will write bad/buggy
code) until I recharge.</p>
  </li>
  <li>
    <p>To write the <strong>code</strong> I tap my fingers on the keys of my keyboard. I use a
<a href="https://emacs.sexy">specialised program</a> for this (i.e. I don’t write it in
MS Word) which has a bunch of features to help, like different colours for the
different parts of the code (e.g. functions vs variable vs numerical parameter
values), and auto-completion, and inline documentation/help about the
particular bit of code that I’m working on. This code is also projected onto a
big screen in the club so that the dancers can look at it (or not).</p>
  </li>
  <li>
    <p>As the code runs, it generates <strong>music</strong>. Different parts of the code are
responsible for different parts of the music, and I try and give the functions
&amp; variables in my code human-readable names (like <code>piano</code>) so that the
correspondence between the code and the music is clear-ish. The music will
only be generated if the code is running nicely (i.e. without bugs/errors) and
is hooked up to the PA system in the club. If I crash the program (or if
someone unplugs the PA) then the music will stop.</p>
  </li>
  <li>
    <p>The people in the club—people dancing, people chilling at the bar, people
watching the code on the screen—are collectively having an experience which
(hopefully) is giving them <strong>good vibes</strong>. Obviously this is a <em>huge</em>
oversimplification, and the extent to which any individual is enjoying
themselves (and therefore contributing positively to the amount of good vibes
in the room) depends on all sorts of things. But, in a real sense, the
creation of <strong>good vibes</strong> in the room is the goal of the live coder—or at
least it’s <em>my</em> goal when I perform in this situtation. So I (like any
performer) feed off the good vibes, replenishing (to some extent) my brain
juice.</p>
  </li>
</ul>

<p>It’s a bit clearer to see in a picture:</p>

<picture style="position: relative;">
  <img alt="system diagram for the livecoder-in-a-club system" src="/assets/images/posts/livecoder-in-a-club-system-diagram.png" />

  
</picture>

<p>So, clearly,</p>

<blockquote>
  <p>a livecoder is a machine for turning <strong>brain juice</strong> into <strong>good vibes</strong> (via
<strong>code</strong> and <strong>music</strong>)</p>
</blockquote>

<p>which is a nice way to think about it, and actually is a relatively accurate
picture of what I <em>feel</em> is going on when I’m performing.</p>

<h2 id="how-does-cybernetics-help-us-understand-and-improve-this-system">How does cybernetics help us understand and improve this system?</h2>

<p>In this account of livecoding-in-the-club, there are a few things worth
noticing:</p>

<ul>
  <li>
    <p>there are different “stocks” (reservoirs of brain juice, code, music and good vibes)</p>
  </li>
  <li>
    <p>there are various flows between those stocks, and in both directions (e.g. I
turn brain juice into code by typing at my keyboard, but I also receive
information about what the code looks like from my laptop screen, via my eyes)</p>
  </li>
  <li>
    <p>there’s a goal: to put on a good show for the audience to enjoy (to increase
the stock of good vibes in the room)</p>
  </li>
  <li>
    <p>the system includes closed loops, and so is capable of feedback</p>
  </li>
</ul>

<p>A key principle of cybernetics is that the structure of the system—what the
parts are, and how they relate to one another—determines the behaviour of the
system. But anyone can sketch out a (highly contestable) jumble of blobs and
arrows to describe whatever thing they’re interested in. What do we gain from
seeing things in this way?</p>

<p>This is where another key idea—and person—in cybernetics/systems thinking
comes in: Donella Meadows’<sup id="fnref:is-meadows-cybernetics" role="doc-noteref"><a href="#fn:is-meadows-cybernetics" class="footnote" rel="footnote">2</a></sup> <a href="https://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/">Leverage Points: Places to
Intervene in a
System</a>.
The key idea is this: once you’ve mapped out your system, you need to know where
the most effective “intervention points” to try and implement change? If you’re
going to expend energy to make things better, where should you focus that energy
to get the most <em>leverage</em>?</p>

<p>In the livecoder-in-the-club system, to make changes to the system in service of
the the goal (as stated above) of creating maximum good vibes. One obvious
solution is to start the gig with a bigger reservoir of brain juice (either by
having a good night’s sleep, popping an adderall, or whatever). Or it could be
to start with a larger code reservoir by starting with a bunch of code
pre-written<sup id="fnref:blank-slate-code" role="doc-noteref"><a href="#fn:blank-slate-code" class="footnote" rel="footnote">3</a></sup>.</p>

<p>However, Meadows’ rules for leverage also suggest that some interventions
provide more leverage than others<sup id="fnref:different-interventions" role="doc-noteref"><a href="#fn:different-interventions" class="footnote" rel="footnote">4</a></sup>. For example,
changing the flow rates (leverage point #10) is likely to have more impact than
just changing the sizes of the stocks/buffers (leverage point #11). This implies
that changing the rate at which I turn brain juice into code (perhaps having a
nicer keyboard, perhaps having better code auto-completion support, or perhaps
just good-ol’ <em>practice</em> to improve my coding skills) is likely to be more
impactful than starting with a bigger store of brain juice (so, thankfully,
there’s no need to buy shady adderall on the dark web). Will Larson (who has
been a software engineering leader at Calm, Stripe, Uber, and Digg) <a href="https://lethain.com/systems-thinking/">has some
interesting ideas on systems thinking as applied to software
development</a> that I’m keen to think more
about as well.</p>

<p>For even greater leverage, there are interventions which are related to
restructuring the system itself, for example adding new information flows
(leverage point #6). The dancers can already see the code, but what if I was
hooked up to a live <a href="https://en.wikipedia.org/wiki/Electroencephalography">EEG</a>
so they could see the current state of my brain juice?<sup id="fnref:eeg" role="doc-noteref"><a href="#fn:eeg" class="footnote" rel="footnote">5</a></sup>. And even higher up (in
terms of leverage) is changing the goals of the system itself (leverage point
#3). Why <em>do</em> people come to a club to dance and have good vibes? What if their
goal was different?</p>

<p>Now, the thing about leverage is that it doesn’t guarantee good or bad outcomes,
it just means you for a small amount of input you see a large effect in the
output. Figuring out where to intervene in the livecoder-in-the-club system is
one thing, figuring out how to intervene so that the changes are positive is a
deep challenge. Leverage means that when things go well they go really well, but
the opposite is also true (e.g. with margin calls in a bear market). I feel like
this is an especially apposite point for programmers, because the cheap leverage
afforded by software is catnip for programmers, but presents some real dangers.
(as <a href="https://idlewords.com/talks/sase_panel.htm">as Maciej Cegłowski puts so
eloquently</a>).</p>

<h2 id="so-whats-the-point">So what’s the point?</h2>

<p>Obviously the livecoder-in-the-club system described above is an
oversimplification; it makes certain things easy to see but renders other things
invisible, and every aspect of both the components (the things it talks about)
and their relationships (the connections between them) is contestable. But
that’s one of the benefits by laying things out like this—we can at least see
the things that we’re explicitly considering, and we may well need to add new
things to the model for consideration (and examine all the new connections and
potential feedback loops those new things create).</p>

<p>My main goal here is really just to provide a worked example of how ideas from
cybernetics and systems thinking can help us move beyond just describing things
to figuring out where to place our energies to effect change—where we’ll get
the most leverage. Being the best livecoder I can be is a lifetime goal, just
like any other instrumental or artistic practice. I’m keen to keep using the
tools of cybernetics to push in that direction, and bring the assemblage of
dancing bodies of the livecoder-in-the-club system with me for the ride :)</p>

<h2 id="meadows-12-places-to-intervene">Appendix: Meadows’ <em>12 Places to Intervene in a System</em></h2>

<p class="hl-para">Note: these are taken straight from the <a href="https://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/">Donella Meadows foundation
website</a>.</p>

<p><em>(lower numbers = less effective, higher numbers = more effective)</em></p>

<ol reversed="">

<li>Constants, parameters, numbers (such as subsidies, taxes, standards).</li>

<li>The sizes of buffers and other stabilizing stocks, relative to their flows.</li>

<li>The structure of material stocks and flows (such as transport networks, population age structures).</li>

<li>The lengths of delays, relative to the rate of system change.</li>

<li>The strength of negative feedback loops, relative to the impacts they are trying to correct against.</li>

<li>The gain around driving positive feedback loops.</li>

<li>The structure of information flows (who does and does not have access to information).</li>

<li>The rules of the system (such as incentives, punishments, constraints).</li>

<li>The power to add, change, evolve, or self-organize system structure.</li>

<li>The goals of the system.</li>

<li>The mindset or paradigm out of which the system — its goals, structure, rules, delays, parameters — arises.</li>

<li>The power to transcend paradigms.</li>

</ol>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:niche" role="doc-endnote">

      <p>It’s a pretty niche activity, but there’s an <a href="https://toplap.org">international
community</a> of us, and if you’re interested then you can
<a href="https://twitter.com/benswift">follow me on twitter</a> to hear about upcoming
gigs. <a href="#fnref:niche" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:is-meadows-cybernetics" role="doc-endnote">

      <p><a href="https://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/">Donella
Meadows</a>
tended to prefer terms like “systems thinking” and “systems change” rather
than using the term “cybernetics” directly, but she certainly was involved
with some of the key people &amp; events in the cybernetics story, and her work
is highly relevant to cybernetic ideas. Plus, <a href="https://books.google.com.au/books/about/Thinking_in_Systems.html?id=leE8R9pehg4C&amp;redir_esc=y">Google Books categorises her
work under <em>Computers &gt;
Cybernetics</em></a>,
and you <em>know</em> the Big G is never wrong about that stuff. <a href="#fnref:is-meadows-cybernetics" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:blank-slate-code" role="doc-endnote">

      <p>This is actually a subtle point in livecoding. I (along with some other
livecoders) am committed to starting each gig “from scratch” with a blank
code page. However, I’ve written a lot of library code ahead of time to
provide me with nice abstractions for making music with code, and I use that
(hidden—not on the screen) from the very first line of code that I write.
Thinking about the livecoder-in-the-club system one question that I’m
pondering is whether that library code constitutes a larger stock of code,
or whether it’s a restructuring (an increase) of the flow rate from code
into music, or both.</p>

      <p>To make things even more complicated, and there’s <em>kindof</em> a blurry line
between where the code ends and the music begins in livecoding (i.e. there’s
the code you see on the screen, which is the code that I’m writing &amp;
executing “live”, but there’s also a bunch of pre-written code in my
operating system’s audio plumbing just to get the music to come out of the
speakers properly). <a href="#fnref:blank-slate-code" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:different-interventions" role="doc-endnote">

      <p>There’s not enough room in this blog post for a full “systems change
analysis” of the livecoder-in-the-club system according to all 12 leverage
points, but if you’re interested I do recommend you <a href="https://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/">check out that
article</a>
as a starting point. <a href="#fnref:different-interventions" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:eeg" role="doc-endnote">

      <p>Coming up with a reliable, portable EEG machine which can measure a useful
biometric signal which corresponds to an individual’s perceived current
level of brain juice is beyond the scope of this blog post. <a href="#fnref:eeg" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="cybernetics" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Cutting ruby CI pipeline times with pre-installed bundles</title><link href="https://benswift.me/blog/2021/10/21/cutting-ruby-ci-pipeline-times-with-pre-installed-bundles/" rel="alternate" type="text/html" title="Cutting ruby CI pipeline times with pre-installed bundles" /><published>2021-10-21T00:00:00+11:00</published><updated>2021-10-21T00:00:00+11:00</updated><id>https://benswift.me/blog/2021/10/21/cutting-ruby-ci-pipeline-times-with-pre-installed-bundles</id><content type="html" xml:base="https://benswift.me/blog/2021/10/21/cutting-ruby-ci-pipeline-times-with-pre-installed-bundles/"><![CDATA[<p>I (and, increasingly many of my colleagues) are using
<a href="https://jekyllrb.com">Jekyll</a> to create open (CC-licenced), hackable, acessible
course websites &amp; teaching content for our classes. We use a self-hosted GitLab
server for all the websites sources, and then build/deploy them with <a href="https://docs.gitlab.com/ee/ci/">GitLab
CI</a>. It works well, it means I don’t have to
fight with our LMS to do interesting things, and it means I can open my learning
materials to everyone (not just those who are privileged enough to be able to
pay the fees to study at the ANU).</p>

<p>The <code>jekyll build</code> step runs in a container, and for a long time we’ve just used
the <a href="https://hub.docker.com/_/ruby/">official ruby image</a> as a starting point,
then done a <code>bundle install</code> inside the container before running the build step
to get all the deps. However, this means the deps are <em>installed from scratch on
every deploy</em>, which isn’t the greenest (although ANU is <a href="https://www.anu.edu.au/research/research-initiatives/anu-below-zero">heading in a good
direction on net
zero</a>) and
it also means the feedback loop from push-&gt;deployed site is much longer than it
needs to be.</p>

<p>Yesterday (prompted by the understandable frustrations of my colleague
<a href="https://charlesmartin.com.au">Charles</a> about the build times) I spent some time
fixing things. I ended up creating a new docker image with the required gems
pre-installed, and it <strong>cut our CI pipeline times by up to 90%</strong> (i.e. a 10x
speedup).</p>

<p>There were a couple of tricky parts, so I include some commentary here in case
anyone else (including future me when if I forget how this works) wants to do
similar things.</p>

<pre><code class="language-Dockerfile"># Choose and name our temporary image.
FROM ruby:3.0.2 as builder

WORKDIR /app

# Take an SSH key as a build argument.
ARG SSH_PRIVATE_KEY

# required to pull from the (private) theme gem repos
# create the SSH directory.
RUN mkdir -p ~/.ssh/ &amp;&amp; \
  # populate the private key file.
  echo "$SSH_PRIVATE_KEY" &gt; ~/.ssh/id_rsa &amp;&amp; \
  # set the required permissions.
  chmod -R 600 ~/.ssh/ &amp;&amp; \
  # add our GitLab server to our list of known hosts for ssh.
  ssh-keyscan -t rsa gitlab.anu.edu.au &gt;&gt; ~/.ssh/known_hosts

# install the deps - this is really just for "caching", the expectation is that
# the CI job will re-run `bundle install` to pick up any differences
COPY Gemfile Gemfile.lock* .
RUN bundle install

# Choose the base image for our final image
FROM ruby:3.0.2
WORKDIR /app

# Copy across the files from our `builder` container
# this really assumes the same base container
COPY --from=builder $BUNDLE_APP_CONFIG $BUNDLE_APP_CONFIG
</code></pre>

<p>The main tricky bit is the ssh setup, because some of the (in-house) gems are
only available in git repos which require authentication. This Dockerfile pulls
in the SSH key from an environment variable, then uses it to <code>bundle install</code>
the required gems. Then, the key part is that there’s a second <code>FROM</code> command to
create a new image (sans any trace of the SSH key) and only the installed gems
are copied across.</p>

<p>To build the container, you need to do something like</p>

<pre><code class="language-shell">MY_KEY=$(cat gitlab-ci-runner-key)
docker build --build-arg SSH_PRIVATE_KEY="$MY_KEY" --tag YOUR_TAG_NAME .
</code></pre>

<p>A couple of caveats with this approach: the container just caches the gems; the
<code>bundle install</code> step will still (probably) need to run in the CI pipeline, but
it’ll be a no-op if <code>Gemfile.lock</code> hasn’t changed. You’ll never be worse off
(time-wise) than if you’re installing from scratch, because only the deps which
have changed in the lock file will be downloaded. But over time, the container
may take longer to run as the list of pre-installed vs actually required
packages diverges.</p>

<p class="hl-para">I did try a similar approach that used <code>bundle cache</code> to pull all the deps into
a <code>vendor/cache</code> folder and then copy <em>that</em> across into the new image, but I
had weird permissions errors that I didn’t have the time to figure out. If
you’ve got tips on whether that’s a more “bundler-y” way to do things then <a href="mailto:ben@benswift.me">hit
me up</a>.</p>

<p>I want to give a shoutout to Jan Akerman who wrote <a href="https://janakerman.co.uk/docker-git-clone/">a helpful blog
post</a> which got me started—and
some of the Dockerfile is taken from that post.</p>]]></content><author><name>Ben Swift</name></author><category term="tools" /><category term="web" /><summary type="html"><![CDATA[I (and, increasingly many of my colleagues) are using Jekyll to create open (CC-licenced), hackable, acessible course websites &amp; teaching content for our classes. We use a self-hosted GitLab server for all the websites sources, and then build/deploy them with GitLab CI. It works well, it means I don’t have to fight with our LMS to do interesting things, and it means I can open my learning materials to everyone (not just those who are privileged enough to be able to pay the fees to study at the ANU).]]></summary></entry><entry><title type="html">openconnect setup for institutional VPN access</title><link href="https://benswift.me/blog/2021/10/18/openconnect-setup-for-institutional-vpn-access/" rel="alternate" type="text/html" title="openconnect setup for institutional VPN access" /><published>2021-10-18T00:00:00+11:00</published><updated>2021-10-18T00:00:00+11:00</updated><id>https://benswift.me/blog/2021/10/18/openconnect-setup-for-institutional-vpn-access</id><content type="html" xml:base="https://benswift.me/blog/2021/10/18/openconnect-setup-for-institutional-vpn-access/"><![CDATA[<p>My <a href="https://www.anu.edu.au">institution’s</a> IT policies have recently changed and
port 22 is now blocked from off-campus. That’s a real pain if you use ssh to
push/pull from our on-prem GitLab servers (which I need to do <em>all the time</em>).</p>

<p>The recommended solution is to come in via a VPN—which is not a terrible idea
in principle. However, the institution’s recommended setup requires some janky
GlobalProtect client, which (for me at least) was pretty crashy. As an
alternative, <a href="http://www.infradead.org/openconnect/">openconnect</a> is just a
<code>brew install openconnect</code> away (on macOS, at least), and after a bit of setting
up, it works seamlessly.</p>

<p>If you’re in a similar situation, here’s a terminal command you can use to
access the campus network via the VPN:</p>

<pre><code>sudo openconnect \
  --user=uXXXXXXX \ ## replace with your uid
  --protocol=gp \   ## because it's a GlobalProtect VPN
  https://staff-access.anu.edu.au
</code></pre>

<p>The above command will prompt for your usual password, which you enter in the
terminal. Since I already have that info in an encrypted file, I have a slightly
modified setup (this is in a script called <code>vpn.sh</code>):</p>

<pre><code>#!/usr/bin/env zsh

# pull ANU password out of encrypted authinfo file, pipe it to stdin
gpg -q --for-your-eyes-only --no-tty -d ~/.authinfo.gpg | \
    awk '/machine smtp.office365.com login uXXXXXXX@anu.edu.au/ {print $NF}' | \
    # start the VPN
    sudo openconnect --user=uXXXXXXX --protocol=gp --passwd-on-stdin https://staff-access.anu.edu.au
</code></pre>

<p>Happy VPN-ing.</p>]]></content><author><name>Ben Swift</name></author><category term="tools" /><summary type="html"><![CDATA[My institution’s IT policies have recently changed and port 22 is now blocked from off-campus. That’s a real pain if you use ssh to push/pull from our on-prem GitLab servers (which I need to do all the time).]]></summary></entry><entry><title type="html">Feedback in livecoding: cui bono?</title><link href="https://benswift.me/blog/2021/09/13/feedback-in-livecoding-cui-bono/" rel="alternate" type="text/html" title="Feedback in livecoding: cui bono?" /><published>2021-09-13T16:22:00+10:00</published><updated>2021-09-13T16:22:00+10:00</updated><id>https://benswift.me/blog/2021/09/13/feedback-in-livecoding-cui-bono</id><content type="html" xml:base="https://benswift.me/blog/2021/09/13/feedback-in-livecoding-cui-bono/"><![CDATA[<p>Here’s the <a href="/blog/2021/09/13/feedback-in-livecoding-cui-bono/">link to the slides</a> for the actual
presentation on Oct 13.</p>

<p>Hi, BUILD students. I know y’all are busy, so this won’t take <em>too</em> long and
will hopefully be fun. Before my guest lecture, I’d like to introduce you (if
you haven’t seen it before) to <a href="https://toplap.org/about/">livecoding</a>. Imagine
you arrive at the club<sup id="fnref:pingers" role="doc-noteref"><a href="#fn:pingers" class="footnote" rel="footnote">1</a></sup>, and as well as the glow sticks and the pingers
and the heaving bodies on the dance floor instead of a “traditional” DJ you see
this:</p>

<div style="width:100%; margin: 0px auto; margin-bottom: 1rem;">
  <div style="position: relative; padding-bottom: 62.5%; height: 0px;">
	<iframe style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%;" src="https://www.youtube.com/embed/C2GH5JmJwhU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
	</iframe>
  </div>
</div>

<div style="width:100%; margin: 0px auto; margin-bottom: 1rem;">
  <div style="position: relative; padding-bottom: 62.5%; height: 0px;">
	<iframe style="position: absolute; top: 0px; left: 0px; width: 100%; height: 100%;" src="https://www.youtube.com/embed/a1RxpJkvqpY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
	</iframe>
  </div>
</div>

<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/592452477?color=be2edd" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen="" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="Ushini &amp;amp; Ben live @ ACMC&amp;#039;21"></iframe></div>
<script src="https://player.vimeo.com/api/player.js"></script>

<h2 id="what-can-cybernetics-teach-us-about-this">What can cybernetics teach us about this?</h2>

<p>Obviously there are lots of feedback loops in this CPS; involving
audience/room/livecoder/screens/lights/loudspeakers etc. But one of the things
that interests me most about livecoding is that it gives the club-goers a chance
to see (and hopefully to <em>care</em>) about the source code of a computer program.</p>

<p>There’s lots we could say about this from a cybernetics perspective, but the
things I want to focus on in this class is <em>feedback</em>.</p>

<p>Here are a few questions for you to think about ahead of class. In each
livecoding video:</p>

<ul>
  <li>what has the performer chosen to <em>show</em>?</li>
  <li>what has the performer chosen to <em>hide</em>?</li>
  <li>who is the feedback <em>for</em><sup id="fnref:cui" role="doc-noteref"><a href="#fn:cui" class="footnote" rel="footnote">2</a></sup>?</li>
  <li>in each case, why do you think those decisions were made?</li>
  <li>when watching, what did <em>you</em> want more/less/different feedback about?</li>
  <li>if you were the AV designer for Ushini &amp; my next livecoding set, what changes
would you make?</li>
  <li>how does a taking a cybernetic approach to this question change answer to
these questions? (because feedback is something that cybernetics has a fair
bit to say about)</li>
</ul>

<p>If you want to go a bit deeper on the semiotics of all this livecoding stuff, my
collaborators and I have written a couple of papers<sup id="fnref:gauche" role="doc-noteref"><a href="#fn:gauche" class="footnote" rel="footnote">3</a></sup> on the
semantics/semiotics of livecoding:</p>

<div class="bibliography"><div>

<p class="pubitem" id="sorensenManyMeaningsLive2014"><span class="title"><a href="https://doi.org/10.1162/COMJ_a_00230">The Many Meanings of Live Coding</a></span> <span class="date">(2014)</span></p>

<p>by <span class="author">Andrew Sorensen</span>, <span class="author"><strong>Ben Swift</strong></span> and <span class="author">Alistair Riddell</span></p>

<p>in <span class="venue">Computer Music Journal 38(1)</span> (<a href="/assets/documents/preprints/sorensen_et_al_2014_the_many_meanings_of_live_coding.pdf">pdf</a>)</p>

<p>doi: <a href="https://doi.org/10.1162/COMJ_a_00230">10.1162/COMJ_a_00230</a></p>

</div>
</div>
<div class="bibliography"><div>

<p class="pubitem" id="swiftVisualCodeAnnotations2013"><span class="title"><a href="https://ieeexplore.ieee.org/document/6617345">Visual Code Annotations for Cyberphysical Programming</a></span> <span class="date">(2013)</span></p>

<p>by <span class="author"><strong>Ben Swift</strong></span>, <span class="author">Andrew Sorensen</span>, <span class="author">Henry Gardner</span> and <span class="author">John Hosking</span></p>

<p>in <span class="venue">LIVE '13: 1st International Workshop on Live Programming at ICSE</span> (<a href="/assets/documents/preprints/swift_et_al_2013_visual_code_annotations_for_cyberphysical_programming.pdf">pdf</a>)</p>

<p>doi: <a href="https://doi.org/10.1109/LIVE.2013.6617345">10.1109/LIVE.2013.6617345</a></p>

</div>
</div>

<p>If there are other thinkers/writers/doers that you can think of who also have
something to say about this, then bring those ideas along and we can discuss
them as well.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:pingers" role="doc-endnote">

      <p>I don’t really have any lived experience with this—I’m approaching 40 and
have young kids and I’m normally in bed by 9:30pm. <a href="#fnref:pingers" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:cui" role="doc-endnote">

      <p>Hence the pretentious latin in the title of this blog post. <a href="#fnref:cui" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:gauche" role="doc-endnote">

      <p>I hope it’s not too gauche to provide <em>two</em> self-citations. <a href="#fnref:gauche" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="teaching" /><category term="livecoding" /><summary type="html"><![CDATA[Here’s the link to the slides for the actual presentation on Oct 13.]]></summary></entry><entry><title type="html">Moving to the School of Cybernetics</title><link href="https://benswift.me/blog/2021/05/11/moving-to-the-school-of-cybernetics/" rel="alternate" type="text/html" title="Moving to the School of Cybernetics" /><published>2021-05-11T00:00:00+10:00</published><updated>2021-05-11T00:00:00+10:00</updated><id>https://benswift.me/blog/2021/05/11/moving-to-the-school-of-cybernetics</id><content type="html" xml:base="https://benswift.me/blog/2021/05/11/moving-to-the-school-of-cybernetics/"><![CDATA[<p>Some job news—I’ve just accepted a position in the new <a href="https://3ainstitute.org">School of
Cybernetics</a> in the ANU College of Engineering
and Computer Science (CECS). I’ll finish out my teaching this semester in the
<a href="https://cs.anu.edu.au">School of Computing</a> then I’ll make the move in July.
For those not familiar with the ANU org-chart or acronymicon, there are multiple
<em>Schools</em> in each <em>College</em> (so I’m moving Schools but staying within the same
College).</p>

<p>I’m really excited by the prospect of designing educational experiences
in/around/through the <a href="https://www.griffithreview.com/articles/touching-the-future/">new
Cybernetics</a>, and
that’s what my<sup id="fnref:my" role="doc-noteref"><a href="#fn:my" class="footnote" rel="footnote">1</a></sup> team in the School of Cybernetics will be doing (under our
Director <a href="https://twitter.com/feraldata">Genevieve Bell</a>). My work as an
interdisciplinary computer scientist/computer musician has always been
cybernetics-adjacent, and I’m stoked to get a chance to lean in to this
interdisciplinarity as we reimagine what a School (within a College, within a
university) might look like.</p>

<p>However, this decision is bittersweet; I love my current job, my colleagues and
(especially) my students, and I’ll be sad to see them less as I move to the
School of Cybernetics. In fact, there are a few things I want to say about this
move, and this blog is as good a place to say them as any.</p>

<p>To <strong>outside observers</strong>, I want to be <em>super clear</em> that I’m not leaving the
School of Computing because it sucks, or because there’s no opportunity to do
cool stuff there—quite the opposite. I think the future of computing is bright
at the ANU and I know there are multiple folks all across the campus working
hard to create that future in the present. To that end, if you’re an
up-and-coming creative code researcher/practitioner/educator, then you should
apply for a <a href="https://jobs.acm.org/jobs/computing-tenure-track-faculty-positions-canberra-australian-capital-territory-2601-122695751-d">tenure-track position in the School of
Computing</a>—it’d
be great to have you join the team, and we’d have chances to work together I’m
sure.</p>

<p>To my <strong>School of Computing colleagues</strong>, I really appreciate your collegiality,
support and friendship over these past nine years (I started as a post-doc on 1
Jan ‘13). I’ll still be around (the School of Cybernetics will sit in the Birch
building), and I’m keen to be a sounding board or collaborator. There are a few
people in particular who have trained and supported me to be the educator that I
am today, and while I won’t do the full “I’d like to thank…” Oscars acceptance
speech thing, I hope you know who you are, and how much I appreciate it.</p>

<p>To the <strong>School of Computing admin team</strong>, you’re awesome. I’ll miss hassling
you on Teams to buy weird gear for the Laptop Ensemble.</p>

<p>To my <strong>current students</strong>, I (and my new colleagues in the School of
Cybernetics) will make sure you don’t get stuffed around. I’ll keep supervising
all my current students till your projects are complete, and you won’t notice
much difference—just that from July onwards we might have our meetings in a
different building. If you’re in my new <a href="https://cs.anu.edu.au/courses/extn1019/">ANU Extension Creative
Computing</a> course, then there’s good
news as well—that course will keep going as planned. In fact, it’s a high
priority activity for the whole College, and there will be amazing doors that
open up for you through participation in that course. If you’re a year 11
student in 2022, you can start thinking about applying to be part of next year’s
cohort—<a href="mailto:ben.swift@anu.edu.au">drop me a line</a> if you’d like some tips
on how to prepare for that process.</p>

<p>To any <strong>prospective/future students</strong> (i.e. those who were hoping to take <a href="/teaching/">one
of my classes</a> in the future) the news is a bit more
mixed. I’ll still be involved with the <a href="https://cs.anu.edu.au/courses/comp2710-lens/">Laptop
Ensemble</a>, and the current plan is
that other courses I’ve designed &amp; taught (e.g.
<a href="https://cs.anu.edu.au/courses/comp2300/">COMP2300</a>,
<a href="https://cs.anu.edu.au/courses/comp1720/">COMP1720</a>) will still continue as
normal with <a href="https://cs.anu.edu.au/code-creativity-culture/people/charles-martin/">other (excellent!)
lecturers</a>.
Finally, while I was previously scheduled to take over <strong>COMP4610 Computer
Graphics</strong> in S2 2021, this now won’t happen. If that stuffs up your study
plans, I’m really sorry.</p>

<p>Finally, while I’m taking on new and exciting responsibilities in the School of
Cybernetics, I’ll still be the same person with the same interests, strengths
and weaknesses. So if we’re excited about the same things now, chances are we’ll
still be excited about the same things into the future—and I hope we have the
opportunity to support one another to pursue those goals.</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:my" role="doc-endnote">

      <p>This is “my” in the “team I’m part of” sense, not the “team I lead” sense.
The new School of Cybernetics is going to be a wildly collaborative place, and I’m just one
part of a multi-talented whole. <a href="#fnref:my" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="teaching" /><category term="research" /><category term="cybernetics" /><summary type="html"><![CDATA[Some job news—I’ve just accepted a position in the new School of Cybernetics in the ANU College of Engineering and Computer Science (CECS). I’ll finish out my teaching this semester in the School of Computing then I’ll make the move in July. For those not familiar with the ANU org-chart or acronymicon, there are multiple Schools in each College (so I’m moving Schools but staying within the same College).]]></summary></entry><entry><title type="html">LENS’21 final gig 2pm June 6 @ sideway</title><link href="https://benswift.me/blog/2021/05/06/lens-21-final-gig-2pm-june-6-sideway/" rel="alternate" type="text/html" title="LENS’21 final gig 2pm June 6 @ sideway" /><published>2021-05-06T00:00:00+10:00</published><updated>2021-05-06T00:00:00+10:00</updated><id>https://benswift.me/blog/2021/05/06/lens-21-final-gig-2pm-june-6-sideway</id><content type="html" xml:base="https://benswift.me/blog/2021/05/06/lens-21-final-gig-2pm-june-6-sideway/"><![CDATA[<picture style="position: relative;">
  <img alt="LENS '21 final gig poster" src="/assets/images/posts/lens2021-final-gig-poster.jpg" />

  
</picture>

<p>If you’re a fan of <em>LENS</em>, the ANU Laptop Ensemble, then you’re welcome to come
to the S1 2021 end-of-semester gig. Here are the details</p>

<ul>
  <li><strong>Date</strong>: Sunday June 6</li>
  <li><strong>Time</strong>: Doors at 2pm, warm-up act (me) to start at 2:30pm, LENS members on from 3pm, finish at 6pm</li>
  <li><strong>Venue</strong>: <a href="https://sidewaybc.com">sideway music bar</a></li>
</ul>

<p>More details (including a full performer schedule) to come, but mark it in your
diaries. There’s also a <a href="https://www.facebook.com/events/831223320837700">FB
event</a> if that’s your jam.</p>

<p>If you want to get psyched, have a look at what the LENS ‘21 cohort have been up
to with their <a href="https://www.youtube.com/watch?v=mcVQ4-5YvlE&amp;list=PLKm3iGh1D7Mvm9byMk4mP40xVxYAbSUIn">weekly Audiovisual Diary submissions on
YouTube</a>.</p>]]></content><author><name>Ben Swift</name></author><category term="lens" /><category term="gigs" /><category term="livecoding" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">ACMC’21 + Synthmoot: Connections announced for August 26-28</title><link href="https://benswift.me/blog/2021/04/29/acmc-21-synthmoot-connections-announced-for-august-26-28/" rel="alternate" type="text/html" title="ACMC’21 + Synthmoot: Connections announced for August 26-28" /><published>2021-04-29T00:00:00+10:00</published><updated>2021-04-29T00:00:00+10:00</updated><id>https://benswift.me/blog/2021/04/29/acmc-21-synthmoot-connections-announced-for-august-26-28</id><content type="html" xml:base="https://benswift.me/blog/2021/04/29/acmc-21-synthmoot-connections-announced-for-august-26-28/"><![CDATA[<p>Great news on the local computer music conference front: the <a href="https://aim.edu.au">Australian
Institute of Music</a> is hosting the
<a href="https://computermusic.org.au">ACMC</a> conference in Sydney/Melbourne/online at
the end of August ‘21. I’ll be there—and if you’re a
computers-and-music-adjacent artist then you should think about submitting a
paper, artist talk or performance as well.</p>

<p>If you’re not sure if this is your scene, then I’d <a href="mailto:ben.swift@anu.edu.au">love to chat to
you</a> and convince you that it might be—we’re
trying to remove the stuffy academic stigma that sometimes hangs around
conferences like this (and also making it super-cheap for indies to attend).</p>

<p>For more details, see the <a href="https://www.acmc21.art">conference website</a>.</p>]]></content><author><name>Ben Swift</name></author><category term="acmc" /><category term="livecoding" /><summary type="html"><![CDATA[Great news on the local computer music conference front: the Australian Institute of Music is hosting the ACMC conference in Sydney/Melbourne/online at the end of August ‘21. I’ll be there—and if you’re a computers-and-music-adjacent artist then you should think about submitting a paper, artist talk or performance as well.]]></summary></entry><entry><title type="html">A short list of Extempore livecoding tricks</title><link href="https://benswift.me/blog/2021/04/23/a-short-list-of-extempore-livecoding-tricks/" rel="alternate" type="text/html" title="A short list of Extempore livecoding tricks" /><published>2021-04-23T00:00:00+10:00</published><updated>2021-04-23T00:00:00+10:00</updated><id>https://benswift.me/blog/2021/04/23/a-short-list-of-extempore-livecoding-tricks</id><content type="html" xml:base="https://benswift.me/blog/2021/04/23/a-short-list-of-extempore-livecoding-tricks/"><![CDATA[<p>Yesterday’s <a href="https://cs.anu.edu.au/courses/comp2710-lens/">LENS</a> class
discussion turned into an <a href="https://www.reddit.com/r/AMA/">AMA</a> of how I do
certain things when I’m livecoding in Extempore. As promised, here’s a blog post
where I’ve put together all of the things we’ve discussed (with a bit more
explanation). If you’re in the LENS ‘21 class this’ll hopefully be a helpful
complement to yesterday’s class discussion. If you’re not in the class, then
maybe you’ve always been curious about certain things I (over)use in my
livecoding sets? This is maybe a bit too niche to go in the general Extempore
documentation, but if you’ve got any questions then you can hit me up on Teams
(for LENS students) or the <a href="mailto:extemporelang@googlegroups.com">Extempore mailing
list</a> (everyone else).</p>

<div id="toc">

  <h2>table of contents</h2>

<ul id="markdown-toc">
  <li><a href="#the-cosr-macro" id="markdown-toc-the-cosr-macro">the <code>cosr</code> macro</a></li>
  <li><a href="#adding-instruments-to-the-sharedsystem-setup" id="markdown-toc-adding-instruments-to-the-sharedsystem-setup">adding instruments to the sharedsystem setup</a></li>
  <li><a href="#markov-chains" id="markdown-toc-markov-chains">markov chains</a></li>
  <li><a href="#weighted-random-selections-from-a-list" id="markdown-toc-weighted-random-selections-from-a-list">weighted random selections from a list</a></li>
  <li><a href="#rel-for-relative-pitches" id="markdown-toc-rel-for-relative-pitches"><code>rel</code> for relative pitches</a></li>
  <li><a href="#nof-and-the-macros-vs-functions-distinction" id="markdown-toc-nof-and-the-macros-vs-functions-distinction"><code>nof</code> and the macros vs functions distinction</a></li>
  <li><a href="#quasiquote--vs-regular-quote-" id="markdown-toc-quasiquote--vs-regular-quote-">quasiquote (<code>`</code>) vs regular quote (<code>'</code>)</a></li>
  <li><a href="#modulating-filter-or-other-params-over-time" id="markdown-toc-modulating-filter-or-other-params-over-time">modulating filter (or other) params over time</a></li>
  <li><a href="#playing-a-loop-which-then-stops" id="markdown-toc-playing-a-loop-which-then-stops">playing a loop which then stops</a></li>
  <li><a href="#multi-laptop-extempore-clock-sync" id="markdown-toc-multi-laptop-extempore-clock-sync">multi-laptop Extempore clock sync</a></li>
  <li><a href="#selecting-the-correct-audio-device" id="markdown-toc-selecting-the-correct-audio-device">Selecting the correct audio device</a></li>
  <li><a href="#audio-underflow-are-you-pushing-extempore-too-hard" id="markdown-toc-audio-underflow-are-you-pushing-extempore-too-hard">Audio underflow: are you pushing extempore too hard?</a></li>
  <li><a href="#creating-a-startup-file" id="markdown-toc-creating-a-startup-file">Creating a “startup” file</a></li>
</ul>

</div>

<h2 id="the-cosr-macro">the <code>cosr</code> macro</h2>

<p><code>cosr</code> is just a convenient way of sinusoidally modulating something with a
period synced to the tempo.</p>

<p>When it came to writing up this part of yesterday’s discussion I had <em>déjà
vu</em>—I was sure I’d written this stuff up elsewhere. Turns out I have—it’s in
the main Extempore docs as part of the
<a href="https://extemporelang.github.io/docs/guides/pattern-language/#what-is-cosr">pattern</a>
guide.</p>

<p>One other tip which I mentioned yesterday: if you use fractions for the period
(final argument) which have a co-prime numerator &amp; denominator (e.g. 7/3 or
15/8) then you’ll get interesting accenting patterns, but which will still
repeat (so it gives a bit more structure than just using <code>random</code>).</p>

<p>One other related trick that I use is doing “mod” calculations on the <code>beat</code>
variable (which is an explicit argument in most temporal recursions), but is
also implicitly bound in the pattern expression if you’re using the pattern language:</p>

<pre><code class="language-extempore">(if (= (modulo beat 2) 0)
    (println 'downbeat))

(if (= (modulo beat 2) 1)
    (println 'upbeat))
</code></pre>

<h2 id="adding-instruments-to-the-sharedsystem-setup">adding instruments to the sharedsystem setup</h2>

<p>The default sharedsystem instruments are defined near the top of the
<code>examples/sharedsystem/audiosetup.xtm</code> file:</p>

<pre><code class="language-extempore">(make-instrument syn1 analogue)
(make-instrument syn2 analogue)
(make-instrument syn3 analogue)
(make-instrument kit dlogue)
(make-instrument samp1 sampler)
</code></pre>

<p>If you want to use different instruments, you can add them to the various DSP
callbacks <code>dsp1</code> to <code>dsp5</code> (this multi-DSP-function setup is to allow the audio
engine’s work to be distributed across multiple cores).</p>

<p>In each of the DSP functions (e.g. <code>dsp1</code>), the actual work of getting the
“signal” out of the relevant instrument happens in a line like this:</p>

<pre><code class="language-extempore">(set! out (syn1 in time chan dat))
</code></pre>

<p>To add e.g. an <code>fmsynth</code> to the sharedsystem setup, there are two steps:</p>

<ol>
  <li>
    <p>define the <code>fmsynth</code> instrument somewhere with <code>(make-instrument fmsynth fmsynth)</code></p>
  </li>
  <li>
    <p>call the <code>fmsynth</code> function in one of your DSP functions (doesn’t matter
which one—they all get summed in the end) callbacks and make sure the
return value is added to one of the <code>out</code> variables</p>
  </li>
</ol>

<p>Here’s an example (in <code>dsp1</code> as per the previous example):</p>

<pre><code class="language-extempore">(set! out (+ (syn1 in time chan dat)
             (fmsynth in time chan dat)))
</code></pre>

<p>Then, any notes you play (using <code>play</code>) on the <code>fmsynth</code> instrument will make
their way into the audio output.</p>

<h2 id="markov-chains">markov chains</h2>

<p>As discussed yesterday, Scheme (most lisps, really) makes it pretty easy to
represent markov chains.</p>

<p>Consider the following temporal recursion, which (in addition to the usual
<code>beat</code> and <code>dur</code> arguments) also takes a <code>pitch</code> argument. This is the exact
code I wrote yesterday:</p>

<pre><code class="language-extempore">(define (obi-lead beat dur) pitch
  (play samp1 pitch (cosr 75 20 5/3) dur 2)
  (callback (*metro* (+ beat (* .5 dur))) 'obi-lead (+ beat dur) dur
            (random (cdr (assoc pitch '((60 60 63 67)
                                        (63 67 60)
                                        (67 58)
                                        (58 60)))))))

(obi-lead (*metro* 'get-beat 4) 1/4 60)
</code></pre>

<p>The markov chain happens in that <code>(random (cdr (assoc ...)))</code> line at the end of
the definition of the <code>obi-lead</code> function. A couple of things to note:</p>

<ul>
  <li>
    <p>the <code>cdr</code> is only necessary to make sure that a “self-transition” (i.e. the
pitch staying the same) only happens if you explicitly add the same pitch as
an element <em>other than</em> the first element of the list. Otherwise, you wouldn’t
be able to make a markof chain that always went to a <em>different</em> pitch from
the current one.</p>
  </li>
  <li>
    <p>the <code>random</code> call isn’t a special “markov” random, it’s just the normal
<code>random</code> picking from a list of pitches (so you can use the same “use the same
element multiple times for weighted sampling” trick mentioned below)</p>
  </li>
  <li>
    <p>you can do whatever you want with the <code>pitch</code> argument inside the body of the
<code>obi-lead</code> function—the fact that we <code>play</code> it here is a common pattern, but
you could e.g. fire out an OSC message, etc.</p>
  </li>
  <li>
    <p>this is just a specific example of the more general class of algorithms which
use a temporal recursion to pass variables to subsequent callbacks, and
potentially do tests/operations on said variables to change them for future callbacks</p>
  </li>
  <li>
    <p>the onus is on the livecoder to make sure that each possible state (i.e. each
value that <code>pitch</code> can take) is represented as the head (<code>cdr</code>) of one of the
lists. But as long as you satisfy that invariant then the <code>obi-lead</code> markov
process will just keep on playing the <code>pitch</code> as it travels through time and
space<sup id="fnref:mb" role="doc-noteref"><a href="#fn:mb" class="footnote" rel="footnote">1</a></sup>.</p>
  </li>
</ul>

<h2 id="weighted-random-selections-from-a-list">weighted random selections from a list</h2>

<p>As promised (Caleb!) here’s the syntax for doing a weighted random sample: the
key is that the argument to <code>random</code> isn’t a list of <code>cons</code> pairs, it’s multiple
<code>cons</code> pair arguments, each one of the form <code>(cons WEIGHTING VALUE)</code>.</p>

<pre><code class="language-extempore">;; here's an example: note that it's in a "do 10 times" loop to show that the
;; values are indeed sampled using the appropriate weighting
(dotimes (i 10)
  (println 'i: i (random (cons 0.1 1) (cons 0.9 -1))))

;; printed output:
;; 
;; i: 0 -1
;; i: 1 -1
;; i: 2 -1
;; i: 3 -1
;; i: 4 1
;; i: 5 1
;; i: 6 -1
;; i: 7 -1
;; i: 8 -1
;; i: 9 -1
</code></pre>

<p>Note that if you’re happy with just rough “this value is twice/three times as
likely as the others” then it’s usually simpler to just randomly sample
(equally-weighted) from a list, including duplicate values for the things you
want to turn up more often in the output:</p>

<pre><code class="language-extempore">;; 0 will be twice as likely as 3 or 7
(random (list 0 0 3 7))
</code></pre>

<h2 id="rel-for-relative-pitches"><code>rel</code> for relative pitches</h2>

<div class="hl-para">

  <p>From here on, all the following code snippets assume you’ve loaded the pattern
language with</p>

  <pre><code class="language-extempore">(sys:load "libs/core/pattern-language.xtm")
</code></pre>

  <p>Note that the pattern language library is loaded when you load the sharedsystem
as well.</p>

</div>

<p>Based on the (global) <code>*scale*</code> variable, you can use <code>rel</code> to calculate pitches
“relative to” a starting pitch.</p>

<p>So, if you’re starting with a middle C (midi note <code>60</code>) and you want to go <code>2</code>
notes up the scale (and you haven’t changed value of the <code>*scale*</code> variable from
the default “C natural minor” scale) then you can use:</p>

<pre><code class="language-extempore">(rel 60 2)
</code></pre>

<p>This can be handy when paired with the <code>range</code> function (which just generates
lists of integers) for running your scales:</p>

<pre><code class="language-extempore">(:&gt; scale-runner 4 0 (play samp1 (rel 60 @1) 80 dur) (range 8))
</code></pre>

<p>Note that this is just a slightly terser version of <code>pc:relative</code> (in
<code>libs/core/pc_ivl.xtm</code>) function, which doesn’t use the <code>*scale*</code> variable by
default. Note <em>further</em> that <code>rel</code> takes an optional third argument for
providing a different scale, if e.g. you want to use a different scale for your
“relative pitch” calculation than you’re currently using elsewhere in the piece.</p>

<p>Here’s an example:</p>

<pre><code class="language-extempore">;; set scale to F natural minor
(set! *scale* (pc:scale 5 'aeolian))

;; use a Fm7 chord (a subset of F natural minor) for the relative pitch calculation
(rel 65 (random 4) (pc:chord 5 '-7))
</code></pre>

<p>If you <code>println</code> the output of that <code>(pc:chord 5 '-7)</code> (<em>Fm7</em>) function, you’ll
get the result <code>(5 8 0 3)</code>, which corresponds to the following pitch classes:</p>

<ul>
  <li><code>5</code>: F</li>
  <li><code>8</code>: A♭</li>
  <li><code>0</code>: C</li>
  <li><code>3</code>: E♭</li>
</ul>

<p>which are the pitches from an Fm7 chord, so it all checks out.</p>

<p>As one final tip, you can just skip the call to <code>pc:chord</code> altogether and do
something like:</p>

<pre><code class="language-extempore">(rel 65 (random 4) '(5 8 0 3))
</code></pre>

<p>This makes it super-easy to make quick edits, e.g. if you want to flatten the
fifth (C). But it does make it a little less readable for the audience (and
let’s face it, reading <code>(pc:chord 5 '-7)</code> was already pretty tough going for
most folks outside the <em>music theory</em> ∩ <em>computer programmer</em> intersection).</p>

<h2 id="nof-and-the-macros-vs-functions-distinction"><code>nof</code> and the macros vs functions distinction</h2>

<p><code>nof</code> (think “give me <em>n</em> of these”) is a Scheme macro for creating a list by
repeatedly evaluating a form.</p>

<p>So, one way to get a list of 10 <code>0</code>s is:</p>

<pre><code class="language-extempore">(nof 10 0)
</code></pre>

<p>To get a list of 4 random integers between <code>0</code> and <code>9</code> (inclusive) you could use:</p>

<pre><code class="language-extempore">(nof 4 (random 10))
</code></pre>

<p>I just evaluated the above form on my machine; the result was <code>(0 9 9 8)</code>. Note
that the numbers are different; so the <code>nof</code> macro is obviously not just taking
the result of a single call to <code>(random 10)</code> and repeating it <code>4</code> times to
create a list.</p>

<p>This is where the fact that it’s a <em>macro</em>—not a function—comes into play.
One trick for looking at what a macro form “macroexpands” out to is calling
<code>macro-expand</code> (note that the <code>nof</code> form has been quoted using <code>'</code>):</p>

<pre><code class="language-extempore">(println (macro-expand '(nof 4 (random 10))))

;; prints:

;; (make-list-with-proc 4 (lambda (idx) (random 10)))
</code></pre>

<p>So, the list is actually formed by four repeated calls to a <code>lambda</code> (anonymous)
function, and that’s why the four random numbers in the above list are different.</p>

<p>If this isn’t actually the behaviour you want—if you want the same random
number repeated four times, there’s a <code>repeat</code> function which you probably want
to use instead. Notice the difference:</p>

<pre><code class="language-extempore">(println (nof 4 (random 10)))
(println (repeat 4 (random 10)))

;; prints:

;; nof: (1 3 3 6)
;; repeat: (5 5 5 5)
</code></pre>

<p>So keep that in mind when you’re using <code>nof</code> in your pattern language. It’ll
probably just work, but this subtle macro vs functino thing may be the cause of
errors or weird behaviour that you see.</p>

<h2 id="quasiquote--vs-regular-quote-">quasiquote (<code>`</code>) vs regular quote (<code>'</code>)</h2>

<p>The quasiquote (<code>`</code>) symbol (which is also called the <em>tilde</em>) is like the
normal quote operator (<code>'</code>), except that you can “undo” the quoting (i.e. eval)
inner forms as necessary using the unquote operator (<code>,</code>).</p>

<p>That’s not easy to get your head around when explained in words, but here’s an
example:</p>

<pre><code class="language-extempore">;; this is kindof tedious to write
'(c3 | | | | | d3 e3)

;; so instead we write
`(c3 ,@(nof 5 '|) d3 e3)

;; don't forget the @ (splicing) part; this is probably not what you want...
`(c3 ,(nof 5 '|) d3 e3)

;; result: (c3 (| | | | |) d3 e3)
</code></pre>

<h2 id="modulating-filter-or-other-params-over-time">modulating filter (or other) params over time</h2>

<p>One way to do it is show in the example file
<code>examples/sharedsystem/analogue_synth_basics.xtm</code>. Have a look around line 39,
where it says:</p>

<pre><code class="language-extempore">;; and now add a second pattern to 'sweep' the filter
(:&gt; B 4 0 (set_filter_env syn1 40.0 100.0 (trir 0.0 1.0 1/32) 100.0) (nof 16 0))
</code></pre>

<h2 id="playing-a-loop-which-then-stops">playing a loop which then stops</h2>

<p>The pattern language isn’t really designed for playing one (or two, or three, or
<em>n</em> for <em>n &lt; ∞</em>) shot loops. It’s really designed for things which will keep on
looping until you stop the pattern.</p>

<p>If you want to play a sequence of events which runs for a while and then stops,
then using a standard temporal recursion is probably best.</p>

<p>Here are a couple of examples. First, this recursion will keep playing the notes
until the <code>pitch</code> argument gets to <code>72</code>.</p>

<pre><code class="language-extempore">(define (ascending-chromatic beat dur pitch)
  (play syn1 pitch 80 dur)
  (if (&lt; pitch 72)
      ;; note the `callback` is inside the `if`
      (callback (*metro* (+ beat (* .5 dur))) 'ascending-chromatic (+ beat dur) dur
                ;; pitch gets incremented by 1 in each subsequent callback
                (+ pitch 1))))

;; kick it off - note that we're passing the initial pitch argument
(ascending-chromatic (*metro* 'get-beat 4) 1/4 60)
</code></pre>

<p>Now, both the “increment” part <code>(+ pitch 1)</code> and the “stopping criteria” part
(i.e. the <code>(if (&lt; pitch 72) ...</code>) are both just Scheme code, so it’s very
flexible. You can handle those things however you like.</p>

<p>One other approach to doing this is a recursion approach straight out of the
functional programming handbook<sup id="fnref:little-schemer" role="doc-noteref"><a href="#fn:little-schemer" class="footnote" rel="footnote">2</a></sup>, just with a temporal twist.
The key idea: start with a list, recurring on the <code>cdr</code> (i.e. the tail of the
list) each time until it’s empty, then stopping.</p>

<p>So, here’s a way of playing a scale—ascending, then descending—then stopping.</p>

<pre><code class="language-extempore">(define (ascending-descending-scale beat dur plist)
  ;; note the (car plist) since plist is a list, not a number
  (play syn1 (car plist) 80 dur)
  (if (not (null? plist))
      (callback (*metro* (+ beat (* .5 dur))) 'ascending-descending-scale (+ beat dur) dur
                (cdr plist))))

(ascending-descending-scale (*metro* 'get-beat 4) 1/4
                            '(60 62 64 65 67 69 71 72 71 69 67 65 64 62 60))
</code></pre>

<p>There are a couple of nice variations on this one:</p>

<ul>
  <li>
    <p>replace the <code>(cdr plist)</code> with <code>(rotate plist -1)</code> to make it cycle through
the pitches, but then go back to the start (have a think about what <code>rotate</code>
does to convince yourself that this works)</p>
  </li>
  <li>
    <p>you can re-trigger this temporal recursion several times, either with the same
<code>plist</code>, or even with different <code>plist</code>s—since all the temporal callback
chains will be independent they’ll all just run nicely over the top of one
another, which can lead to some interesting musical “layerings”</p>
  </li>
</ul>

<h2 id="multi-laptop-extempore-clock-sync">multi-laptop Extempore clock sync</h2>

<p>If you’re jamming with another Extempore laptop musician, you often want to make
sure your tempos &amp; metronomes sync up. One way to do this is to use the topclock
protocol (the name comes from <a href="https://toplap.org">TOPLAP</a>). You’ll need a
network connection between all the machines—wired is best if you can manage
it, or at least on a private-ish wifi LAN (you’ll probably have a bad time
trying to do it over <em>ANU Secure</em>).</p>

<p>There’s an example in <code>examples/core/topclock_metro.xtm</code> which will show you the details.</p>

<p>The alternative (manual) way to sync up two laptops is to:</p>

<ol>
  <li>
    <p>make sure you’re both using the same tempo:</p>

    <pre><code class="language-extempore">(*metro* 'set-tempo 140) ;; or whatever tempo you like
</code></pre>
  </li>
  <li>
    <p>both starting “playing time”, i.e. something simple which is easy to listen
to and feel the tempo</p>
  </li>
  <li>
    <p>listening carefully, try and figure out if you’re <a href="https://www.youtube.com/watch?v=ZQ_6VUs2VCk">rushing or
dragging</a>, and then executing the
appropriate <code>*metro*</code> function call:</p>

    <pre><code class="language-extempore">;; if you're rushing
(*metro* 'pull)
   
;; if you're dragggin
(*metro* 'push)
</code></pre>
  </li>
</ol>

<p>Pros with this approach: no network connection required, and it’s usually not
too tricky to get the timing close enough to jam together. Cons: it’s pretty
manual (and requires some careful listening, which is a skill that takes time to
master), and it also doesn’t help with getting the exact beat clock synced up
(so that you’ll still have to be careful that e.g. your 4-beat bars line up, and
you may have to keep fiddling with some offsets to make it all work).</p>

<h2 id="selecting-the-correct-audio-device">Selecting the correct audio device</h2>

<p>We discussed this during the demo day class, but you should visit the <a href="https://extemporelang.github.io/docs/overview/quickstart/#no-sound-check-your-audio-device">doco
website</a>
to see how to do it. The tip about using <code>--device-name</code> instead of <code>--device</code>
is particularly good advice when you’re trying to bump in quickly in a gig
situation.</p>

<h2 id="audio-underflow-are-you-pushing-extempore-too-hard">Audio underflow: are you pushing extempore too hard?</h2>

<p>The sharedsystem is actually kindof heavyweight<sup id="fnref:cpu-requirements" role="doc-noteref"><a href="#fn:cpu-requirements" class="footnote" rel="footnote">3</a></sup> (or at least medium-weight) from
a CPU use perspective. It loads up 4 analogue synths and a sampler, plus some FX
(e.g. global convolution reverb) and tries to distribute them across multiple
cores on your machine.</p>

<p>If you’re getting lots of “audio underflow” messages, you’ve got a few options,
in order of easiest fixes to most difficult:</p>

<ol>
  <li>
    <p>don’t run any other software on your machine that you don’t absolutely need
for the gig</p>
  </li>
  <li>
    <p>try starting Extempore with a larger frame size (e.g. <code>./extempore --frames 8192</code>)</p>
  </li>
  <li>
    <p>if you don’t need MIDI I/O, just load <code>(sys:load
"examples/sharedsystem/audiosetup.xtm")</code> instead of <code>(sys:load
"examples/sharedsystem/setup.xtm")</code></p>
  </li>
  <li>
    <p>remove some of the <code>dspN</code> (for <code>N</code> = 1..5) functions from the signal chain in
<code>examples/sharedsystem/audiosetup.xtm</code> (e.g. if you’re only using <code>syn1</code> and
<code>syn2</code> you could remove <code>dsp3</code> from the signal chain, which uses <code>dspmt</code> as
the final output sink)</p>
  </li>
  <li>
    <p>get your hands on a beefier laptop (not an option for many people, obviously)</p>
  </li>
</ol>

<p>If you go with option #4, remember that you can create a new copy of
<code>audiosetup.xtm</code> and modify it to your heart’s content. Even if you’ve just been
messing with the original <code>audiosetup.xtm</code> file directly, remember that you can
get a “pristine” version at any time from
<a href="https://github.com/digego/extempore/blob/master/examples/sharedsystem/audiosetup.xtm">GitHub</a>.</p>

<h2 id="creating-a-startup-file">Creating a “startup” file</h2>

<p><code>sys:load</code> can load any file on your computer (even files outside your
<code>extempore</code> directory if you provide a full path). So it’s a good idea to put
<em>everything</em> required to set up your Extempore session into one file, then you
can just load that file with e.g.</p>

<pre><code class="language-extempore">(sys:load "/Users/ben/Documents/research/extemporelang/xtm/sessions/lens-2021-demo/setup.xtm")
</code></pre>

<p>and then once that’s loaded you’re good to go.</p>

<p>Extra tip: also memorise a line of code that you can quickly type &amp; execute to
check if there’s sound coming out.</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:mb" role="doc-endnote">
      <p>to the <a href="https://www.youtube.com/watch?v=uf723IeStzc">world of the Mighty Boosh</a> <a href="#fnref:mb" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:little-schemer" role="doc-endnote">

      <p>that’s not just an idiom, I’m actually thinking of a <a href="https://mitpress.mit.edu/books/little-schemer-fourth-edition">specific
book</a> <a href="#fnref:little-schemer" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:cpu-requirements" role="doc-endnote">

      <p>I mean, it’s still supposed to work on a half-decent laptop—it shouldn’t
require a real beast—but if you’re on a particularly old/wheezy machine
then even with the below tricks the sharedsystem might not be a good choice.
The other examples (e.g. <code>examples/core/fmsynth.xtm</code>) show how to create a
lighter-weight DSP chain, and from there you could add only the instruments
&amp; effects that you need. <a href="#fnref:cpu-requirements" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ben Swift</name></author><category term="livecoding" /><category term="extempore" /><category term="lens" /><summary type="html"><![CDATA[Yesterday’s LENS class discussion turned into an AMA of how I do certain things when I’m livecoding in Extempore. As promised, here’s a blog post where I’ve put together all of the things we’ve discussed (with a bit more explanation). If you’re in the LENS ‘21 class this’ll hopefully be a helpful complement to yesterday’s class discussion. If you’re not in the class, then maybe you’ve always been curious about certain things I (over)use in my livecoding sets? This is maybe a bit too niche to go in the general Extempore documentation, but if you’ve got any questions then you can hit me up on Teams (for LENS students) or the Extempore mailing list (everyone else).]]></summary></entry></feed>