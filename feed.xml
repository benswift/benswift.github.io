<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://benswift.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://benswift.me/" rel="alternate" type="text/html" /><updated>2025-07-07T14:43:56+10:00</updated><id>https://benswift.me/feed.xml</id><title type="html">benswift.me</title><subtitle>livecoder &amp; researcher homepage - code, creativity, culture</subtitle><author><name>Ben Swift</name></author><entry><title type="html">DIYChatGPT Short Course for ANU Undergraduates</title><link href="https://benswift.me/blog/2025/07/07/diychatgpt-short-course-for-anu-undergraduates/" rel="alternate" type="text/html" title="DIYChatGPT Short Course for ANU Undergraduates" /><published>2025-07-07T00:00:00+10:00</published><updated>2025-07-07T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/07/07/diychatgpt-short-course-for-anu-undergraduates</id><content type="html" xml:base="https://benswift.me/blog/2025/07/07/diychatgpt-short-course-for-anu-undergraduates/"><![CDATA[<p>If you‚Äôre an ANU undergraduate student you can enrol in an upcoming (next week!)
<a href="https://mccuskerinstitute.anu.edu.au/study/knots/diy-chatgpt-llms-as-information-processing-machines/">upcoming 3hr course</a>
I‚Äôve created called <em>DIY ChatGPT: LLMs as Information Processing Machines</em>. It‚Äôs
running for the first time <strong>next Thursday 17 July 10am‚Äì1pm</strong> (although it will
run again later in the semester).</p>

<p>Here‚Äôs the blurb:</p>

<blockquote>
  <p>In this hands-on workshop you will train and use your own language model from
scratch - with just pen and paper and a bit of dice rolling. Through
interactive exercises and guided discussions, you‚Äôll see how language models
(even large language models like ChatGPT) are fundamentally information
processing machines, turning language inputs into language outputs. The
workshop builds from basic principles to more complex applications through an
exploration of language modeling as a probabilistic process of predicting
‚Äúwhat token comes next‚Äù in a sequence, and ends with a poetry slam (for real).</p>
</blockquote>

<p>This course is a
<a href="https://mccuskerinstitute.anu.edu.au/study/knots_explained/">‚ÄúKnow the Nature Of Things‚Äù KNoT</a>
offered as part of the ANU‚Äôs
<a href="https://mccuskerinstitute.anu.edu.au/">McCusker Institute</a>. They‚Äôre for-credit
courses run by different folks all across campus, and a great way to explore
some of the cool stuff going on at ANU.</p>]]></content><author><name>Ben Swift</name></author><summary type="html"><![CDATA[If you‚Äôre an ANU undergraduate student you can enrol in an upcoming (next week!) upcoming 3hr course I‚Äôve created called DIY ChatGPT: LLMs as Information Processing Machines. It‚Äôs running for the first time next Thursday 17 July 10am‚Äì1pm (although it will run again later in the semester).]]></summary></entry><entry><title type="html">Agentic Elixir superpowers: Zed + Tidewave + AshAI</title><link href="https://benswift.me/blog/2025/06/06/agentic-elixir-superpowers-zed-tidewave-ashai/" rel="alternate" type="text/html" title="Agentic Elixir superpowers: Zed + Tidewave + AshAI" /><published>2025-06-06T00:00:00+10:00</published><updated>2025-06-06T00:00:00+10:00</updated><id>https://benswift.me/blog/2025/06/06/agentic-elixir-superpowers-zed-tidewave-ashai</id><content type="html" xml:base="https://benswift.me/blog/2025/06/06/agentic-elixir-superpowers-zed-tidewave-ashai/"><![CDATA[<p>For a few years now whenever I need to build any sort of networked interactive
experience, I reach for Elixir (with Phoenix LiveView and Ash). It‚Äôs an
<em>extremely</em> productive combination, especially when you want to do more complex
client/server information flows than the standard request/response UX. I‚Äôve also
(for the last year or two) ditched Emacs for <a href="https://zed.dev">Zed</a>. Again, if
you take the time to master these tools I think they‚Äôre excellent.</p>

<p>Like every indie hacker and their wombat I‚Äôm experimenting with LLM Agents as
part of my software development workflow. The new frontier (which I‚Äôm excited
about, but ask me in a few months how it‚Äôs gone) is putting all of the above
together. If you‚Äôd like to do that too, and want to see a real-world example of
how these things can be plugged into one another, here‚Äôs my setup.</p>

<ul>
  <li>Zed as the text editor/MCP host</li>
  <li>both <a href="https://hexdocs.pm/tidewave/zed.html">Tidewave</a> and
<a href="https://hexdocs.pm/ash_ai/readme.html">AshAI</a> as MCP servers</li>
</ul>

<p>Then, my Zed settings file has this MCP server configuration:</p>

<pre><code class="language-json">"context_servers": {
  "ash_ai": {
    "command": {
      "path": "mcp-proxy",
      "args": ["http://localhost:4000/ash_ai/mcp"],
      "env": {}
    },
    "settings": {}
  },
  "tidewave": {
    "command": {
      "path": "mcp-proxy",
      "args": ["http://localhost:4000/tidewave/mcp"],
      "env": {}
    },
    "settings": {}
  }
},
</code></pre>

<p>Even after following the instructions for those tools, you‚Äôll need a way to set
up all the proxies and pipe everything together. I wrote
<a href="https://github.com/benswift/.dotfiles/blob/master/bin/tidewave-proxy.sh">this script</a>,
which you‚Äôre free to use (MIT Licence) if it helps.</p>

<p>Put it on your <code>~PATH</code> and then (in your Phoenix project root) run it like so:</p>

<pre><code>[16:03] daysy:panic $ tidewave-proxy.sh
üöÄ MCP Proxy Development Environment
====================================
Configuration:
  Host: localhost
  Port: 4000
  Ash AI MCP: http://localhost:4000/ash_ai/mcp
  Tidewave MCP: http://localhost:4000/tidewave/mcp

üîó Starting mcp-proxy instances...
Starting mcp-proxy for ash_ai...
‚úÖ mcp-proxy for ash_ai started (PID: 88815)
   Log file: /tmp/mcp_ash_ai.log
Starting mcp-proxy for tidewave...
‚úÖ mcp-proxy for tidewave started (PID: 88816)
   Log file: /tmp/mcp_tidewave.log

üéâ MCP Proxy environment is ready!
=====================================

Available MCP Servers:
  üìä Ash AI MCP:    http://localhost:4000/ash_ai/mcp
  üåä Tidewave MCP:  http://localhost:4000/tidewave/mcp

MCP Proxy Commands:
  For Ash AI:    mcp-proxy http://localhost:4000/ash_ai/mcp
  For Tidewave:  mcp-proxy http://localhost:4000/tidewave/mcp

Next Steps:
1. Configure your MCP client (Zed, Claude Desktop, etc.)
2. Use the endpoints above with your MCP client
3. Test with: curl -H 'Accept: application/json' &lt;endpoint&gt;

Log Files:
  Ash AI Proxy: /tmp/mcp_ash_ai.log
  Tidewave Proxy: /tmp/mcp_tidewave.log

Press Ctrl+C to stop all services and clean up

üî• Starting Phoenix development server with REPL...
==================================================
Erlang/OTP 27 [erts-15.2.5] [source] [64-bit] [smp:16:16] [ds:16:16:10] [async-threads:1] [jit]

[info] Migrations already up
[info] Running PanicWeb.Endpoint with Bandit 1.7.0 at 127.0.0.1:4000 (http)
[info] Access PanicWeb.Endpoint at http://localhost:4000
Interactive Elixir (1.18.4) - press Ctrl+C to exit (type h() ENTER for help)
[watch] build finished, watching for changes...

Rebuilding...

Done in 196ms.
</code></pre>

<p>Then, to test, in the Agent panel try something like:</p>

<pre><code>Use the tidewave project eval tool to add 10+15 in Elixir.
</code></pre>

<p>If everything‚Äôs hooked up right, you‚Äôll get the answer.</p>

<p>One other tip: make sure you‚Äôre using OTP27 rather than the (newest)
OTP28‚Äîthere‚Äôs an error on one of the AshAI deps which stops it compiling on
the latest OTP. I suspect it‚Äôll be fixed soon, though.</p>]]></content><author><name>Ben Swift</name></author><category term="elixir" /><category term="web" /><category term="zed" /><summary type="html"><![CDATA[For a few years now whenever I need to build any sort of networked interactive experience, I reach for Elixir (with Phoenix LiveView and Ash). It‚Äôs an extremely productive combination, especially when you want to do more complex client/server information flows than the standard request/response UX. I‚Äôve also (for the last year or two) ditched Emacs for Zed. Again, if you take the time to master these tools I think they‚Äôre excellent.]]></summary></entry><entry><title type="html">LLM Parlour Games for Overeducated Wankers</title><link href="https://benswift.me/blog/2024/11/07/llm-parlour-games-for-overeducated-wankers/" rel="alternate" type="text/html" title="LLM Parlour Games for Overeducated Wankers" /><published>2024-11-07T00:00:00+11:00</published><updated>2024-11-07T00:00:00+11:00</updated><id>https://benswift.me/blog/2024/11/07/llm-parlour-games-for-overeducated-wankers</id><content type="html" xml:base="https://benswift.me/blog/2024/11/07/llm-parlour-games-for-overeducated-wankers/"><![CDATA[<p><em>Note: this stuff is the workshop content for an alumni workshop in November
2024 hosted by the Cybernetic Studio at the ANU School of Cybernetics.</em></p>

<h2 id="abstract">Abstract</h2>

<p>In this interactive design session participants will design and prototype their
own language-model-based parlour game. We‚Äôll think critically about what
language models are (and aren‚Äôt) and what they‚Äôre good for (and rubbish at).
You‚Äôll interact with other humans and design systems with goals and guardrails,
and think about what it means to give input to (and understand output from) LLMs
and genAI systems.</p>

<p>Prerequisites: ability to lounge around and use big words to impress your
friends in games of no stakes whatsoever. Self-satisfied smugness about said
loquaciousness is helpful but not essential.</p>

<h2 id="tech-note">Tech note</h2>

<p>This workshop requires access to a chat-based LLM (e.g. ChatGPT). If you‚Äôve got
a laptop (or even a phone, although you‚Äôll be typing on your janky little phone
keyboard) you can head to <a href="https://chatgpt.com">https://chatgpt.com</a> (no sign-up required). But if
you‚Äôve got a different favourite chat-based LLM, feel free to use that instead.</p>

<h2 id="outline">Outline</h2>

<ul>
  <li>intro</li>
  <li>play 20 questions
    <ul>
      <li>shareback</li>
    </ul>
  </li>
  <li>adventures in amphibology
    <ul>
      <li>shareback</li>
    </ul>
  </li>
  <li>design your own LLM parlour game
    <ul>
      <li>shareback</li>
    </ul>
  </li>
</ul>

<h2 id="thesis-statement">Thesis statement</h2>

<blockquote>
  <p>the killer app for genAI is parlour games for overeducated wankers</p>
</blockquote>

<p>(this is a demographic I know well, because I am one)</p>

<p>To break it down:</p>

<ul>
  <li>
    <p><strong>parlour</strong>: involving co-located humans. Bored. Night outside, drinks and a
warm fire inside‚Ä¶ and so</p>
  </li>
  <li>
    <p><strong>games</strong>: to entertain ourselves and each other</p>
  </li>
</ul>

<p>‚Ä¶for‚Ä¶</p>

<ul>
  <li>
    <p><strong>overeducated</strong>: word games, word play, we are masters of language and we
love to show off</p>
  </li>
  <li>
    <p><strong>wankers</strong>: (literally) self-indulgent. Not about a bigger goal, or doing
good in the world, it‚Äôs just for the heck of it.</p>
  </li>
</ul>

<h2 id="mechanics-of-a-parlour-game">Mechanics of a parlour game</h2>

<pre><code class="language-text">10 someone says something
20 someone(s) says something in response
30 GOTO 20
</code></pre>

<p>As an example, consider the game of
<a href="https://en.wikipedia.org/wiki/Twenty_questions">Twenty Questions</a>.</p>

<p><em>Ben scrawls on the whiteboard for 5mins</em></p>

<h2 id="play-llm-augmented-20-questions">Play: <em>LLM-augmented 20 Questions</em></h2>

<p>Here are the prompts (you can copy-paste them automatically with the widget at
the top right of the text).</p>

<p>To start, let‚Äôs have the LLM be the ‚Äúanswerer‚Äù and you (and other human players)
be the ‚Äúquestioner‚Äù, although you‚Äôll need to have someone at a laptop be the LLM
surrogate. Have them use this (or similar) prompt, and don‚Äôt show the output to
the questioner(s).</p>

<pre><code class="language-markdown">You're playing Twenty Questions, so you need to choose an object and tell me
what it is (I won't tell the other players).
</code></pre>

<p>Then, the questioner(s) can ask the LLM questions (via the surrogate) like:</p>

<pre><code class="language-markdown">Is the object you're thinking of bigger than a car?
</code></pre>

<p>The surrogate continues to mediate between the LLM and the questioners until
there‚Äôs a winner (either a questioner guesses correctly, or there have been 20
questions).</p>

<h3 id="shareback">Shareback</h3>

<ul>
  <li>what‚Äôs the funnest/funniest moment, and why?</li>
  <li>what parts sucked?</li>
  <li>how did your play/strategy/behaivour change over time?</li>
  <li>did you tweak the game rules at all? if so, how?</li>
</ul>

<h2 id="play-adventures-in-amphibology">Play: <em>Adventures in Amphibology</em></h2>

<p>Every player thinks of a word or (short) phrase, writes it down on a piece of
paper and puts it in a hat.</p>

<p>These words/phrases are all fed into an LLM using the following prompt template:</p>

<pre><code class="language-text">You are an expert in the use of language, and you have been given the following words/phrases:

- &lt;phrase one&gt;
- &lt;phrase two&gt;
- &lt;phrase three&gt;
- etc.

Which of these words/phrases is the most ambiguous, and why?
</code></pre>

<p>The player who wrote the thing that the LLM chooses scores 1 point. But they‚Äôre
not the winner just yet.</p>

<p>Using the same words/phrases (i.e. without starting a new ChatGPT session) ask
the LLM to pick again, based on successively different criteria:</p>

<pre><code class="language-markdown">Which of these words/phrases best describes your perfect date?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases is the purplest?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases most evokes the experience of a crisp winter sunrise in Canberra?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the best title for a sci-fi movie?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases is the most nihilistic?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases has the most assonance?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases is the most disrespectful?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the best name for a pet cat?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases sounds most like a boy band track from the 90s?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the easiest to explain to a toddler?</code></pre>

<pre><code class="language-markdown">Which of these words/phrases would be the best password for a secret underground antifascist network?</code></pre>

<p>At the end, the player with the most points wins. You can play as many times as
you like, with new words/phrases and/or new ‚Äújudging‚Äù criteria.</p>

<h3 id="shareback-1">Shareback</h3>

<ul>
  <li>what‚Äôs the funnest/funniest moment, and why?</li>
  <li>what parts sucked?</li>
  <li>how did your play/strategy/behaivour change over time?</li>
  <li>did you tweak the game rules at all? if so, how?</li>
</ul>

<h2 id="play-design-your-own-llm-parlour-game">Play: design your own LLM parlour game</h2>

<p>Write/draw it up in such a way that a different group (in this workshop) could
play it without you being there to help them out.</p>

<h3 id="shareback-and-remember">Shareback, and remember</h3>

<p>There‚Äôs no higher purpose here. It‚Äôs all just parlour games for overeducated
wankers.</p>]]></content><author><name>Ben Swift</name></author><category term="teaching" /><category term="ai" /><summary type="html"><![CDATA[Note: this stuff is the workshop content for an alumni workshop in November 2024 hosted by the Cybernetic Studio at the ANU School of Cybernetics.]]></summary></entry><entry><title type="html">Format Markdown on save in Zed using Prettier</title><link href="https://benswift.me/blog/2024/09/20/format-markdown-on-save-in-zed-using-prettier/" rel="alternate" type="text/html" title="Format Markdown on save in Zed using Prettier" /><published>2024-09-20T00:00:00+10:00</published><updated>2024-09-20T00:00:00+10:00</updated><id>https://benswift.me/blog/2024/09/20/format-markdown-on-save-in-zed-using-prettier</id><content type="html" xml:base="https://benswift.me/blog/2024/09/20/format-markdown-on-save-in-zed-using-prettier/"><![CDATA[<p>One thing I‚Äôve particularly enjoyed since switching to Zed is format-on-save,
which is turned on by default in most programming modes. However, it‚Äôs not
turned on by default for Markdown files.</p>

<p>Since I like my Markdown files formatted with <a href="https://prettier.io">Prettier</a>,
including the ‚Äúwrap to 80 cols‚Äù <code>--prose-wrap always</code> option, I set that command
as an ‚Äúexternal‚Äù formatter for Markdown files and turned on <code>format_on_save</code>.
Here‚Äôs the config if you want to do the same:</p>

<pre><code class="language-json">"languages": {
    "Markdown": {
      "format_on_save": "on",
      "formatter": {
        "external": {
          "command": "prettier",
          "arguments": [
            "--prose-wrap",
            "always",
            "--stdin-filepath",
            "{buffer_path}"
          ]
        }
      }
    }
  }
</code></pre>]]></content><author><name>Ben Swift</name></author><category term="tools" /><category term="zed" /><summary type="html"><![CDATA[One thing I‚Äôve particularly enjoyed since switching to Zed is format-on-save, which is turned on by default in most programming modes. However, it‚Äôs not turned on by default for Markdown files.]]></summary></entry><entry><title type="html">Hosting a genAI trivia night</title><link href="https://benswift.me/blog/2024/08/12/hosting-a-genai-trivia-night/" rel="alternate" type="text/html" title="Hosting a genAI trivia night" /><published>2024-08-12T00:00:00+10:00</published><updated>2024-08-12T00:00:00+10:00</updated><id>https://benswift.me/blog/2024/08/12/hosting-a-genai-trivia-night</id><content type="html" xml:base="https://benswift.me/blog/2024/08/12/hosting-a-genai-trivia-night/"><![CDATA[<p>I was recently tasked with organising a trivia night, and decided to generate
all the questions (and answers) with a large language model (I used
<a href="https://claude.ai/">Claude</a>, although obviously this would work with any model.</p>

<p>Here‚Äôs the initial prompt I used:</p>

<blockquote>
  <p>Write a set of questions (10 rounds, 5 questions per round) for a trivia
night, including answers. Each round must have a different theme, including
rounds on the topics of ‚Äú<em>insert list of rounds here</em>‚Äù. You must provide
questions which have a single, unambiguous correct answer. Include a mix of
easy and difficult questions, such that a graduate-level audience would get
approximately 50% of the answers correct.</p>
</blockquote>

<p>Looking over the answers, they looked a little too easy, so I provided a
follow-up:</p>

<blockquote>
  <p>Those questions are all too easy. Try again, and dial up the difficulty.</p>
</blockquote>

<p>which gave questions which looked (to my eyes) to be around the right level.</p>

<p>Now, LLMs are notorious for hallucinating/making up facts, and I couldn‚Äôt be
bothered to check that all the answers were correct. So I incorporated this ‚Äúis
the LLM making stuff up?‚Äù dynamic into the rules. As well as the usual trivia
night procedure:</p>

<ul>
  <li>1 pt per question</li>
  <li>each question will be read <strong>twice</strong> (no more than that)</li>
  <li>we‚Äôll give answers and tally scores after each round</li>
  <li>no cheating (internet <em>or</em> AI models, inc. self-hosted ones)</li>
</ul>

<p>there was an additional rule: at the end of each round, each team can challenge
any answer(s) they think the LLM got wrong. For each challenge, the trivia hosts
would investigate (using the internet, or whatever) to see what the correct
answer is.</p>

<ul>
  <li>if the LLM‚Äôs answer was wrong, <em>all teams</em> have that question re-marked with
the correct answer</li>
  <li>if the LLM‚Äôs answer was ambiguous (i.e. it was correct, but there are other
answers that were equally correct) then <em>all teams</em> have that question
re-marked, accepting any of the correct answers</li>
  <li>if the LLM‚Äôs answer was correct (or if we can‚Äôt find a reliable answer in an
appropriate timeframe) then the question is not re-marked, and the challenging
team receives an additional one-point penalty</li>
</ul>

<p>As for whether the LLM was correct/ambiguous/wrong, all decisions by the trivia
hosts were final.</p>

<p>And how‚Äôd it go? Pretty well, overall. In the end the questions were a bit too
tricky. Turns out it‚Äôs really hard to eyeball questions <em>with</em> answers to guess
how many you‚Äôd get correct, so if you‚Äôre going to do that make sure you do it
without looking at the answers.</p>

<p>There was one successful challenge on the night, but overall there didn‚Äôt seem
to be too many hallucinations. In some ways it would have been more fun if there
were.</p>

<p>Anyway if you need to organise a trivia night and don‚Äôt want to do any
painstaking research, then give the above prompts a try.</p>]]></content><author><name>Ben Swift</name></author><category term="ai" /><summary type="html"><![CDATA[I was recently tasked with organising a trivia night, and decided to generate all the questions (and answers) with a large language model (I used Claude, although obviously this would work with any model.]]></summary></entry><entry><title type="html">Livecoding set ICLC‚Äô24 - Shanghai Concert Hall</title><link href="https://benswift.me/blog/2024/05/30/livecoding-set-iclc-24-shanghai-concert-hall/" rel="alternate" type="text/html" title="Livecoding set ICLC‚Äô24 - Shanghai Concert Hall" /><published>2024-05-30T00:00:00+10:00</published><updated>2024-05-30T00:00:00+10:00</updated><id>https://benswift.me/blog/2024/05/30/livecoding-set-iclc-24-shanghai-concert-hall</id><content type="html" xml:base="https://benswift.me/blog/2024/05/30/livecoding-set-iclc-24-shanghai-concert-hall/"><![CDATA[<p>I‚Äôm currently in Shanghai tomorrow night I‚Äôll be performing at the <a href="https://www.shanghaiconcerthall.org/">Shanghai Concert Hall</a> as part of <a href="https://iclc.toplap.org/2024/program.html#table-day2">ICLC ‚Äò24</a>. I‚Äôm told tickets are already sold out, but if you‚Äôve already got one and you‚Äôre coming along then come say hi after the set.</p>

<p>I‚Äôll try and put up a video of the performance (I‚Äôll be doing music, with LXT
and Beverly Edwards on visuals) once it‚Äôs all done.</p>]]></content><author><name>Ben Swift</name></author><summary type="html"><![CDATA[I‚Äôm currently in Shanghai tomorrow night I‚Äôll be performing at the Shanghai Concert Hall as part of ICLC ‚Äò24. I‚Äôm told tickets are already sold out, but if you‚Äôve already got one and you‚Äôre coming along then come say hi after the set.]]></summary></entry><entry><title type="html">Congrats Dr. Chenchen Xu</title><link href="https://benswift.me/blog/2023/06/01/congrats-dr-chenchen-xu/" rel="alternate" type="text/html" title="Congrats Dr. Chenchen Xu" /><published>2023-06-01T00:00:00+10:00</published><updated>2023-06-01T00:00:00+10:00</updated><id>https://benswift.me/blog/2023/06/01/congrats-dr-chenchen-xu</id><content type="html" xml:base="https://benswift.me/blog/2023/06/01/congrats-dr-chenchen-xu/"><![CDATA[<p>A big congratulations do my PhD student Chenchen for successfully completing his
PhD; his thesis title was <em>Weakly Supervised Vision and Language Representation
Learning in Sign Language Understanding</em>. It‚Äôs freelly available online from the
<a href="https://openresearch-repository.anu.edu.au/items/174662f9-0aba-4ed2-a340-eea1574969b4">ANU Library</a>.</p>

<p>Not only did he do excellent work, but (look at the date!) did a lot of it
alone, under lockdown. Really tough circumstances and a credit to you Chenchen
for coming out the other side.</p>]]></content><author><name>Ben Swift</name></author><category term="research" /><category term="ai" /><summary type="html"><![CDATA[A big congratulations do my PhD student Chenchen for successfully completing his PhD; his thesis title was Weakly Supervised Vision and Language Representation Learning in Sign Language Understanding. It‚Äôs freelly available online from the ANU Library.]]></summary></entry><entry><title type="html">Parental leave take 3</title><link href="https://benswift.me/blog/2023/05/13/parental-leave-take-3/" rel="alternate" type="text/html" title="Parental leave take 3" /><published>2023-05-13T00:00:00+10:00</published><updated>2023-05-13T00:00:00+10:00</updated><id>https://benswift.me/blog/2023/05/13/parental-leave-take-3</id><content type="html" xml:base="https://benswift.me/blog/2023/05/13/parental-leave-take-3/"><![CDATA[<p>Apologies for the radio silence over the last couple of months. I have the happy
opportunity to take one more round of parental leave. So I‚Äôll be off doing Dad
stuff until March ‚Äò24.</p>]]></content><author><name>Ben Swift</name></author><summary type="html"><![CDATA[Apologies for the radio silence over the last couple of months. I have the happy opportunity to take one more round of parental leave. So I‚Äôll be off doing Dad stuff until March ‚Äò24.]]></summary></entry><entry><title type="html">Writing graph theory proofs with ChatGPT</title><link href="https://benswift.me/blog/2023/02/08/writing-graph-theory-proofs-with-chatgpt/" rel="alternate" type="text/html" title="Writing graph theory proofs with ChatGPT" /><published>2023-02-08T00:00:00+11:00</published><updated>2023-02-08T00:00:00+11:00</updated><id>https://benswift.me/blog/2023/02/08/writing-graph-theory-proofs-with-chatgpt</id><content type="html" xml:base="https://benswift.me/blog/2023/02/08/writing-graph-theory-proofs-with-chatgpt/"><![CDATA[<p>Posting logs of <a href="https://chat.openai.com">ChatGPT</a> interaction is getting a bit
pass√© by now in Feb ‚Äò23, so I‚Äôll stop doing it soon (I promise). But yesterday I
was hanging out with my father-in-law and showing how it might impact his
classes‚Äîhe‚Äôs Associate Chair of the Mathematics Department at a top uni in
China.</p>

<p>The prompts were his (with a bit of light suggesting from me), responses from
ChatGPT.</p>

<p><strong>Write a proof of of Pythagoras‚Äôs theorem.</strong></p>

<p>Pythagoras‚Äôs theorem states that in a right triangle, the square of the length
of the hypotenuse (the side opposite the right angle) is equal to the sum of the
squares of the lengths of the other two sides. Symbolically, this can be written
as:</p>

<p>c^2 = a^2 + b^2</p>

<p>where c is the length of the hypotenuse, and a and b are the lengths of the
other two sides.</p>

<p>Proof:</p>

<p>Draw a right triangle ABC with sides AB (a), BC (b), and AC (c).</p>

<p>Draw squares with sides AB, BC, and AC on each side of the triangle, as shown
below:</p>

<p>[Diagram of a right triangle with squares on each side]</p>

<p>The area of each of the squares is equal to the square of the length of its
side. Hence:</p>

<p>a^2 = AB x AB = area of square with side AB</p>

<p>b^2 = BC x BC = area of square with side BC</p>

<p>c^2 = AC x AC = area of square with side AC</p>

<p>Consider the combined area of the squares on the two smaller sides (a and b). It
can also be obtained by covering the entire triangle with a larger square and
then subtracting the area of the square on the hypotenuse (c). Hence:</p>

<p>a^2 + b^2 = (AB + BC) x (AB + BC) - AC x AC</p>

<p>On the right-hand side of the equation, (AB + BC) is equal to the length of the
hypotenuse (c**, since it is the longest side in a right triangle. Hence:</p>

<p>a^2 + b^2 = c x c - c x c</p>

<p>Simplifying the right-hand side of the equation, we obtain:</p>

<p>a^2 + b^2 = c x c</p>

<p>Hence, we have proved that:</p>

<p>c^2 = a^2 + b^2</p>

<p>This is Pythagoras‚Äôs theorem.</p>

<p><strong>Ok, that‚Äôs great. Now it‚Äôs time for some graph theory. Can you give me a proof
of the 5-color theorem?</strong></p>

<p>The five color theorem states that any planar graph (a graph that can be drawn
on a plane without any of its edges crossing) can be colored with no more than
five colors, such that no two adjacent vertices (vertices connected by an edge)
have the same color.</p>

<p>Proof:</p>

<p>Consider a planar graph G with a minimum number of vertices, and suppose that it
cannot be colored with fewer than five colors.</p>

<p>Since G has a minimum number of vertices, there must be a vertex v in G with the
smallest degree (the number of edges incident to v). Let d be the degree of v.</p>

<p>Since G cannot be colored with fewer than five colors, d must be greater than or
equal to 4.</p>

<p>Consider the set of vertices adjacent to v, and let S be the set of colors that
have been used to color these vertices.</p>

<p>Since the degree of v is greater than or equal to 4, the number of colors in S
is greater than or equal to 4.</p>

<p>Suppose, S contains all five colors. Then, since each color can be assigned to
at most one of the vertices adjacent to v, we have a contradiction, as there are
more vertices than colors.</p>

<p>Hence, there must be a color not in S. Call this color c.</p>

<p>Color vertex v with color c. Since no two adjacent vertices have the same color,
it follows that G can be colored with no more than five colors.</p>

<p>This proves the five color theorem: any planar graph can be colored with no more
than five colors, such that no two adjacent vertices have the same color.</p>

<p><strong>Ok, can you prove the 4-color theorem as well?</strong></p>

<p>The four color theorem states that any planar graph can be colored with no more
than four colors, such that no two adjacent vertices have the same color.</p>

<p>Proof:</p>

<p>Consider a planar graph G and let H be a subgraph of G.</p>

<p>We proceed by induction on the number of vertices of H. If H has only one
vertex, it can be colored with a single color.</p>

<p>Suppose that the statement is true for all subgraphs of G with at most n - 1
vertices, where n is a positive integer.</p>

<p>Let H be a subgraph of G with n vertices, and let v be a vertex in H with the
minimum degree.</p>

<p>By the induction hypothesis, all subgraphs of H with fewer than n vertices can
be colored with at most four colors.</p>

<p>Consider the set of vertices adjacent to v in H, and let S be the set of colors
used to color these vertices.</p>

<p>If |S| &lt; 4, then we can color vertex v with a color not in S, and H can be
colored with at most four colors.</p>

<p>Otherwise, |S| = 4. We can color v with any color not in S, and since all
subgraphs of H with fewer than n vertices can be colored with at most four
colors, it follows that H can be colored with at most four colors.</p>

<p>Hence, the statement is true for all subgraphs of G with at most n vertices.</p>

<p>Since the statement is true for all positive integers n, it follows that any
planar graph can be colored with no more than four colors, such that no two
adjacent vertices have the same color.</p>

<p>This proves the four color theorem: any planar graph can be colored with no more
than four colors, such that no two adjacent vertices have the same color.</p>

<h2 id="his-response">His response?</h2>

<p>Shock, really. The proof of the <a href="https://en.wikipedia.org/wiki/Five_color_theorem">five colour
theorem</a> is an assignment he
might set in one of his early-year undergraduate classes, and the proof given
certainly looks ok. It might well be wrong (he didn‚Äôt look at it in detail) but
it‚Äôs the sort of thing a harried marker would assume was written by a student
who had some idea of what was going on in class. The <a href="https://en.wikipedia.org/wiki/Four_color_theorem">four colour
theorem</a> is a much trickier
proof‚Äîit was an unsolved research problem until 2005. The proof for that one
is almost certainly wrong, but that would only be obvious to someone with
considerable expertise in the domain (graph theory). Such is the way of LLMs.</p>

<p>What does it all mean? I dunno. It‚Äôs gonna be a wild decade.</p>]]></content><author><name>Ben Swift</name></author><category term="ai" /><category term="teaching" /><summary type="html"><![CDATA[Posting logs of ChatGPT interaction is getting a bit pass√© by now in Feb ‚Äò23, so I‚Äôll stop doing it soon (I promise). But yesterday I was hanging out with my father-in-law and showing how it might impact his classes‚Äîhe‚Äôs Associate Chair of the Mathematics Department at a top uni in China.]]></summary></entry><entry><title type="html">Jekyll build speedups for Ruby 3.2</title><link href="https://benswift.me/blog/2023/02/02/jekyll-build-speedups-for-ruby-3-2/" rel="alternate" type="text/html" title="Jekyll build speedups for Ruby 3.2" /><published>2023-02-02T00:00:00+11:00</published><updated>2023-02-02T00:00:00+11:00</updated><id>https://benswift.me/blog/2023/02/02/jekyll-build-speedups-for-ruby-3-2</id><content type="html" xml:base="https://benswift.me/blog/2023/02/02/jekyll-build-speedups-for-ruby-3-2/"><![CDATA[<p>There‚Äôs a <a href="https://news.ycombinator.com/item?id=34413012">bunch of buzz</a> about
Ruby‚Äôs new YJIT in v3.2.0. I have to develop and maintain a bunch of Jekyll
websites for work, some of which are getting into ‚Äúnon-trivial build time‚Äù
territory (or maybe I‚Äôm really easily distracted, but a 30s build is enough for
me to break my flow).</p>

<p>Here‚Äôs some very unscientifc benchmarks from my <a href="https://jekyllrb.com">Jekyll</a>
(v4.3) builds on my M1 Max MBP (64GB).</p>

<h2 id="ruby-312p20-2022-04-12-revision-4491bb740a-arm64-darwin21">ruby 3.1.2p20 (2022-04-12 revision 4491bb740a) [arm64-darwin21]</h2>

<p>First, this is the previous version of Ruby, which I‚Äôve been happily working
with for ages.</p>

<pre><code>bundle exec jekyll build
</code></pre>

<p>Averaged over 3 runs, the build time was 17.7 (¬±0.8) seconds.</p>

<h2 id="ruby-320-2022-12-25-revision-a528908271-arm64-darwin22">ruby 3.2.0 (2022-12-25 revision a528908271) [arm64-darwin22]</h2>

<p>After upgrading to the latest Ruby v3.2.0, the average over 3 runs was 17.2 (¬±0.2) seconds.</p>

<h2 id="ruby-320-2022-12-25-revision-a528908271-yjit-arm64-darwin22">ruby 3.2.0 (2022-12-25 revision a528908271) +YJIT [arm64-darwin22]</h2>

<p>I <a href="https://dev.to/dpaluy/install-ruby-320-yjit-with-asdf-b82">followed these
instructions</a> to
build a YJIT-enabled Ruby v3.2. Then, after re-installing all the deps:</p>

<pre><code>RUBY_YJIT_ENABLE=true bundle exec jekyll build
</code></pre>

<p>The build was <em>heaps</em> faster‚Äîthe average over 3 runs was 3.7 (¬±0.2) seconds.
<strong>That‚Äôs a 4.8x speedup</strong>.</p>

<p>Obviously, like all benchmarks on the internet this one is wrong &amp; stupid and
you should do your own testing. Three runs probably isn‚Äôt enough to shake out
any cache effects, and I don‚Äôt know (but could guess?) that the JIT might have
even more pronounced cold start issues (although I didn‚Äôt really see it above).
Anyway, if you build a lot of Jekyll sites, my anecdotal evidence is that you
can save yourself a bunch of time, and tighten those feedback loops to stay in
the flow state. Have fun!</p>]]></content><author><name>Ben Swift</name></author><category term="web" /><category term="tools" /><summary type="html"><![CDATA[There‚Äôs a bunch of buzz about Ruby‚Äôs new YJIT in v3.2.0. I have to develop and maintain a bunch of Jekyll websites for work, some of which are getting into ‚Äúnon-trivial build time‚Äù territory (or maybe I‚Äôm really easily distracted, but a 30s build is enough for me to break my flow).]]></summary></entry></feed>